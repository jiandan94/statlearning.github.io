{"categories":[{"title":"学术杂谈","uri":"https://jiandan94.github.io/categories/%E5%AD%A6%E6%9C%AF%E6%9D%82%E8%B0%88/"},{"title":"杂文","uri":"https://jiandan94.github.io/categories/%E6%9D%82%E6%96%87/"},{"title":"笔记","uri":"https://jiandan94.github.io/categories/%E7%AC%94%E8%AE%B0/"},{"title":"诗词歌赋","uri":"https://jiandan94.github.io/categories/%E8%AF%97%E8%AF%8D%E6%AD%8C%E8%B5%8B/"},{"title":"随笔","uri":"https://jiandan94.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"posts":[{"content":"最近有读者私信希望介绍R并行计算的方法。在处理较大规模问题时，默认单线程的R显得力不从心。提升运算速度确实在数据分析中意义重大，其中酸爽调参侠定深有感触。\n不过我觉得饭要一口一口吃，如果算法先天不足、设计不合理，指望通过某个函数或程序包暴力提升运算速度，就很难得到满意的结果。\n因此，我打算从一些简单的细节入手，给大家分享R运算速度提升的常见方法和思维误区。\n1. 内置函数与向量化思维 R对常见的运算如矩阵乘积，进行过专门的优化。那么，我们应当有意识将算法中的相关部分转用R优化过的内置函数处理，从而降低计算成本。\n又如，我们在学习模型时，常常看到作者用分量形式展示模型的求解过程——虽然这令人易懂，但让我们在设计算法时极易忽视将其向量化，最后造成计算开销骤增。\n- 案例展示 我们不妨考虑不带截距项的单变量最小二乘模型\n$$\\hat{\\beta}=arg\\min\\sum_{i=1}^n\\left( y_i - x_i\\hat{\\beta}\\right)^2$$\n很容易得到最小二乘解为\n$$\\hat{\\beta} = \\frac{\\sum_{i=1}^nx_iy_i}{\\sum_{i=1}^nx_i^2}$$\n这时，你可能将程序写成\n## 算法1 ols1 \u0026lt;- function(x, y){ a1 \u0026lt;- 0 a2 \u0026lt;- 0 for(i in 1:length(y)){ a1 \u0026lt;- a1 + x[i]*y[i] a2 \u0026lt;- a2 + x[i]^2 } return(a1/a2) }  假如你知道R中*的用法，你可能会将程序写成\n## 算法2 ols2 \u0026lt;- function(x, y){ return(sum(x*y)/sum(x^2)) }  当然你还可以写成向量形式\n## 算法3 ols3 \u0026lt;- function(x, y){ return(c(x%*%y/x%*%x)) }  为了揭示这三者计算效率的差别，我们不妨从标准正态分布中产生1000个观测，重复计算100次的结果如下\n\u0026gt; x \u0026lt;- rnorm(1000) \u0026gt; y \u0026lt;- x*2 + 0.1*rnorm(1000) \u0026gt; system.time(# 算法1 + replicate(100, ols1(x, y)) + ) 用户 系统 流逝 0.02 0.00 0.02 \u0026gt; system.time(# 算法2 + replicate(100, ols2(x, y)) + ) 用户 系统 流逝 0.01 0.00 0.01 \u0026gt; system.time(# 算法3 + replicate(100, ols3(x, y)) + ) 用户 系统 流逝 0 0 0  结果是显然的：向量化的运算比for循环效率明显高。因此，我们要记住 能避免使用for循环就避免使用， 因为for循环在R中很低效。\n善用R内置函数与向量化思维是很多新手易忽视的技巧。这个例子提醒大家须将其铭记于心。\n2. 警惕apply族函数 稍有R常识的读者可能怀疑我题目写错了，因为大家印象中apply族函数可以简化代码和提升效率，所以要广泛使用而不是警惕啊。简化代码我同意，但提升效率真的不一定正确。\n- apply函数 我们先看最常见的apply函数的用法\napply(X, MARGIN, FUN, ...)  其中X表示一个数组或者矩阵；MARGIN表示维度，1表示按行计算，2表示按列计算；FUN是需要施加在X的行或者列上的函数。\n例如，我们希望得到矩阵x每一行的标准差。利用for循环，我们可以将函数写成\nmyrowsd1 \u0026lt;- function(x){ n \u0026lt;- nrow(x) p \u0026lt;- ncol(x) rowsd \u0026lt;- c() for(i in 1:n){ rowsd \u0026lt;- c(rowsd, sqrt(sum((x[i,] - mean(x[i,]))^2)/(p - 1))) } return(rowsd) }  有了前面的向量化意识后，我们可以试着将代码写成\n## 算法2 myrowsd2 \u0026lt;- function(x){ n \u0026lt;- nrow(x) p \u0026lt;- ncol(x) rmean \u0026lt;- rowMeans(x) xbar \u0026lt;- matrix(rep(rmean, p), ncol = p) rowsd \u0026lt;- sqrt(rowSums((x - xbar)^2)/(p - 1)) return(rowsd) }  下面利用apply将sd函数作用到x的每一行上，立即得到我们想要的结果\n## 算法3 myrowsd3 \u0026lt;- function(x){ return(apply(x, 1, sd)) }  从代码上看，apply函数最终的代码确实简洁漂亮。然而，是美与实力兼有还是花瓶一个，需要实践出真知。\n为了比较这三种算法的差异，我们从正态分布中产生1000行10列的矩阵，重复100次后得到\n\u0026gt; x \u0026lt;- matrix(rnorm(10000), ncol = 10) \u0026gt; system.time(# 算法1 + replicate(100, myrowsd1(x)) + ) 用户 系统 流逝 2.08 0.19 2.37 \u0026gt; system.time(# 算法2 + replicate(100, myrowsd2(x)) + ) 用户 系统 流逝 0.03 0.04 0.08 \u0026gt; system.time(# 算法3 + replicate(100, myrowsd3(x)) + ) 用户 系统 流逝 2.52 0.02 2.58  结果令人大跌眼镜——apply函数耗时最多！使用内置函数rowMeans和rowSums的算法2最高效。\n- 并行apply函数 可否让apply函数的简洁与高效并存呢？考虑到apply使用FUN的过程是相互独立的，所以并行计算是个可行的思路。\n在R中已经内置parallel程序包，可以使用类似apply但能多线程运算的parRapply（对矩阵的行进行处理）命令。它的基本调用形式为\ncl \u0026lt;- makeCluster(\u0026lt;size of pool\u0026gt;)# 建立并行线程数 parRapply(cl, x, FUN) stopCluster(cl)# 关闭并行  我们利用并行apply命令重新计算上述问题得到\n\u0026gt; library(parallel) \u0026gt; cl \u0026lt;- makeCluster(2)# 并行两次 \u0026gt; system.time(# 算法3 + replicate(100, parRapply(cl, x, sd)) + ) 用户 系统 流逝 0.39 0.16 2.33 \u0026gt; stopCluster(cl)  可见并行两次的速度确实有提升，超过了for循环但远逊色于算法2的速度——我的surfaceGo只有两个物理核心，如果电脑允许你可以增加并行次数而降低运算时间。\n尽管如此，我想说的是apply函数并不是如我们以为的那样高效，我们在乎计算时间时便要警惕使用该命令！\n不过，这并不意味着apply函数没有存在的价值。在时间开销并不那么重要时，apply得到的代码十分优雅简洁，也容易理解。\n- apply族其他函数 apply族函数除了基本的apply命令外，常用的还有lapply和sapply等。\nlapply(X, FUN, ...)# X通常为向量、数据框或列表  apply命令并不适用于向量，而lapply则显得更加灵活——其中X可取向量、数据框、列表等。例如，我们给定一个向量，然后针对每个元素生成相应数目的均匀随机数，结果用列表储存。\n\u0026gt; x \u0026lt;- c(2,3,4) \u0026gt; lapply(x, runif) [[1]] [1] 0.2485223 0.4080533 [[2]] [1] 0.9620718 0.5607855 0.2601507 [[3]] [1] 0.23525977 0.05190834 0.05177627 0.72261602  如果不喜欢列表储存结果，可以考虑sapply命令，它返回向量或者矩阵。如果FUN得到的结果是一维的，那么sapply就返回一个向量；如果FUN得到的结果是多维的，那么sapply就返回一个矩阵，其每一列对应一个输出结果。\n例如，我们输入一个列表，求其中每一项的均值和方差。\n\u0026gt; x \u0026lt;- list(a = 1:3, b = rnorm(100)) \u0026gt; myfunction \u0026lt;- function(x){ + return(c(mean(x), var(x))) + } \u0026gt; sapply(x, myfunction) a b [1,] 2 -0.1352013 [2,] 1 1.1843918  更多的使用方式和注意细节限于篇幅，不再赘述。\n3. 多线程运算 实际的数据分析中，我们常常需要重复某一过程数十次以上，例如Cross Validation寻找最优参数。一般每次计算相互独立，但计算的规律又极其相似。除了for循环外，我们能否有高效的方法实现该目的呢？\n答案是肯定的：R中的多线程运算就能办到。\n我们这里主要介绍foreach程序包中foreach命令的使用方式。假如我们需要计算100个向量的均值（计算互不影响），for循环意味着依次进行均值计算，这显然低效；foreach多线程则可以同时计算多个向量的均值（同时计算多少取决于你电脑的线程数），自然就显得高效。\n因此，foreach可以看作for循环的加强版。它需要事先安装foreach和doPrarllel两个程序包。\ninstall.packages(\u0026quot;foreach\u0026quot;) install.packages(\u0026quot;doParallel\u0026quot;)  - 基本使用介绍 它的基本用法为\nlibrary(foreach) library(doParallel) cl \u0026lt;- makeCluster(\u0026lt;size of pool\u0026gt;) registerDoParallel(cl)# 注册并行的线程 foreach(..., .combine) %dopar% FUN # 并行计算FUN stopImplicitCluster()# 关闭并行  首先，我们需要确定多线程数并进行注册使用。如果你不知道自己电脑可用的线程数，可以使用detectCores命令获得；通过添加参数logical = F可以得到电脑的实际物理核心数，也即是真正可供调用的实体核心。\n\u0026gt; detectCores() [1] 4 \u0026gt; detectCores(logical = F) [1] 2  可见我的surfaceGo实际有两个物理核心，但可以虚拟两个核心出来。因此，我们可以设置的线程数不超过3个——电脑的运行也需要核心维持，故不能跑满。\n其次，我们解释一下foreach的各个参数。...是函数FUN进行计算的变量，可为一个或多个。.combine则是设置输出的结果形式，如用+连接、用rbind拼接成矩阵等。\n最后，我们使用stopImplicitCluster()关闭并行而不是stopCluster()命令。\n案例分析 我们考虑两个向量对应的的分量和与分量积，输出结果为一个矩阵。那么，程序可以写成\nmyfun \u0026lt;- function(a, b){ return(c(a+b, a*b)) } cl \u0026lt;- makeCluster(2) registerDoParallel(cl) res \u0026lt;- foreach(x = 1:5, y = 6:10, .combine = \u0026quot;rbind\u0026quot;) %dopar% myfun(x, y) stopImplicitCluster()  查看结果为\n\u0026gt; res [,1] [,2] result.1 7 6 result.2 9 14 result.3 11 24 result.4 13 36 result.5 15 50  我在使用foreach包的过程中发现，当问题规模不大、计算过程简单时，foreach调用计算机资源所消耗的时间反而让人得不偿失。 我们用下面的案例来说明问题。\n考虑前面的最小二乘案例，样本观测数固定为1000，重复估计1000次。\n\u0026gt; myfun \u0026lt;- function(){ + x \u0026lt;- rnorm(1000) + y \u0026lt;- x*2 + 0.1*rnorm(1000) + ols3(x, y) + } \u0026gt; system.time(# for循环 + for(i in 1:1000){ + myfun() + } + ) 用户 系统 流逝 0.36 0.00 0.36 \u0026gt; cl \u0026lt;- makeCluster(2) \u0026gt; registerDoParallel(cl) \u0026gt; system.time(# foreach并行 + foreach(i = 1:1000, .combine = \u0026quot;rbind\u0026quot;) %dopar% myfun() + ) 用户 系统 流逝 0.83 0.33 1.45 \u0026gt; stopImplicitCluster()  这也和我们开头所呼应：如果算法先天不足、设计不合理，指望通过某个函数或程序包暴力提升运算速度，就很难得到满意的结果。\n- 一点补充 现在我们编写函数，将foreach得到的100个最小二乘估计的平均值作为最终的解。你的函数若像下面一样编写\nmeanb \u0026lt;- function(){ cl \u0026lt;- makeCluster(2) registerDoParallel(cl) bhat \u0026lt;- foreach(i = 1:10, .combine = \u0026quot;rbind\u0026quot;) %dopar% myfun() stopImplicitCluster() return(mean(bhat)) }  调用后，就现找不到myfun函数的错误提示\n\u0026gt; meanb() Error in myfun() : task 1 failed - \u0026quot;没有\u0026quot;myfun\u0026quot;这个函数\u0026quot;  原因是foreach被嵌套进函数后无法使用函数外部的变量。 因此，我们需要添加.export参数，将需要的外部变量传递到函数内部，也即是\nbhat \u0026lt;- foreach(i = 1:10, .export = c(\u0026quot;ols3\u0026quot;, \u0026quot;myfun\u0026quot;), .combine = \u0026quot;rbind\u0026quot;)  这样，你在外部定义所需变量后再运行meanb函数，就能正常输出结果\n\u0026gt; meanb() [1] 2.001765 }  4. 写在最后 本次我们学习了R提升速度的常见思路和注意事项。最后再次提醒大家，理解自己的问题和程序结构最重要，盲目通过某些命令来暴力提升计算速度，可能会适得其反。\n","id":0,"section":"posts","summary":"最近有读者私信希望介绍R并行计算的方法。在处理较大规模问题时，默认单线程的R显得力不从心。提升运算速度确实在数据分析中意义重大，其中酸爽调参","tags":["R语言","算法"],"title":"提升R的运算速度","uri":"https://jiandan94.github.io/2020/03/rparallel/","year":"2020"},{"content":"1 线性模型的局限性 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题往往更加复杂，$y$ 并不总是满足正态分布的假设。\n例如，在医学诊断中，我们希望通过病人的各项检查数据判断其是否患癌症。这里是否患癌症作为响应变量 $y$ 只有两个可能的取值：患和不患。 显然，$y$ 不服从正态分布。\n类似的例子还有很多。在气象领域中，通过观测到的气象数据判断是否会下雨；在金融领域中，根据当前的金融指标判断是否抛售股票；在图像领域，综合输入的人脸数据判断是否通过验证\u0026hellip;\u0026hellip;\n上述问题的共同点是，$y$ 的取值是离散的，我们关心的不再是预测具体数值而是变成判断分类。\n为了讨论方便，我们以下考虑二分类问题。\n机灵的同学可能想到，依然对数据建立线性模型，然后再设定一个阈值 $a$，当 $y\u0026gt;a$ 时，预测数据归为一类；当 $y\u0026lt;a$ 时，预测数据归为另一类。\n这么做确实可以得到一个分类模型(上图红实线)。但是，如果我们在原有数据上额外得到一个新观测且远离原始数据时， 线性思路得到的新模型就会出现明显的变动(上图蓝虚线)。\n2 逻辑回归(Logistic Regression) 究竟哪里出现问题了呢？我们回忆所学的线性模型的形式：\n$$ y_i = x_i^T\\beta + \\epsilon_i, \\quad\\epsilon_i~N(0, \\sigma^2) $$\n对模型两边取期望和方差可得：\n$$ E(y_i) = x_i^T\\beta, \\quad Var(y_i) = \\sigma^2 $$\n显然可以看出：(1)线性模型对y的估计，本质是对 $y$ 期望的估计；(2)线性模型中 $y$ 的期望和方差两者互相独立。\n接着来看二分类问题，一般我们考虑 $y_i$ 独立同分布于成功概率为 $p_i$ 的伯努利分布(Bernouilli Distribution)：\n$$ y_i \\overset{iid}{\\sim} B(p_i) = p_i^{y_i}(1 - p_i)^{1 - y_i} $$\n两边取期望和方差可得：\n$$ E(y_i) = p_i, \\quad Var(y_i) = p_i(1 - p_i) $$\n此时的期望和方差两者之间存在函数关系且期望取值在 $[0,1]$ 之内。 因此，在二分类问题中完全套用线性模型的框架显然是有问题。\n该怎么办呢？\n试想 $y_i$ 是一个随机变量，在不知道额外信息的前提下，用它的期望作为估计还是挺合理的。因此，我们可以继续采用期望估计的思路。 但是二分类问题中 $y_i$ 的期望有0-1的约束，这就需要我们寻找一种满足该要求的函数。通常，这样的函数选取成Logistic函数：\n$$ E(y_i) = \\frac{1}{1 + \\exp(-x_i^T\\beta)} $$\n很容易验证Logistic函数满足前面的条件。选取这种函数形式的深层原因，从统计的角度来看，源自于指数族分布。 有兴趣的朋友可以翻阅我关于广义线性模型相关理论的介绍以及其他参考资料。\n于是，我们终于等到本次的主角——逻辑回归模型(Logistic Regression Model)：\n$$ y_i \\overset{iid}{\\sim} B(p_i),\\quad E(y_i) = \\frac{1}{1 + exp(-x_i^T\\beta)} $$\n我们之所以称其为回归模型，是因为线性回归深入人心，在其之上生长发展的理论便继承了这个名称。事实上，逻辑回归更多的是面向分类问题。\n3 逻辑回归的求解 3.1 极大似然法 最小二乘常被用来求解线性模型中的系数估计值，这在逻辑回归中失效了——我们无法得到逻辑回归的最小二乘损失函数的具体形式。\n然而我们知道问题的概率分布信息，因此可以得到相应的似然函数：\n$$ \\mathscr{L} = \\prod_{i = 1}^n \\left(\\frac{1}{1 + \\exp(-x_i^T\\hat{\\beta})} \\right)^{y_i} \\left(1 - \\frac{1}{1 + \\exp(-x_i^T\\hat{\\beta})} \\right)^{1 - y_i}$$\n再进而得到对数似然函数并进行优化即可。不过，与线性模型具有显示解不同，逻辑回归没有解析解，故而需要结合梯度下降法等算法进行求解，可参考广义线性模型中的相关方法。限于篇幅，不在赘述。\n3.2 R求解逻辑回归模型 R中内置glm函数可以求解逻辑回归模型，它的使用方式如下所示。\nlogmodel \u0026lt;- glm(formula, data, family = binomial(link = \u0026quot;logit\u0026quot;)) predict.glm(logmodel, newdata)# 预测模型  与线性模型求解函数lm区别在于，glm需要添加link参数。我们使用该函数对前面的模型再次求解并做图，如下所示。\n可见，逻辑回归很好的刻画了模型的主要特征，且不会因为新数据的出现模型发生巨大变动。\n4 案例分析 4.1 数据描述 我们从UCI机器学习数据库中获取到威斯康辛乳腺癌数据(Breast Cancer Wisconsin)共计569个观测，包含1个字符型分类变量，取值B表示良性肿瘤，取值M表示恶性肿瘤；30个数值型检测指标，如肿块厚度、大小、形态等。\n4.2 建立逻辑回归模型 我们使用R读入数据并进行分析。由于数据没有特定顺序，我们选择前70%作为训练集，剩下的作为测试集。为了方便展示结果，我们仅选取前三个变量 进行模型拟合，对应的代码如下所示。\n## 逻辑回归模型处理WDBC数据 wdbc \u0026lt;- read.csv(\u0026quot;wdbc.csv\u0026quot;, header = F, stringsAsFactors = F) colnames(wdbc) \u0026lt;- c(\u0026quot;y\u0026quot;, paste0(\u0026quot;x\u0026quot;, 1:30)) wdbc$y \u0026lt;- ifelse(wdbc$y==\u0026quot;M\u0026quot;, 1, 0) # 划分训练集和测试集 wdbc_train \u0026lt;- wdbc[1:400, 1:4] wdbc_test \u0026lt;- wdbc[401:569, 1:4] # 训练模型 wdbc_fit \u0026lt;- glm(y~., data = wdbc_train, family = binomial(link = \u0026quot;logit\u0026quot;))  4.3 评价模型 首先，我们使用summary()函数查看模型相关的统计指标，具体如下所示。\n可见，三个系数都非常显著。\n其次，我们使用predict.glm()查看模型的内预测效果，得到混淆矩阵(Confusion Matrix)如下\n    真实M 真实B 总计     预测M 215 18 233   预测B 12 155 167   总计 227 173 400    可以算出模型的预测准确率为(155+215)/400=0.925，仅三个变量就可以很好的预测病人肿瘤的状态。\n不过分类问题中，大家比较关心模型的敏感性和特异性。 敏感性指真阳性率，而特异性指真阴性率。以本案例而言，模型的敏感性为0.947，特异性为0.896。这么看来，我们的模型假阳性率为0.053，还是不错的。\nlibrary(pROC) roclong \u0026lt;- plot.roc(wdbc_train$y, lty = 1, train_pre, grid = T, percent=TRUE,ci=TRUE,col=\u0026quot;red\u0026quot;, print.auc = T,print.auc.cex = 0.8,print.auc.x = 60,print.auc.y = 50, main = \u0026quot;逻辑回归的ROC曲线和AUC值\u0026quot;)  进一步分析敏感性和特异性，可以利用ROC曲线 和AUC值 来刻画。我们在R中使用pROC包画出ROC曲线以及进行AUC的计算和检验，结果参见下图。\nAUC达到了92.2%， 说明模型能够很好的刻画我们的数据。\n最后，我们对剩下的测试集进行验证，得到预测准确率为88.76%，还是挺理想的——毕竟我们仅使用三个变量进行建模。关于逻辑回归更多的内容，还请读者自己了解。\n参考文献\n[1] Myers, Raymond H., et al. Generalized linear models: with applications in engineering and the sciences. 2012.\n","id":1,"section":"posts","summary":"1 线性模型的局限性 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题往往更加复杂，$y$ 并不总是满足正态分布的假设。 例","tags":["多元统计","分类算法","r语言"],"title":"逻辑回归模型","uri":"https://jiandan94.github.io/2020/03/logistic-regression/","year":"2020"},{"content":"最近，外国人永久居住条例在年轻人群体中掀起不小的风波。估计有关部门可能都没想到这个条例是个大雷，会有这么的能量。\n围观微博一圈，大家对于该条例基本呈强烈反对的态度。不过，那些为中国亿万女青年未来幸福纷纷哀呼的抨击者，窃以为大概率应该没有把条例完整看完。当然，将所有的人打成无脑喷，显然置信度不够。\n所以，我也实在好奇这个条例，更好奇大家的群情激愤，便谈谈自己简单的看法。\n作为一名典型理科男，对于移民政策知之甚少，但是通读条例几遍后也没理解出网友认为的居住条例的颁布等于大量的外来人口涌入。 不过，既然这是大家的一个痛点，不妨就顺着这个没道理的阅读理解答案分析一下。\n1 假装很理性 在认为条例的颁发必然导致外来人口大量涌入的假设下，作为全球人口数量最大的国家的民众，一个直接的质疑便是，我们不缺人啊？为什么还要吸引众多的移民？是不是有阴谋呢？\n人口第一这个结论自然毋庸置疑，但缺不缺人这个论断还要仔细斟酌。因为发展建设不能只盯着眼前，未来也很重要。\n我从中国统计年鉴中搜集了1982年到2018年的人口数据，这期间的变化趋势图如下所示。\n1982年，我国将“计划生育”写入宪法使之成为一项基本国策，到2016年国家正式施行“二胎”政策。从图中看到，“计划生育”实施后约20年，人口增长的趋势明显变缓才开始显露——所以，政策的影响并不是想当然的立竿见影。\n虽然2016年施行了“二胎”政策，但是这对人口增长的刺激显得有点力不从心。从下图的人口自然增长率可以看出，我们国家的人口增长率常年处于下滑状态。并且，在2016年“二胎政策”后还出现了一个陡降。\n 直觉有时候并不可靠，这在统计学中并不是一个新鲜的话题。\n 所以，现在不缺人，但不太久的将来不一定。我们再来看下图所示的2018年全国人口年龄结构柱状图。\n粗略的计算，60岁以上的人群占比17.88%，约2.5亿人；10年后将退休的的人群占比14.56%，而相应的参加工作（按20岁开始参加工作算）占比10.53%，也就是说\n10年后，约减少0.56亿人的主要劳动力，约为韩国的总人口数。\n所以，建设社会的主要劳动力数目确实以超乎我们想象的速度在减少。 那么，单纯的根据人口数目而深陷“阴谋论”实在不可取。\n不过，上面粗略的分析可以说明不需要引入移民 是一厢情愿的想法，但还不足以让激愤者冷静思考。\n 因为人口减少了，我们想当然的会认为就业机会将增加，落到个人头上便是更加富足。\n 事实果真如此么？\n我在网上搜集到了2018年全球各个国家的人口数以及相应的失业率数据。我们选择人口数前50的国家画出相应的散点图，如下所示。\n瞟一眼难以发现这两者有相关关系。对数据敏感的人可能发现右下角两个离群点：中国和印度，两个人口大国。是不是离群点使得数据信息淹没了呢？不妨移除他们，得到下图。\n这时数据呈现出某种倾向，难以直接断定两者无关。我们不妨对两者做皮尔逊相关性检验，得到P值为0.7141，不显著。\n因此，我们也不能轻易的认为，人口变少，意味着大家更容易就业。\n2 假装在说教 网上经常有很多所谓的流量大V关于某些时事恶意带节奏，如果我们按照他们的思维引导，很容易因为“直觉”被带跑偏而成为网络暴民。理性是个好东西，既然都立足在国家建设、民族发展的高度了，难道不应该去看看条例具体内容，不应该去切实分析具体问题么？\n用自己的想当然去抨击别人在想当然，自然得不到真理。\n要知道我们前面的分析是在一个不合理的假设下进行的，得到的结果虽不足以说明条例合理，却实实在在可以说明“无脑喷子”让人害怕。事实上，条例并没有哪里表明将要大量的引入外来人口，遑论中国男人娶不到老婆。\n3 写在最后 关于条例，我的着眼点其实不是它的条文是否合理，因为不懂相关领域，就不能胡言乱语。那么可以做什么呢？\n我们以往很少直接对国家大政方针建言献策，这次司法部向社会公开征询，让我们以为事关重大，造成神经紧张。因此我们完全可以借这次公开征询的机会慢慢将民众的监督职能进行完善。 某种程度上，这更加重要。\n","id":2,"section":"posts","summary":"最近，外国人永久居住条例在年轻人群体中掀起不小的风波。估计有关部门可能都没想到这个条例是个大雷，会有这么的能量。 围观微博一圈，大家对于该条例","tags":["时政","相关性分析"],"title":"关于外国人永久居住条例的简单看法","uri":"https://jiandan94.github.io/2020/03/immigration-policy/","year":"2020"},{"content":"远行不经意，归途暮雨袭。\n阑风入沉阁，竹斋听雨息。\n乱蝉声渐渐，梧桐叶槭槭。\n雁阵来时路，年年有归期。\n","id":3,"section":"posts","summary":"远行不经意，归途暮雨袭。 阑风入沉阁，竹斋听雨息。 乱蝉声渐渐，梧桐叶槭槭。 雁阵来时路，年年有归期。","tags":["虎溪岁月","诗文"],"title":"偶遇秋雨","uri":"https://jiandan94.github.io/2019/11/oyqy/","year":"2019"},{"content":"没想到易烊千玺的演技这么在线，看完《少年的你》感觉还是挺不错的。不过客观来说，我对电影主题略感模糊，感觉少了一点深度。\n开头那会儿突然就出现霸凌了，有点摸不着头脑的感觉。转念想到近几年也报到过很多次类似的新闻，这样切入也勉强可以吧。\n爱情线路是各类题材不可缺少的一条主线，设计的好也确实能够带动观影人的情绪，比如《少年的你》，但我觉得这也是这个电影跑偏的地方。我记得在观影时，看到小北保护陈念时的义无反顾，其他人都发出羡慕的赞声。尤其是小北把烟头弹向魏莱时，简直帅出天际。\n然而，电影究竟是想在屈辱和挣扎中揭露霸凌现象，还是想在霸凌背景下表达希望和美好？我认为这两者是完全不一样的，看完电影后我觉得主题在这之间模糊了。因为，我发现大家更多的在回味男主女主之间的信任和扶持等等。\n其实，在成长的阶段，很多人都有过委屈，有过忍气吞声。所以，谁不期待在自己孤独无助的时候，有个小北可以义无反顾、风雨无阻得站在自己身后？ 在霸凌的背景下，这样的信任，这样的依靠，这样的担当，无一不能引起观众内心中那根柔软的心弦——尽管大家并未都经历过霸凌。从这点来说，电影是成功的。\n不过，说它揭露清楚了霸凌，我不太认可。\n镜头更多得是展现被霸凌的人的屈辱和无奈，对于那些霸凌产生的根源所提甚少。电影也就浮光掠影得几个镜头，不是原生家庭的支离破碎，就是原生家庭的骄傲无礼，符合普罗大众的\u0026quot;社会映像\u0026quot;嘛。可是，这样就表明挖掘的还不够。 找不到根源就不能\u0026quot;于病视神，未有形而除之\u0026rdquo;，不能让一些施暴者在成为施爆者前得以挽救，比如魏莱的一些跟班姐妹。\n我看电影的时候，不自觉就想起以前的初中。那也是小混混不少，远近闻名。我自己也被小混混拦截抢过钱，也被别人恐吓过，吓得好几天战战兢兢。所以，当我看到女警官质疑陈念为什么在被欺负后不找大人求助，陈念沉默不语的神情时，我挺有感触的。\n因为，大人们不是小北，不会风雨无阻得站在我的背后。而跟大人们说了，就等于和小混混公开宣战，那肯定会被打的很惨。所以，我会选择沉默和忍耐。\n我的好朋友当时也被喊到厕所差点扇耳光。很多人可能没见过厕所外排了一条长队，一个一个走过去给里面的老大扇耳光的场景吧。\n我印象中，我知道和他们一起混的那几个同学，家里不是支离破碎，也没有富甲一方。就是普普通通的家庭，但是父母对孩子管教不上心。有的想法就是，小孩欺负人总比被欺负好，饿不死就行。这种情况不是少数，但比起破碎的家庭，富人的傲慢与偏见等等，少了吸引大众眼球的能力。作为商业电影，我觉得导演这么设定无可厚非。\n只是，我想如果一部揭露社会现象的电影，在关众还未看之前，就已经猜到了现象的设定，那它究竟揭露了什么？或者，在众所周知的原因下，它展现了什么更为深刻的反思？小混混，肯定是家破人亡？我在初中那会儿成绩不错，但我们班主任一直说和xxx比差远了。后面听别人说，xxx初一那会儿被小混混抢了好几回钱，开始班主任送他回家，但送了几次后就没继续了，因为自己也忙啊。后面xxx又被抢了几次。最后，xxx直接选择他们一起混了，而且还凶的很。xxx的爸妈是工人，家境还不错。听说这件事后，就说哪里没一口饭吃，管不了。\n我记得初中那会儿打算写一本小说，要狠狠的揭露这种现象。最后好像写了两章，我还拿去给我爸看了，我爸说有些用词故作老成，实际不够深刻，哈哈哈。无奈文笔不佳，后来不了了之了。现在想来，也是令人感慨。\n校园霸凌的确存在，需要关注，需要更多的思考，更需要从源头认真整治。\n","id":4,"section":"posts","summary":"没想到易烊千玺的演技这么在线，看完《少年的你》感觉还是挺不错的。不过客观来说，我对电影主题略感模糊，感觉少了一点深度。 开头那会儿突然就出现霸","tags":["虎溪岁月","影评"],"title":"看完《少年的你》后的一点想法","uri":"https://jiandan94.github.io/2019/10/yp-sndn/","year":"2019"},{"content":" 如果随便逮到一个统计专业的学生问他“统计方法谁家强”，相信大部分人会异口同声得说“最小二乘法”。\n 的确，最小二乘法是一种非常重要的统计方法，它的重要性不仅仅体现在对问题求解的自然、简单、有效层面，其背后所蕴含的“最小二乘思想”更在不同领域、不同问题中应用广泛。\n虽然以现在的眼光来看最小二乘法出发点朴素而又自然，但是它的产生也是历经波折。本文我们一起来了解一下最小二乘法的“心路历程”吧！\n1 天文和测地学 早期，在天文和测地学中经常会遇到这么一种数据分析情况：我们有若干个可以测量的量 $x_1$, $x_2$,\u0026hellip;, $x_p$ 和 $y$，他们之间呈现一种线性关系\n$$ y = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p $$\n这里 $\\beta_1$,\u0026hellip;, $\\beta_p$ 都是未知参数，需要我们运用一定的方法估计出来从而应用到实际问题中去。\n现在稍有统计背景的都知道这用最小二乘可以轻易求解，不过当时还没有最小二乘的概念呢，因此求解就是一个令人头疼的问题。\n 可能有人说根据线性方程理论这也不是一个很困难的问题啊！只要测量得到 $p$ 组数据，那么应用线性方程的知识不就将未知参数求解出来了么？\n 可是在实际天文和测地学中同一问题研究人员会测量多组数据以降低测量过程中产生的误差，而相应的数据量基本都是大于未知参数的个数的 (其实就是超定线性方程组)，这就使得问题的求解比较棘手。\n我们希望的是，一方面对于方程的个数大于未知参数个数的问题，求解方法应该尽可能的简单、有效；另一方面，由于实际问题中测量误差不可避免，求解方法在理论上应该对误差控制有一个保证。所以总结起来就是四个字：又快又稳！\n2 早期工作 在最小二乘还未诞生的时代，各路英雄豪杰面对该问题也是使劲浑身解数。不过他们的方法核心思路是从大量的方程中挑选或者组合恰当数量的方程来进行参数求解。因此，各家各派使得都是挑选和组合的功夫。\n例如在1750年，天文学家梅耶发表了一种测定航海船只经度的方法，其中要从27个方程中求解出3个未知参数。他主要采用分组法将27个方程分成3组分别相加得到3个方程，最终求解出未知参数。 这个方法曾经一度流行，被冠以梅耶的名字。\n 此外，拉普拉斯和欧拉也在天文学中研究过类似的问题，但是解法极其繁杂而且杂乱无章。 他们二人这样的大数学家一生不知道解决过多少数学中的“疑难杂症”，对于这么一个并不是很困难的问题竟然束手无策。这确实让人感到不可思议。\n 3 勒让德和最小二乘 勒让德是法国大数学家，在数学的很多领域包括椭圆几分、数论和几何等方面都有着重大的贡献。\n$$ (\\hat{\\beta}, \\hat{\\beta}_0) = \\min \\sum _{i=1}^n \\left( y_i - x_i^T\\hat{\\beta} - \\hat{\\beta}_0 \\right)^2 $$\n最小二乘法最早于1805年勒让德公开发表的文章《计算彗星轨道的新方法》中问世。在这本著作的附录中，勒让德描述了最小二乘法的思想、具体做法和优缺点。\n 最小二乘法一经提出，由于其思想自然合理、操作简单有效，很快就得到欧洲一些国家的天文和测地工作者的广泛使用。据不完全统计，自1805年到1864年，有关这一方法的研究论文约有250篇。\n 尽管勒让德的工作没有涉及到最小二乘的误差分析理论，但是他也注意到了各个方程因为误差不独立而不能直接运用最小二乘法， 这的确难能可贵。最小二乘法的“快”勒让德已经说明了，关于它的“稳”则是高斯的工作了。\n4 高斯的工作 1809年，高斯在《绕日天体的运动理论》的一节中讨论了“数据结合”的问题，实际就是误差分布的确立问题。假设真值为 $\\theta$，有 $n$ 个独立的测量值 $x_1$,\u0026hellip;, $x_n$，高斯将后者的概率定为\n$$ \\mathscr{L}(\\theta) = f(x_1 - \\theta)\\cdots f(x_n - \\theta) $$\n其中 $f$ 就是待定的误差密度函数。在确立密度函数形式过程中，高斯有两个创新点。\n  一是他没有采取已有的贝叶斯推理方法，而是直接将 $\\mathscr{L}(\\theta)$ 的最大值——极大似然的思想 ——定为 $\\theta$ 的估计值。\n  二是他先承认了观测值 $x_1$,\u0026hellip;, $x_n$ 的算数平均值为 $θ$ 的估计值，然后再去找误差的密度函数来迎合这一点——在这样的 $f$ 下，$\\theta$ 的估计值就是算数平均值。最后他得出只有在\n  $$ f(x) = \\frac{1}{\\sqrt{2\\pi}h}e^{-\\frac{1}{2h^2}} $$\n时才成立。这就是均值为 $\\theta$ 标准差为 $h$ 的正态分布.\n 使用正态分布就可以对最小二乘给出一种解释，也就是可以对其误差做出理论上的分析，保证了这种方法的优越性。后世将最小二乘的发明权归功于它，也正是因为这一项工作。\n 尽管高斯讨论最小二乘法的文章发表较晚，但是他声称自己很早之前就运用勒让德的最小二乘法来解决问题。这也导致两人后来最小二乘的首创权争论。\n不过在高斯的证明中有点循环论证 的感觉，先承认算数平均值估计的优越性，再得到误差正态密度函数形式，然后再说明算术平均值作为估计的合理性。这一缺陷在拉普拉斯运用其发现的中心极限定理得以解决。 他指出现实中的误差可以看成很多量的叠加，那么根据他的中心极限定理，误差的分布就是正态分布。\n5 写在最后 从最小二乘法的发展历史来看，一项科学理论的发展并无坦途， 尽管这项理论看起来朴素而又简单。\n","id":5,"section":"posts","summary":"如果随便逮到一个统计专业的学生问他“统计方法谁家强”，相信大部分人会异口同声得说“最小二乘法”。 的确，最小二乘法是一种非常重要的统计方法，它","tags":["数学史"],"title":"漫谈最小二乘法","uri":"https://jiandan94.github.io/2019/08/forum-ols/","year":"2019"},{"content":"1 导言 回归分析是一个古老的话题。一百多年前，英国的统计学家高尔顿 (F. Galton，1822-1911) 和他的学生皮尔逊 (K. Pearson，1857-1936) 在研究父母和其子女身高的遗传关系问题中，统计了1078对夫妇的平均身高以及他们的一个成年儿子的身高数据。\n他们将孩子的身高作为自变量 $x$，父母的平均身高作为因变量 $y$，然后将两者画在同一张直角坐标系上。结果，他们发现这些数据点“惊人的”位于一条直线的附近，并且经过计算得到了直线的拟合方程:\n$$ y = 33.73 + 0.516x $$\n 这个结果看起来是违背直觉的。因为统计的结果表明，高个子父母的子女有低于父母身高的趋势；而矮个子的子女则有高于父母的趋势。高尔顿解释说，自然界存在某种约束力将人的身高向某个“平均数”靠拢——或者说是“回归”——也即是统计学上回归的涵义。\n 那么本文的主题便是了解线性回归模型并通过R来解决线性回归分析中的若干问题。\n2 基础回顾 回归的概念来源于实际问题，那么现在我们所说的线性回归分析问题具体指的是什么呢？一般说来，如果我们研究的问题中的 $p$ 个自变量 $x_1$, $x_2$, \u0026hellip;, $x_p$ 和因变量 $y$ 的关系形式如下所示\n$$ y_i = \\beta_0 + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip} + \\epsilon_i $$\n那么我们就说这是一个线性回归问题，其中 $\\epsilon_i$ 是随机误差项，$i$ 表示第 $i$ 个观测。在线性回归问题中我们的核心任务就是估计出未知参数 $\\beta_0$, $\\beta_1$, $\\cdots$, $\\beta_p$ 的值。\n 注意，线性回归问题的确定并不是通过自变量的形式，而是问题中待估计的未知参数最高次都为一次且关于未知参数呈线性关系。例如 $y = \\beta_0 + \\beta_1x_1^2 + \\epsilon$；$y = \\beta_0 + \\beta_1x_1x_2 + \\epsilon$ 都是线性回归问题。\n 通常在线性回归中估计未知参数方法是最小二乘法（OLS），而为了保证估计值能够很好的解释模型，我们又有如下前提条件：\n 正态性：$\\epsilon_i$ 服从正态分布； 独立性：$\\epsilon_i$ 之间是独立的； 线性性：$x$ 和 $y$ 必须线性相关； 同方差性：$\\epsilon_i$ 的方差不变。  这些条件又被称为高斯—马尔可夫条件， 它们保证了在经典线性回归中最小二乘估计的优越性。\n3 求解线性回归模型函数 3.1 极大似然法 最小二乘法和极大似然法都可以用来求解线性回归模型，我们在往期文章中讨论过最小二乘法，这里对似然法进行简单介绍。\n假设我们得到下面一组观测数据：\n$$ (x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n) $$\n那么根据高斯-马尔可夫假设，我们可以得到残差估计的似然函数 为\n$$ \\mathscr{L} = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(y_i - x_i^T\\hat{\\beta} - \\hat{\\beta}_0)^2}{2\\sigma^2}\\right] $$\n 这个式子的成立还需要假设残差分布的均值为0，标准差为 $\\sigma$。这个假设是可行的。因为残差如果均值不为零，可以将其移到模型的截距项里。\n 如何通过上面的函数得到系数的估计值呢？极大似然的思想便是，让这些估计值使得似然函数达到最大！ 这个想法很朴素：每个观测数据随机且互相独立，我们一次搜集便得到眼前的数据，那么自然而然认为这些数据组合出现的概率是最大的。\n不过，数据已经搜集好便不能改动。我们自然想到，系数的估计值便是让这些数据对应的概率可能性最大——也即是似然函数最大。\n现在假装大家已经理解了极大似然的原理，下面我们来求解它。直接最大化不太可行，我们通常对似然函数取对数得到对数似然函数\n$$ \\ln\\mathscr{L} = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\sum_{i = 1}^n\\frac{(y_i - x_i^T\\hat{\\beta} - \\hat{\\beta}_0)^2}{2\\sigma^2} $$\n然后再分别对各个参数进行优化。限于篇幅，不再赘述。\n3.2 R求解线性回归模型 我们可以利用现有软件进行模型求解。在R中求解线性回归问题的最基本函数就是lm()，其格式为：\nmyfit \u0026lt;- lm(formula, data)\r# formula 是要拟合的模型形式，用一个R公式表示\r# data 就是模型的数据构成的数据框\r 下面我们解释一下formula具体的形式，首先看下表总结的formula中常用的符号\n   符号 说明     ~ 分隔符号，左边为因变量，右边为自变量   + 分隔自变量   : 自变量的交互项，如 xz 可以表示成 x:z   * 自变量的所有交互项，如 x*z*w 展开即为 x+z+w+x:z+x:w+z:w+x:z:w   ^ 交互项可以达到某个次数，如(x+z+w)^2展开即为x+z+w+x:z+x:w+z:w   . 除因变量外的所有自变量   -1 删除截距项   I() 如x+I((z+w)^2)等价于x+h，h是z+w平方构成的新变量    如果自变量为 $x_1$, $x_2$ 和 $x_3$ 而预测变量为 $y$，我们假定的线性模型形式为：\n$$ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\epsilon $$\n那么formula可以写成：\ny ~ x1 + x2 + x3\r# 或者为 y ~ .\r 其他形式的模型formula的表达式还请读者自行琢磨。\n当模型拟合成功后，我们使用summary()函数来得到拟合的具体结果。而其他常用的获取线性回归模型拟合结果的函数如下表所示。\n   函数 说明     summary() 拟合情况概述，包括系数显著性、模型显著性等   coefficients() 拟合系数   fitted() 因变量拟合值   residuals() 残差值   plot() 画出模型诊断图    4 实例分析 下面我们将用实例具体介绍lm()函数的使用方法。\n4.1 简单线性回归 本例中我们使用基础安装中的数据集women数据，它记录了15个年龄在30~39岁间女性的身高和体重信息，我们现在来探究体重关于身高的关系。\nmyfit \u0026lt;- lm(weight~height, data = women)\rsummary(myfit) # 展示拟合详细结果\r 程序的输出结果如下所示\n这里主要给读者解释这么几项指标的含义：\n  Residuals 体重预测值和真实值之差的统计信息，从左到右分别为最小值、下四分位数、中位数、上四分位数和最大值。\n  Coefficients 第一列Estimate中Intercept对应的数值为截距项，height对应的即为身高变量前的估计系数。\n  Multiple R-squared 介于0-1之间，越接近1说明线性关系越强。\n  p-value 模型的F检验统计量的p值，值越小说明模型越可靠。\n  因此本例中体重和身高的回归方程为：\n$$ \\hat{Weight} = -87.51667 + 3.45\\times Height $$\n根据R方 (Multiple R-squared) 和p值 (p-value) 可知模型是可靠的。此外，我们可以作图观察最终的拟合结果。\n4.2 具有交互项的线性回归 继续考虑上例，如果模型中存在一个交互项比如一个平方项，那么即有：\nmyfit \u0026lt;- lm(weight~height + I(height^2), data = women)\rsummary(myfit) # 展示拟合详细结果\r 程序的输出结果如下所示。\n可以看到通过比较R方、p值，添加了平方项的线性模型效果更好。我们同样可以做出相应的图像。 5 写在最后 本文主要介绍了R中线性回归分析的简单操作方法。不过，这里仅仅涉及线性回归分析的冰山一角，关于线性回归问题中的回归诊断和异常点的判断等内容，限于篇幅这里就不做介绍了。有兴趣的读者可以学习《R in action》第8章中关于回归的讲解。\n","id":6,"section":"posts","summary":"1 导言 回归分析是一个古老的话题。一百多年前，英国的统计学家高尔顿 (F. Galton，1822-1911) 和他的学生皮尔逊 (K. Pearson，185","tags":["多元统计","r语言","回归分析"],"title":"简单回归分析的R实现","uri":"https://jiandan94.github.io/2019/07/simplelm/","year":"2019"},{"content":"远山疏林轻雾笼，暖阳探窗听晨钟。\n庭前初落银杏雨，芳华凋零又几重。\n不期秋霜侵柳绿，可怜金风老梧桐。\n昨日阆苑尚新蕊，行人行色行匆匆。\n","id":7,"section":"posts","summary":"远山疏林轻雾笼，暖阳探窗听晨钟。 庭前初落银杏雨，芳华凋零又几重。 不期秋霜侵柳绿，可怜金风老梧桐。 昨日阆苑尚新蕊，行人行色行匆匆。","tags":["虎溪岁月","诗文"],"title":"久雨初晴有感","uri":"https://jiandan94.github.io/2018/11/jycqyg/","year":"2018"},{"content":"晴岚裙腰山染黛，\n空谷幽幽暗香来。\n时人剪却好风光，\n月下说与娇娘猜。\n","id":8,"section":"posts","summary":"晴岚裙腰山染黛， 空谷幽幽暗香来。 时人剪却好风光， 月下说与娇娘猜。","tags":["虎溪岁月","诗文"],"title":"七夕","uri":"https://jiandan94.github.io/2018/08/qx/","year":"2018"},{"content":"1 指数族分布与广义线性模型 1.1 引入指数族分布 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题中 $y$ 并不总是满足正态分布的假设。因此，我们考虑更加一般的指数族分布。\n指数族分布定义如下：\n$$ f\\left( y;\\theta ,\\phi \\right) =\\exp \\left[ \\frac{y\\theta -b\\left( \\theta \\right)}{a\\left( \\phi \\right)}+c\\left( y,\\phi \\right) \\right] $$\n其中 $a(\\cdot)$，$b(\\cdot)$ 和 $c(\\cdot)$ 是已知给定的函数。参数 $\\theta$ 为分布族的位置参数（location parameter），参数 $\\phi$ 通常被称为分散参数（dispersion parameter）。一般来说，函数 $a(\\phi)$ 通常有形式 $a(\\phi)=\\phi\\cdot\\omega$，其中 $\\omega$ 是一个已知的常数。指数族分布包括常见的二项分布、泊松分布、正态分布和指数分布等。\n1.2 联系回归问题 前面直接给出指数族分布，有点让人一时难以和回归问题构建联系。我们不妨回忆线性模型的内容，此时响应变量 $y$ 满足下面的正态分布\n$$ N(x^T\\beta, \\sigma^2) $$\n我们写出它的密度函数具体表达式\n$$ f\\left( y \\right) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{(y - x\\beta)^2}{2\\sigma^2} \\right] = \\exp\\left[ -\\frac{(y^2 - 2yx\\beta + \\beta^Tx^Tx\\beta)}{2\\sigma^2} - \\frac{1}{2} \\ln(2\\pi\\sigma^2) \\right] $$\n整理一下就得到\n$$ f\\left( y \\right) = \\exp\\left[ \\frac{yx\\beta - 1/2\\beta^Tx^Tx\\beta}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2} \\ln(2\\pi\\sigma^2) \\right] $$\n原来线性回归问题就是 $\\theta = x\\beta$ 和 $\\phi = \\sigma^2$ 的指数族问题。 因此，我们保留线性模型中的线性结构假设，把正态分布约束推广成指数族分布，这就得到的更为一般的广义线性模型。\n对应线性模型的假设，广义线性模型也有若干前提假设：\n 观测 $y_1,\\cdots ,y_n$ 是相互独立的，且对应的均值为 $\\mu _1,\\mu _2,\\cdots ,\\mu _n$； 每个观测 $y_i$ 具有指数族分布； 模型建立在线性预测因子 $\\eta _1,\\cdots ,\\eta _n$ 上，其中 $\\eta _i=x _{i}^{\\mathrm{T}}\\beta $，$x_i$ 是设计矩阵第 $i$ 个行向量； 模型通过链接函数（link function）建立，其中：$\\eta _i=g\\left( \\mu _i \\right) ,\\ i=1,2,\\cdots ,n$； 链接函数是单调可微的（其反函数存在）。  不难看出待求参数 $\\beta$ 和指数族分布之间的关系链接：\n$$ \\beta \\overset{\\eta _i=g\\left( \\mu _i \\right)}{\\leftrightarrow}\\mu _i\\overset{\\mu _i=\\dot{b}\\left( \\theta _i \\right)}{\\leftrightarrow}\\theta _i $$\n上述这种关系对我们逐步得到参数 $\\beta$ 的求解公式意义重大。\n2 广义线性模型的参数估计 在线性模型的假设下，最小二乘法和极大似然法都能用于参数的求解。在广义线性模型中，我们无法写出二乘形式的优化函数。因此，我们根据分布信息利用极大似然法来估计参数。 模型的似然函数 $\\mathscr{L}(\\theta;Y)$ 为\n$$ \\mathscr{L}(\\theta;Y) = \\prod _{i=1}^n \\exp \\left[ \\frac{y_i\\theta_i -b\\left( \\theta-i \\right)}{a\\left( \\phi_i \\right)}+c\\left( y_i,\\phi_i \\right) \\right] $$\n为了方便理解算法的具体推导过程，下面首先介绍指数族的两个重要结论，然后再具体推导求解算法。\n2.1 两个重要的结论 对于指数族分布，首先讨论两个重要的结论。根据密度函数可以得到相应的对数似然函数：\n$$ \\ln \\mathscr{L}\\left( \\theta ;Y \\right) =\\sum_{i=1}^n{\\frac{y_i\\theta _i-b\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}}+\\sum_{i=1}^n{c\\left( y_i,\\phi _i \\right)} $$\n这里仅假定 $y_i$ 是独立的。则似然函数对 $\\theta$ 求导有：\n$$ \\frac{\\partial \\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta}=\\sum_{i=1}^n{\\frac{y_i-\\dot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}}=\\sum_{i=1}^n{S_i} $$\n似然函数对 $\\theta$ 的二阶导为：\n$$ \\frac{\\partial ^2\\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta ^2}=\\sum_{i=1}^n{\\frac{\\ddot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}} $$\n其中 $\\dot{b}(\\theta)$ 和 $\\ddot{b}(\\theta)$ 是 $b(\\theta)$ 对 $\\theta$ 的一阶导和二阶导（下同）。那么，大部分指数族分布（要求密度函数积分号和求导号可以交换顺序）满足：\n$$ E\\left( \\frac{\\partial \\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta} \\right) =0, \\quad E\\left( \\frac{\\partial \\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta} \\right) =0 $$\n显然，根据上述两条性质可以推出：\n$$ \\mu _i=Ey_i=\\dot{b}\\left( \\theta _i \\right) $$\n和\n$$ Var\\left( y_i \\right) =\\ddot{b}\\left( \\theta _i \\right) a\\left( \\phi \\right) =\\frac{\\mathrm{d}\\mu _i}{\\mathrm{d}\\theta _i}a\\left( \\phi \\right) = Var _{\\mu _i}a\\left( \\phi \\right) $$\n2.2 极大似然估计 根据极大似然思想，对数似然函数对 $\\beta$ 求导得到：\n$$ \\begin{split} S\\left( \\beta \\right) \u0026amp;=\\frac{\\partial \\ln \\mathscr{L}\\left( \\beta ;Y \\right)}{\\partial \\beta}=\\sum_{i=1}^n{\\frac{\\partial}{\\partial \\theta _i}\\left( \\frac{y_i\\theta _i-b\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)} \\right) \\cdot \\frac{\\partial \\theta _i}{\\partial \\mu _i}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}} \\newline \u0026amp;=\\sum_{i=1}^n{\\frac{y_i-\\dot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}\\cdot \\frac{\\partial \\theta _i}{\\partial \\mu _i}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}}=\\sum_{i=1}^n{\\frac{y_i-\\dot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}\\cdot \\frac{1}{\\frac{\\partial \\mu _i}{\\partial \\theta _i}}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}} \\newline \u0026amp;=\\sum_{i=1}^n{\\left( y_i-\\dot{b}\\left( \\theta _i \\right) \\right) \\frac{1}{a\\left( \\phi _i \\right) \\frac{\\partial \\mu _i}{\\partial \\theta _i}}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}}=\\sum_{i=1}^n{\\left( y_i-\\mu _i \\right) \\frac{1}{Var\\left( y_i \\right)}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}} \\newline \u0026amp;=\\left( \\frac{\\partial \\mu _1}{\\partial \\beta},\\cdots ,\\frac{\\partial \\mu _n}{\\partial \\beta} \\right) \\left( \\begin{matrix} \\frac{1}{Var\\left( y_1 \\right)}\u0026amp;\t\u0026amp;\t\\newline \u0026amp;\t\\ddots\u0026amp;\t\\newline \u0026amp;\t\u0026amp;\t\\frac{1}{Var\\left( y_n \\right)}\\newline \\end{matrix} \\right) \\left( \\begin{array}{c} y_1-\\mu _1\\newline \\newline y_n-\\mu _n\\newline \\end{array} \\right) \\newline \u0026amp;=\\frac{\\partial \\mu ^{\\mathrm{T}}}{\\partial \\beta}V^{-1}\\left( y-\\mu \\right) =\\left( \\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\newline \u0026amp;=D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\end{split} $$\n显然，这和之前求解得到的得分函数形式是一致的。为了得到具体的表达式，需要进一步求解 $D$ 的具体形式。\n因为 $D=\\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}}$，而 $x_{i}^{\\mathrm{T}}\\beta =\\eta _i=g\\left( \\mu _i \\right) $，考虑到链接函数 $g(\\cdot)$ 是单调可微函数，则其反函数存在，不妨设为 $h(\\cdot)$，所以\n$$ \\frac{\\partial \\mu _i}{\\partial \\beta}=\\frac{\\partial}{\\partial \\beta}h\\left( x _{i}^{\\mathrm{T}}\\beta \\right) =\\dot{h}\\left( x _{i}^{\\mathrm{T}}\\beta \\right) x_i $$\n进而\n$$ \\begin{split} D\u0026amp;=\\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}}=\\left( \\frac{\\partial \\mu}{\\partial \\beta _1},\\cdots ,\\frac{\\partial \\mu}{\\partial \\beta _n} \\right) =\\left( \\dot{h}\\left( x _{1}^{\\mathrm{T}}\\beta \\right) x_1,\\cdots ,\\dot{h}\\left( x _{n}^{\\mathrm{T}}\\beta \\right) x _n \\right) \\newline \u0026amp;=\\left( \\begin{matrix} \\dot{h}\\left( x _{1}^{\\mathrm{T}}\\beta \\right)\u0026amp;\t\u0026amp;\t\\newline \u0026amp;\t\\ddots\u0026amp;\t\\newline \u0026amp;\t\u0026amp;\t\\dot{h}\\left( x _{n}^{\\mathrm{T}}\\beta \\right)\\newline \\end{matrix} \\right) \\left( x _1,\\cdots ,x _n \\right) \\newline \u0026amp;=\\Delta X \\end{split} $$\n所以得到 $S(\\beta)$ 的最终表达式为\n$$ S\\left( \\beta \\right) =\\left( \\Delta X \\right) ^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) =X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) $$\n为了得到极大似然估计，我们接着求解对应的Fisher信息矩阵。根据定义有：\n$$ I\\left( \\beta \\right) =E\\left( \\frac{\\partial}{\\partial \\beta ^{\\mathrm{T}}}S\\left( \\beta \\right) \\right) =E\\left( \\frac{\\partial ^2\\ln \\mathscr{L}}{\\partial \\beta ^{\\mathrm{T}}\\partial \\beta} \\right) $$\n根据指数族的性质有\n$$ E\\left( \\frac{\\partial ^2\\ln \\mathscr{L}}{\\partial \\beta ^{\\mathrm{T}}\\partial \\beta} \\right) =-E\\left( SS^{\\mathrm{T}} \\right) $$\n考虑其第 $(i,j)$ 个元素 $S_{ij}=S_iS_j$，则\n$$ \\begin{split} E\\left( S_iS_j \\right) \u0026amp;=E\\left[ x _{i}^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) x _{j}^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\right] \\newline \u0026amp;=E\\left[ x _{i}^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\left( y-\\mu \\right) ^{\\mathrm{T}}V^{-1}\\Delta x _j \\right] \\newline \u0026amp;=x _{i}^{\\mathrm{T}}\\Delta V^{-1}\\Delta x _j \\end{split} $$\n所以\n$$ I\\left( \\beta \\right) =X^{\\mathrm{T}}\\Delta V^{-1}\\Delta X $$\n这时，再回到头考虑得分函数，根据极值的必要条件有\n$$ S\\left( \\hat{\\beta} \\right) =X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) =0 $$\n将上式在真值 $\\beta$ 处进行Taylor展开\n$$ S\\left( \\hat{\\beta} \\right) =S\\left( \\beta \\right) +\\frac{\\partial}{\\partial \\beta ^{\\mathrm{T}}}S\\left( \\beta \\right) \\left( \\hat{\\beta}-\\beta \\right) =0 $$\n所以\n$$ \\hat{\\beta}=\\beta +\\left( -\\frac{\\partial S\\left( \\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{-1}S\\left( \\beta \\right) $$\n其中括号内的一项过于复杂，在实际求解中可以根据大数定律用其期望代替，由前面的讨论可知其期望即为 $I(\\beta)$。这样，模型的极大似然估计可以近似成\n$$ \\begin{split} \\hat{\\beta}\u0026amp;\\doteq \\beta +\\left( X^{\\mathrm{T}}\\Delta V^{-1}\\Delta X \\right) ^{-1}X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\newline \u0026amp;=\\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\end{split} $$\n2.3 渐近正态性 根据得到的估计量的近似表达式，其中等式右侧只有 $y$ 是服从正态分布的随机变量，那么得到的参数估计可以看成正态随机变量一个线性组合，显然估计量也服从正态分布，故而只需要求其均值和方差即可。\n$\\hat{\\beta}$ 的均值\n$$ E\\hat{\\beta}=E\\left[ \\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\right] =\\beta $$\n$\\hat{\\beta}$ 的方差\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left[ \\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\right] \\newline \u0026amp;=Var\\left[ I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\right] \\newline \u0026amp;=I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}Var\\left( y-\\mu \\right) V^{-1}DI\\left( \\beta \\right) ^{-1} \\newline \u0026amp;=I\\left( \\beta \\right) ^{-1}\\left( D^{\\mathrm{T}}V^{-1}D \\right) I\\left( \\beta \\right) ^{-1} \\newline \u0026amp;=I\\left( \\beta \\right) ^{-1} \\end{split} $$\n所以，可知估计量服从 $N\\left( \\beta ,I\\left( \\beta \\right) ^{-1} \\right)$。有了渐近分布，我们就能构造显著性检验了。\n2.4 拟似然方法 广义线性模型中对于分布有前提假设，但是实际问题中并不知道具体的分布是什么。考虑到在求解问题中一个重要的信息就是信息函数，它和分布前两阶矩有关，故而可以假设分布的前两阶矩存在，从而推出不含分布信息的拟似然方法。\n假定响应变量 $y_i$ 的均值为 $\\mu_i$，方差函数为 $\\mathrm{Var}y_i=a\\left( \\phi \\right) \\mathrm{Var} _{\\mu _i}$，并且假定均值 $\\mu_i$ 和 $x_i^T\\beta$ 之间存在链结函数，链结函数的性质和广义线性模型的链结函数性质一样。据此，根据广义线性模型的思想实施拟似然方法。\n构造逆得分函数\n$$ S_i\\left( \\mu _i \\right) =\\frac{y_i-\\mu _i}{\\mathrm{Var}y_i} $$\n它满足\n$$ \\begin{cases} ES_i\\left( \\mu _i \\right) =0\u0026amp;\t\\newline ES _{i}^{2}=E\\left( -\\frac{\\partial S_i}{\\partial \\mu _i} \\right)\u0026amp;\t\\newline \\end{cases} $$\n其中第二条性质是因为\n$$ \\begin{split} E\\left( -\\frac{\\partial S_i}{\\partial \\mu _i} \\right) \u0026amp;=-E\\left( \\frac{\\partial}{\\partial \\mu _i}\\left( \\frac{y_i-\\mu _i}{\\mathrm{Var}y_i} \\right) \\right) \\newline \u0026amp;=\\frac{1}{\\mathrm{Var}y_i}=E\\left( S _{i}^{2} \\right) \\end{split} $$\n所以根据广义线性模型极大似然思想有\n$$ \\theta _i=\\int _{y_i}^{\\mu _i}{S_i\\left( t \\right) \\mathrm{d}t}, \\quad \\theta \\left( \\mu \\right) =\\sum _{i=1}^n{\\theta \\left( \\mu _i \\right)}=\\theta \\left( \\beta \\right) $$\n那么拟似然方法的得分函数可以写成\n$$ S\\left( \\beta \\right) =\\frac{\\partial}{\\partial \\beta}\\theta \\left( \\beta \\right) =\\sum_{i=1}^n{\\frac{\\partial \\theta _i}{\\partial \\beta}=\\sum_{i=1}^n{\\frac{\\partial \\mu _{i}^{\\mathrm{T}}}{\\partial \\beta}\\frac{\\partial \\theta _i}{\\partial \\mu _i}=D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right)}} $$\n类似的可以得到Fisher信息矩阵为\n$$ I\\left( \\beta \\right) =D^{\\mathrm{T}}V^{-1}D $$\n3 迭代求解算法 考虑模型的极大似然估计，其中括号内的一项过于复杂，在实际求解中可以根据大数定律用其期望代替，由前面的讨论可知其期望即为 $I(\\beta)$。这样，模型的极大似然估计可以近似成\n$$ \\begin{split} \\hat{\\beta}\u0026amp;\\doteq \\beta +\\left( X^{\\mathrm{T}}\\Delta V^{-1}\\Delta X \\right) ^{-1}X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\newline \u0026amp;=\\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\end{split} $$\n这样就得到了估计量的求解迭代公式。\n4 模拟分析 在R中，我们有glm函数求解广义线性模型。这里，我结合前面的分析自己编写的相应的参数估计和假设检验的函数，包括二项分布（逻辑回归）、Poisson分布。这些代码附在最后供参考。\n以Poisson分布举例，考虑符合Poisson分布的观测，其中链接函数为\n$$ \\eta _i=\\ln \\mu _i $$\n我们生成 $\\mathbb{R}^{1000\\times 5}$ 的设计矩阵 $X$，系数 $\\beta =\\left( 1,3,2,4,5 \\right) ^{\\mathrm{T}}$，使用自编的广义线性模型求解算法求解该模型得到\n这里的myglm是我自编的广义线性模型求解函数，其中囊括了线性模型、二项分布模型（逻辑回归）和Poisson模型的参数估计和显著性检验。对比一下R中的glm包求解的结果\n可以看到结果几乎是一样的。\n5 自编函数代码 5.1 模型求解 实际使用，主要运行该代码即可。这个代码整合了参数估计、检验等代码，将结果合并输出。\n## This program is to solve the generalized linear models: logistic, poisson myglm \u0026lt;- function(x,y,b0,alpha = 0.05,family = \u0026quot;poisson\u0026quot;){# options(digits = 4) # family has two choice: logistic and poisson # alpha is the significance of the interval estimate of the coefficients # solve the model if(family == \u0026quot;logistic\u0026quot;){ glm.result \u0026lt;- myglmlogistic(x,y,beta1 = b0) } else if(family == \u0026quot;poisson\u0026quot;){ glm.result \u0026lt;- myglmpoisson(x,y,beta1 = b0) } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic,poisson之一\u0026quot;)) } b \u0026lt;- glm.result$模型的解 y.fit \u0026lt;- glm.result$拟合值 num \u0026lt;- glm.result$算法迭代次数 # hypothesis testing indivi \u0026lt;- myglmindivi(x,y,b,expr = family) # confidence interval inter.sig \u0026lt;- myglminterval(x,y,b,alpha = alpha,expr = family) bname \u0026lt;- paste0(rep(\u0026quot;beta\u0026quot;,length(b)),1:length(b)) # the residuls # glm.residual \u0026lt;- y - y.fit # res1 \u0026lt;- summary(glm.residual) # names(res1) \u0026lt;- c(\u0026quot;最小值\u0026quot;,\u0026quot;下四分位数\u0026quot;,\u0026quot;中位数\u0026quot;,\u0026quot;均值\u0026quot;,\u0026quot;上四分位数\u0026quot;,\u0026quot;最大值\u0026quot;) estimate.test \u0026lt;- data.frame( \u0026quot;估计值\u0026quot; = b, \u0026quot;下界\u0026quot; = inter.sig[,2], \u0026quot;上界\u0026quot; = inter.sig[,3], \u0026quot;z值\u0026quot; = indivi$Tu, \u0026quot;P值(\u0026gt;|z|)\u0026quot; = indivi$Tp, \u0026quot;置信度\u0026quot; = indivi$Ts,check.names = F ) # output the results cat(\u0026quot;\\n\u0026quot;) cat(\u0026quot;Call: 这是不带截距项的\u0026quot;,family,\u0026quot;模型,\u0026quot;,\u0026quot;下面是模型的分析结果：\u0026quot;,seq = \u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;\\n\u0026quot;) cat(\u0026quot;参数估计结果:\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;\\n\u0026quot;) print(estimate.test) cat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;置信度: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;区间估计的置信水平为：\u0026quot;,alpha,\u0026quot;\\n\u0026quot;) cat(\u0026quot;---\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;Fisher信息矩阵的迭代次数为:\u0026quot;,num,\u0026quot;次\u0026quot;,\u0026quot;\\n\u0026quot;) }  5.2 Logistic模型的解 ##本程序用来求解广义线性模型中——logistic模型的解 myglmlogistic \u0026lt;- function(x,y,beta1,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) x \u0026lt;- as.matrix(x) y \u0026lt;- as.matrix(y) g \u0026lt;- expression(log(mu/(1 - mu)))# the link function h \u0026lt;- expression(1/(1 + exp(-eta)))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(log(1 + exp(theta)))# the b(theta) function of the pdf db \u0026lt;- D(b,\u0026quot;theta\u0026quot;)# the first derivative funtion of b ddb \u0026lt;- D(db,\u0026quot;theta\u0026quot;)# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu/(1 - mu)))# the inverse of db k \u0026lt;- 1 beta0 \u0026lt;- beta1 + 1 while(sum((beta0 - beta1)^2) \u0026gt;= e){ beta0 \u0026lt;- beta1 # compute the D matrix eta \u0026lt;- x%*%beta1 delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x # compute v.inverse—the inverse matrix of the covariance matrix of y mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb))) p1 \u0026lt;- solve(t(D)%*%v.inverse%*%D) p2 \u0026lt;- t(D)%*%v.inverse%*%delta p3 \u0026lt;- x%*%beta1 + v.inverse%*%(y - mu) beta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(beta1) break } else{ k \u0026lt;- k + 1 } } colnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1)) eta \u0026lt;- x%*%beta1 y.fit \u0026lt;- eval(h)# compute the fitting values of y glmlogistic.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1,\u0026quot;拟合值\u0026quot; = y.fit) return(glmlogistic.result) }  5.3 Poisson模型的解 ##本程序用来求解广义线性模型中——Poisson模型的解 myglmpoisson \u0026lt;- function(x,y,beta1,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) x \u0026lt;- as.matrix(x) y \u0026lt;- as.matrix(y) g \u0026lt;- expression(log(mu))# the link function h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(exp(theta))# the b(theta) function of the pdf db \u0026lt;- expression(exp(theta))# the first derivative funtion of b ddb \u0026lt;- expression(exp(theta))# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu))# the inverse of db k \u0026lt;- 1 beta0 \u0026lt;- beta1 + 1 while(sum((beta0 - beta1)^2) \u0026gt;= e){ beta0 \u0026lt;- beta1 # compute the D matrix eta \u0026lt;- x%*%beta1 delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x # compute v.inverse—the inverse matrix of the covariance matrix of y mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb))) p1 \u0026lt;- solve(t(D)%*%v.inverse%*%D) p2 \u0026lt;- t(D)%*%v.inverse%*%delta p3 \u0026lt;- x%*%beta1 + v.inverse%*%(y - mu) beta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(beta1) break } else{ k \u0026lt;- k + 1 } } colnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1)) eta \u0026lt;- x%*%beta1 y.fit \u0026lt;- eval(h)# compute the fitting values of y glmpoisson.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1,\u0026quot;拟合值\u0026quot; = y.fit) return(glmpoisson.result) }  5.4 拟似然 ## ##This program is quasi-likelihood method ## myquasimle \u0026lt;- function(x,y,beta1,family,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) x \u0026lt;- as.matrix(x) y \u0026lt;- as.matrix(y) v.mu \u0026lt;- expression(mu^2)# The covariance function of y if(family == \u0026quot;possion\u0026quot;){ h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h } else if(family == \u0026quot;logistic\u0026quot;){ h \u0026lt;- expression(1/(1 + exp(-eta))) dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;) } else{ print(\u0026quot;分布参数错误, 请选择logistic或者possion分布！！！！\u0026quot;) } k \u0026lt;- 1 beta0 \u0026lt;- beta1 + 1 while(sum((beta0 - beta1)^2) \u0026gt;= e){ beta0 \u0026lt;- beta1 # compute the D matrix eta \u0026lt;- x%*%beta1 delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x # compute v.inverse—the inverse matrix of the covariance matrix of y mu \u0026lt;- eval(h) v.inverse \u0026lt;- diag(1/as.vector(eval(v.mu))) p1 \u0026lt;- solve(t(D)%*%v.inverse%*%D) p2 \u0026lt;- t(D)%*%v.inverse%*%delta p3 \u0026lt;- x%*%beta1 + v.inverse%*%(y - mu) beta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(beta1) break } else{ k \u0026lt;- k + 1 } } colnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1)) eta \u0026lt;- x%*%beta1 y.fit \u0026lt;- eval(h)# compute the fitting values of y quasi.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1,\u0026quot;拟合值\u0026quot; = y.fit) return(quasi.result) }  5.5 单个系数检验 ## 求解广义线性模型单个系数检验 myglmindivi \u0026lt;- function(x,y,b,expr){# # expr 表示模型：logistic, poisson # b 表示由广义线性模型估计得到的估计量 n \u0026lt;- length(x) p \u0026lt;- length(b) beta.hat \u0026lt;- b D \u0026lt;- matrix(0, nrow = n, ncol = p) # get the D matrix if(expr == \u0026quot;logistic\u0026quot;){ h \u0026lt;- expression(1/(1 + exp(-eta)))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(log(1 + exp(theta)))# the b(theta) function of the pdf db \u0026lt;- D(b,\u0026quot;theta\u0026quot;)# the first derivative funtion of b ddb \u0026lt;- D(db,\u0026quot;theta\u0026quot;)# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu/(1 - mu)))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else if(expr == \u0026quot;poisson\u0026quot;){ h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h db \u0026lt;- expression(exp(theta))# the first derivative funtion of b ddb \u0026lt;- expression(exp(theta))# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic,poisson之一\u0026quot;)) } cc \u0026lt;- solve(t(D)%*%v.inverse%*%D) diagc \u0026lt;- diag(cc) Tt \u0026lt;- 1:p;Tp \u0026lt;- Tt;Ts \u0026lt;- Tt for(i in 1:p){ Tt[i] \u0026lt;- beta.hat[i]/sqrt(diagc[i]) Tp[i] \u0026lt;- 2*(1 - pnorm(Tt[i])) if(Tp[i] \u0026lt; 0.001 ){# 判断置信度 Ts[i] \u0026lt;- c(\u0026quot;***\u0026quot;) } else if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){ Ts[i] \u0026lt;- c(\u0026quot;**\u0026quot;) } else if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){ Ts[i] \u0026lt;- c(\u0026quot;*\u0026quot;) } else if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){ Ts[i] \u0026lt;- c(\u0026quot;.\u0026quot;) } else{ Ts[i] \u0026lt;- c(\u0026quot; \u0026quot;) } } indivitest \u0026lt;- data.frame(\u0026quot;Tu\u0026quot; = Tt,\u0026quot;Tp\u0026quot; = Tp,\u0026quot;Ts\u0026quot; = Ts) return(indivitest) }  5.6 区间估计 ## the confidence interval of the generalized linear model myglminterval \u0026lt;- function(x,y,b,expr,alpha = 0.05){ n \u0026lt;- length(x) p \u0026lt;- length(b) beta.hat \u0026lt;- b D \u0026lt;- matrix(0, nrow = n, ncol = p) # get the D matrix if(expr == \u0026quot;logistic\u0026quot;){ h \u0026lt;- expression(1/(1 + exp(-eta)))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(log(1 + exp(theta)))# the b(theta) function of the pdf db \u0026lt;- D(b,\u0026quot;theta\u0026quot;)# the first derivative funtion of b ddb \u0026lt;- D(db,\u0026quot;theta\u0026quot;)# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu/(1 - mu)))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else if(expr == \u0026quot;poisson\u0026quot;){ h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h db \u0026lt;- expression(exp(theta))# the first derivative funtion of b ddb \u0026lt;- expression(exp(theta))# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic, gompertz, weibull之一\u0026quot;)) } Ti \u0026lt;- matrix(0,nrow = p,ncol = 2) cc \u0026lt;- solve(t(D)%*%v.inverse%*%D) diagc \u0026lt;- diag(cc) for(i in 1:p){ Ti[i,1] \u0026lt;- beta.hat[i] - qnorm((1-alpha/2))*sqrt(diagc[1]) Ti[i,2] \u0026lt;- beta.hat[i] + qnorm((1-alpha/2))*sqrt(diagc[1]) } out \u0026lt;- cbind(beta.hat,Ti) colnames(out) \u0026lt;- c(\u0026quot;Estimator\u0026quot;,\u0026quot;LowerBound\u0026quot;,\u0026quot;UpperBound\u0026quot;) return(out) }  ","id":9,"section":"posts","summary":"1 指数族分布与广义线性模型 1.1 引入指数族分布 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题中 $y$ 并不总是满足正态分布的","tags":["多元统计","回归分析"],"title":"广义线性模型及其一般求解方式","uri":"https://jiandan94.github.io/2018/06/glm/","year":"2018"},{"content":"本文总结了线性模型的主要知识点，分别为参数估计，包括最小二乘估计和极大似然估计，区间估计，假设检验。此外，针对每个内容，本文还给出了相应的R软件求解算法，并做了相应的模拟。\n设在线性模型中：$y\\in \\mathbb{R}^n$，$X\\in \\mathbb{R}^{n\\times p}$，$n$ 表示观测数，$p$ 表示变量数。$\\beta \\in \\mathbb{R}^n$ 表示回归系数。$\\epsilon \\sim N\\left( 0,\\sigma ^2I_n \\right) $，是为独立同分布的高斯随机变量。\n1 估计量的表达式 1.1 最小二乘估计量  有截距项的参数估计：  首先令带有截距项的线性模型表达式为：\n$$ y=1\\beta _0+X\\beta+\\epsilon $$\n根据最小二乘思想可知目标函数为 $f\\left( \\beta _0,\\beta \\right) =\\lVert y-1\\beta _0-X\\beta \\rVert _{2}^{2} $，所以将 $f\\left( \\beta _0,\\beta \\right) $ 展开有：\n$$ \\begin{split} f\\left( \\beta _0,\\beta \\right) =\u0026amp;y^{\\mathrm{T}}y+\\beta _01^{\\mathrm{T}}1\\beta _0+2\\beta ^{\\mathrm{T}}X^{\\mathrm{T}}1\\beta _0 \\newline \u0026amp;+\\beta ^{\\mathrm{T}}X^{\\mathrm{T}}X\\beta -2\\beta _01^{\\mathrm{T}}y-2y^{\\mathrm{T}}X\\beta \\end{split} $$\n然后分别对 $\\beta_0$，$\\beta$ 求偏导并令为零：\n$$ \\begin{split} \\frac{\\partial f}{\\beta _0}\u0026amp;=2\\cdot 1^{\\mathrm{T}}1\\beta _0+2\\beta ^{\\mathrm{T}}X ^{\\mathrm{T}}1-2y ^{\\mathrm{T}}1=0 \\newline \\frac{\\partial f}{\\beta }\u0026amp;=2X^{\\mathrm{T}}1\\beta _0+2X^{\\mathrm{T}}X\\beta -2X ^{\\mathrm{T}}y=0 \\end{split} $$\n由上式第1式求得：$\\hat{\\beta}_0=\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta =\\bar{y}-\\bar{x}^{\\mathrm{T}}\\hat{\\beta} $，其中 $\\bar{X}=\\left( \\bar{x}_1,\\cdots ,\\bar{x}_p \\right) ^{\\mathrm{T}}$ 表示由 $X$ 的每一列的均值组成的列向量。然后将求得的结果带入到第2式中，有：\n$$ X^{\\mathrm{T}}1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-X^{\\mathrm{T}}1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta +X^{\\mathrm{T}}X\\beta -X^{\\mathrm{T}}y=0 $$\n整理可得：\n$$ -X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] y+X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] X\\beta =0 $$\n注意到矩阵 $I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}$ 是对称幂等的，并且 $\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X$ 可以看成是 $X$ 的每个元素减去所在列的均值得到的新矩阵（就是每一列元素进行中心化）。则记\n$$ X_c=\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] X $$\n所以可以得到\n$$ X_{c}^{\\mathrm{T}}X_c\\beta =X_{c}^{\\mathrm{T}}y $$\n故而得到有截距的最小二乘参数估计为：\n$$ \\begin{split} \\hat{\\beta}_0\u0026amp;=\\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\newline \\hat{\\beta}\u0026amp;=\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n对所得到的估计进行分析，显然有：\n$$ \\begin{split} E\\hat{\\beta}\u0026amp;=E\\left[ \\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}y \\right] \\newline \u0026amp;=E\\left[ \\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}\\left( 1\\beta _0+X\\beta +\\epsilon \\right) \\right] \\newline \u0026amp;=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{}^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) \\left( 1\\beta _0+X\\beta \\right) \\newline \u0026amp;=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{}^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X\\beta \\newline \u0026amp;=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}X_c\\beta \\newline \u0026amp;=\\beta \\end{split} $$\n这里利用了 $\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) $ 对称幂等的性质。同理有\n$$ \\begin{split} E\\hat{\\beta}_0\u0026amp;=E\\left( \\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\right) \\newline \u0026amp;=\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}Ey-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta \\newline \u0026amp;=\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}\\left( 1\\beta _0+X\\beta \\right) -\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta \\newline \u0026amp;=\\beta _0 \\end{split} $$\n因此 $\\beta_0$，$\\beta$ 都是无偏估计量。 下面考虑他们的方差，则有\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left( \\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}y \\right) \\newline \u0026amp;=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}Var\\left( y \\right) X_{c}^{}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\end{split} $$\n和\n$$ \\begin{split} Var\\left( \\hat{\\beta}_0 \\right) \u0026amp;=Var\\left( \\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\right) \\newline \u0026amp;=Var\\left[ \\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right] \\newline \u0026amp;=\\frac{\\sigma ^2}{n^2}\\left[ 1^{\\mathrm{T}}1-0-0+1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X^{\\mathrm{T}}1 \\right] \\newline \u0026amp;=\\sigma ^2\\left( \\frac{1}{n}+\\bar{X}^{\\mathrm{T}}\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\bar{X} \\right) \\end{split} $$\n上式只需注意到 $1^{\\mathrm{T}}X_{c}=1^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X=0$ 即可。最后，计算 $\\beta_0$ 和 $\\beta$ 之间的协方差：\n$$ \\begin{split} Cov\\left( \\hat{\\beta}_0,\\hat{\\beta} \\right) \u0026amp;=Cov\\left( \\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta},\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right) \\newline \u0026amp;=\\sigma ^2\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}\\left( I-X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\right) X _{c}\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}\\left( X _{c}\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1}-X\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1} \\right) \\newline \u0026amp;=-\\sigma ^2\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1} \\newline \u0026amp;=-\\sigma ^2\\bar{X}^{\\mathrm{T}}\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1} \\end{split} $$\n 不带截距项的参数估计：  假设 $\\hat{\\beta}$ 是参数 $\\beta$ 的最小二乘估计量，$\\hat{y}$ 是相应的 $y$ 的最小二乘估计，则根据最小二乘估计的思想，最小化以下目标函数即可：\n$$ \\hat{\\beta}=\\mathrm{arg}\\ \\mathop {\\min} \\limits_\\beta\\lVert y-X\\beta \\rVert _{2}^{2} $$\n令 $f\\left( \\beta \\right) =\\lVert y-X\\beta \\rVert _{2}^{2}$，对 $\\beta$ 进行求导有：\n$$ \\begin{split} \\frac{\\partial f\\left( \\beta \\right)}{\\partial \\beta}\u0026amp;=-2X^{\\mathrm{T}}y+2X^{\\mathrm{T}}X\\beta \\newline \\frac{\\partial ^2f\\left( \\beta \\right)}{\\partial \\beta ^2}\u0026amp;=2X^{\\mathrm{T}}X \\end{split} $$\n根据极值的必要条件，令上式为零有：\n$$ \\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y $$\n显然在求解中并不需要 假定 $\\epsilon$ 一定服从正态分布。\n对所得到的估计进行分析，显然有：\n$$ \\begin{split} E\\hat{\\beta}\u0026amp;=E\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\newline \u0026amp;=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}X\\beta \\newline \u0026amp;=\\beta \\end{split} $$\n因此 $\\hat{\\beta}$ 是无偏估计量。 下面考虑他的方差：\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left( \\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\right) \\newline \u0026amp;=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}Var\\left( y \\right) X\\left( X^{\\mathrm{T}}X \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( X^{\\mathrm{T}}X \\right) ^{-1} \\end{split} $$\n1.2 极大似然估计量 首先给出多元正态联合密度函数公式：\n$$ f\\left( x|\\mu ,\\Sigma \\right) =\\left[ \\left( 2\\pi \\right) ^{\\mathrm{T}}|\\Sigma |^{-\\frac{1}{2}} \\right] \\exp \\left[ -\\frac{1}{2}\\left( x-\\mu \\right) ^{\\mathrm{T}}\\Sigma ^{-\\frac{1}{2}}\\left( x-\\mu \\right) \\right] $$\n其中 $x$ 服从 $N(\\mu, \\Sigma)$ 分布。在本问题中，我们假定 $\\Sigma =\\sigma ^2I$，也就是随机变量是独立同分布于高斯分布的。\n因此，根据极大似然原理得到似然函数：\n$$ \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =\\left( 2\\pi \\sigma ^2 \\right) ^{-\\frac{n}{2}}\\exp \\left[ -\\frac{1}{2\\sigma ^2}\\left( y-X\\beta \\right) ^{\\mathrm{T}}\\left( y-X\\beta \\right) \\right] $$\n取对数得到对数似然函数：\n$$ \\ln \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =-\\frac{n}{2}\\ln \\left( 2\\pi \\sigma ^2 \\right) -\\frac{1}{2\\sigma ^2}\\left( y-X\\beta \\right) ^{\\mathrm{T}}\\left( y-X\\beta \\right) $$\n上式分别对 $\\beta$ 和 $\\sigma^2$ 求偏导：\n$$ \\begin{split} \\frac{\\partial \\ln \\mathscr{L}}{\\partial \\beta}\u0026amp;=-\\frac{1}{2\\sigma ^2}\\left( -2X^{\\mathrm{T}}y+X^{\\mathrm{T}}X\\beta \\right) =0 \\newline \\frac{\\partial \\ln \\mathscr{L}}{\\partial \\sigma ^2}\u0026amp;=-\\frac{n}{2\\sigma ^2}+\\frac{1}{2\\sigma ^4}\\left( y-X\\beta \\right) ^{\\mathrm{T}}\\left( y-X\\beta \\right) \\end{split} $$\n显然根据第1式得到 $\\beta$ 的极大似然估计量为：\n$$ \\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y $$\n并且我们还可以得到 $\\sigma^2$ 的估计量：\n$$ \\hat{\\sigma}^2=\\frac{RSS}{n} $$\n其中 $RSS=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) $ 是残差平方和。\n 从这里可以看出，在正态分布的假设下，最大似然法和最小二乘法的得到的估计量是一致的。 因此，线性模型下使用最小二乘法或者极大似然法估计系数都可以。然而，这种等价性在其他分布类型的回归问题中不成立。\n 2 方差分析 2.1 带截距项的方差分解 首先给出模型总的离差平方和（$SYY$），回归平方和（$SS_{Reg}$）和残差平方和（$RSS$）的定义：\n$$ \\begin{split} SYY\u0026amp;=\\left( y-\\bar{y}1 \\right) ^{\\mathrm{T}}\\left( y-1\\bar{y} \\right) \\newline SS_{Reg}\u0026amp;=\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right) \\newline RSS\u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\end{split} $$\n则对 $SYY$ 有：\n$$ \\begin{split} SYY\u0026amp;=\\left( y-\\hat{y}+\\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y}+\\hat{y}-1\\bar{y} \\right) \\newline \u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) +\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right)+2\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right) \\newline \u0026amp;=SS_{\\mathrm{Re}g}+RSS+2\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\end{split} $$\n考虑带有截距项的模型的参数估计：\n$$ \\begin{split} \\hat{\\beta}_0\u0026amp;=\\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\newline \\hat{\\beta}\u0026amp;=\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n由此可以得到 $\\hat{y}$ 为：\n$$ \\begin{split} \\hat{y}\u0026amp;=1\\hat{\\beta}_0+X\\hat{\\beta} \\newline \u0026amp;=1\\left[ \\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right]+X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \u0026amp;=1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y+\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \u0026amp;=1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y+X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n所以：\n$$ \\begin{split} y-\\hat{y}\u0026amp;=\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] y-X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \\hat{y}-1\\bar{y}\u0026amp;=X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n所以：\n$$ \\begin{split} \\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \u0026amp;=y^{\\mathrm{T}}X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] y \\newline \u0026amp;\\quad-y^{\\mathrm{T}}X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n而 $\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] $ 对称幂等，则有：\n$$ \\begin{split} X _{c}^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] \u0026amp;=X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] ^2 \\newline \u0026amp;=X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] \\newline \u0026amp;=X _{c}^{\\mathrm{T}} \\end{split} $$\n所以得到 $\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right)$。这样我们就得到一个重要的分解恒等式：\n$$ SYY=RSS+SS_{\\mathrm{Reg}} $$\n注意，在按照前面各个方差的定义下，只有带截距项的模型才使得上式成立。 通常的模型都是带有截距的，因此在实际运用中，按照前面定义的不同离差是比较常见的。\n 在本节不同离差的定义下，不带截距项的模型分解恒等式不一定成立。\n 2.2 通用型方差分解式 除了以上形式的方差类型的定义方式外，还有一种模型方差分解的定义：\n$$ \\begin{split} SYY\u0026amp;=y^{\\mathrm{T}}y \\newline SS_{Reg}\u0026amp;=\\hat{y}^{\\mathrm{T}}\\hat{y} \\newline RSS\u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\end{split} $$\n将上式中的 $RSS$ 乘开得到：\n$$ RSS=y^{\\mathrm{T}}y+\\hat{y}^{\\mathrm{T}}\\hat{y}-2y^{\\mathrm{T}}\\hat{y} $$\n考虑不带截距项的模型有 $\\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y$，所以 $\\hat{y}=X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y$，故而有：\n$$ \\begin{split} \\hat{y}^{\\mathrm{T}}\\hat{y}\u0026amp;=y^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\newline \u0026amp;=y^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\newline y^{\\mathrm{T}}\\hat{y}\u0026amp;=y^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y=\\hat{y}^{\\mathrm{T}}\\hat{y} \\end{split} $$\n所以将上式结果带入到 $RSS$ 中得到：\n$$ \\begin{split} RSS\u0026amp;=y^{\\mathrm{T}}y+\\hat{y}^{\\mathrm{T}}\\hat{y}-2y^{\\mathrm{T}}\\hat{y}=y^{\\mathrm{T}}y+\\hat{y}^{\\mathrm{T}}\\hat{y}-2\\hat{y}^{\\mathrm{T}}\\hat{y} \\newline \u0026amp;=y^{\\mathrm{T}}y-\\hat{y}^{\\mathrm{T}}\\hat{y} \\newline \u0026amp;=SYY-SS_{\\mathrm{Re}g} \\end{split} $$\n这样就证明了方差分解公式成立。\n 在本节方差的定义下，带截距项模型的分解恒等式也成立。\n 通过上述分析可以看出，如果是带有截距项的模型，一般按照第一种定义进行分解；如果是不带截距项的模型，则按照式第二种定义进行分解。\n2.3 方差分析表  带截距项的模型  对带截距项的模型有：\n$$ \\begin{split} \\hat{\\beta}_0\u0026amp;=\\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\newline \\hat{\\beta}\u0026amp;=\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n根据2.1节各个离差的定义，假定设计矩阵 $X$ 列满秩且 $\\epsilon \\sim N\\left( 0,\\sigma ^2I_n \\right)$。\n虑总离差平方和 $SYY$：\n$$ \\begin{split} SYY\u0026amp;=\\left( y-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-1\\bar{y} \\right) \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) ^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) y \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) y \\end{split} $$\n注意到 $P_1=1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}$ 对称、幂等且 $I-P_1$ 秩为 $n-1$。对 $y=1\\beta _0+X\\beta+\\epsilon $，容易得到 $y\\sim N\\left( \\hat{\\mu},\\sigma ^2I_n \\right) $，其中 $\\hat{\\mu}=1\\beta _0+X\\beta$，故而：\n$$ \\frac{SYY}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}\\left( I-P_1 \\right) \\left( y/\\sigma \\right) \\sim \\chi _{n-1}^{2}\\left( \\frac{\\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1 \\right) \\hat{\\mu}}{\\sigma ^2} \\right) $$\n考虑残差平方和 $RSS$：\n$$ \\begin{split} RSS \u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\newline \u0026amp;=\\left( y-1\\hat{\\beta}_0-X\\hat{\\beta} \\right) ^{\\mathrm{T}}\\left( y-1\\hat{\\beta}_0-X\\hat{\\beta} \\right) \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-P_1-P _{X_c}+P_1P _{X_c}+P _{X_c}P_1 \\right) y \\end{split} $$\n其中 $P_{X_c}$ 秩为 $p$ 且满足：\n$$ \\begin{split} P_{X_c}\u0026amp;=X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\newline \u0026amp;=\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\newline \u0026amp;=\\left( I-P_1 \\right) X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\end{split} $$\n所以有 $\\left( I-P_1 \\right) P_{X_c}=P_{X_c}$，同理可以验证 $P_{X_c}\\left( I-P_1 \\right) =P_{X_c}$ 也成立。则对 $\\left( I-P_1-P_{X_c}+P_1P_{X_c}+P_{X_c}P_1 \\right) $ 有：\n$$ \\begin{split} \u0026amp;I-P_1-P_{X_c}+P_1P_{X_c}+P_{X_c}P_1 \\newline \u0026amp;=I-\\left( I-P_1 \\right) P_{X_c}-P_1+P _{X_c}P_1 \\newline \u0026amp;=I-P_{X_c}-P_1+P_{X_c}P_1 \\newline \u0026amp;=I-P_1-P_{X_c}\\left( I-P_1 \\right) \\newline \u0026amp;=I-P_1-P_{X_c} \\end{split} $$\n可以验证 $I-P_1-P_{X_c}$ 是对称、幂等的且秩为 $n-p-1$，因此\n$$ \\frac{RSS}{\\sigma ^2}=\\left( \\frac{y}{\\sigma} \\right) ^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\left( \\frac{y}{\\sigma} \\right) \\sim \\chi _{n-p-1}^{2} $$\n这里的卡方分布中心没有偏移是因为\n$$ \\begin{split} \\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \u0026amp;=\\left( 1\\beta _0+X\\beta \\right) ^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\newline \u0026amp;=\\left( \\beta _01^{\\mathrm{T}}-\\beta _01^{\\mathrm{T}}P_1 \\right) -\\beta _01^{\\mathrm{T}}P_{X_c}+\\beta^{\\mathrm{T}}X^{\\mathrm{T}}-\\beta^{\\mathrm{T}}X^{\\mathrm{T}}P_1-\\beta^{\\mathrm{T}}X^{\\mathrm{T}}P_{X_c} \\newline \u0026amp;=0-0+\\beta^{\\mathrm{T}}X^{\\mathrm{T}}\\left( I-P_1 \\right) -\\beta^{\\mathrm{T}}X^{\\mathrm{T}}P_{X_c} \\newline \u0026amp;=0 \\end{split} $$\n考虑回归平方和 $SS_{Reg}$：\n$$ \\begin{split} SS_{Reg}\u0026amp;=\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right) \\newline \u0026amp;=y^{\\mathrm{T}}X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \u0026amp;=y^{\\mathrm{T}}P_{X_c}y \\end{split} $$\n结合 $y\\sim N\\left( \\hat{\\mu},\\sigma ^2I_n \\right)$，$\\hat{\\mu}=1\\beta _0+X\\beta+\\mu$，故而\n$$ \\frac{SS_{Reg}}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}P_{X_c}\\left( y/\\sigma \\right) \\sim \\chi _{p}^{2}\\left( \\frac{\\tilde{\\mu}^{\\mathrm{T}}P _{X_c}\\tilde{\\mu}}{\\sigma ^2} \\right) $$\n 观察到 $\\left( I-P_1-P_{X_c} \\right) \\times P_{X_c}=0$，故而 $RSS$ 和 $SS_{Reg}$ 是独立同分布的。\n 综合上述可得到带截距项的模型方差分析表\n   方差来源 平方和 自由度 $df$ 均方     回归 $SS_{Reg}$ $p$ $SS_{Reg}/p$   残差 $RSS$ $n-p-1$ $RSS/(n-p-1)$   总离差 $SYY$ $n-1$      不带截距的模型  对不带截距项的模型有：\n$$ \\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y $$\n根据式2.2各个离差的定义，假定设计矩阵 $X$ 列满秩且 $\\epsilon \\sim N\\left( 0,\\sigma ^2I_n \\right) $。\n考虑总离差平方和 $SYY$：\n其中 $y=X\\beta +\\epsilon \\sim \\left( \\tilde{\\mu},\\sigma ^2I_n \\right) $，$\\tilde{\\mu}=X\\beta $，故而\n$$ \\frac{SYY}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}\\left( y/\\sigma \\right) \\sim \\chi _{n}^{2}\\left( \\frac{\\tilde{\\mu}^{\\mathrm{T}}\\tilde{\\mu}}{\\sigma ^2} \\right) $$\n考虑残差平方和 $RSS$\n$$ \\begin{split} RSS=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \u0026amp;=\\left( y-X\\hat{\\beta} \\right) ^{\\mathrm{T}}\\left( y-X\\hat{\\beta} \\right) \\newline \u0026amp;=\\left( y-X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\right) ^{\\mathrm{T}}\\left( y-X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\right) \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-P_X \\right) y \\end{split} $$\n其中 $P_X=X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}$ 是空间 $\\mathscr{R}\\left( X \\right) $ 的正交投影算子。根据矩阵知识， $I-P_X$ 对称、幂等且秩为 $n-p$，所以\n$$ \\frac{RSS}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}\\left( I-P_X \\right) \\left( y/\\sigma \\right) \\sim \\chi _{n-p}^{2} $$\n这里的卡方分布也是没有中心偏移的。\n考虑回归平方和 $SS_{Reg}$\n$$ SS_{Reg}=\\hat{y}^{\\mathrm{T}}\\hat{y}=y^{\\mathrm{T}}P_Xy $$\n结合 $y=X\\beta +\\epsilon \\sim N\\left( \\tilde{\\mu},\\sigma ^2I_n \\right)$，$\\tilde{\\mu}=X\\beta +\\mu $，故而\n$$ \\frac{SS_{Reg}}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}P_X\\left( y/\\sigma \\right) \\sim \\chi _{p}^{2}\\left( \\frac{\\tilde{\\mu}^{\\mathrm{T}}P_X\\tilde{\\mu}}{\\sigma ^2} \\right) $$\n 通过上述分析，观察到 $\\left( I-P_X \\right) \\times P_X=0$，故而 $RSS$ 和 $SS_{Reg}$ 是独立同分布的。\n 最后，综合上述可得到不带截距项的模型方差分析表\n   方差来源 平方和 自由度 $df$ 均方     回归 $SS_{Reg}$ $p$ $SS_{Reg}/p$   残差 $RSS$ $n-p$ $RSS/(n-p)$   总离差 $SYY$ $n$     3 统计显著性检验 3.1 模型显著性  带截距项的模型  在这个检验前提下，我们的原假设和备择假设通常是\n$$ \\begin{split} \u0026amp;H_0:\\beta=0 \\newline \u0026amp;H_1:\\beta\\ne 0 \\end{split} $$\n根据前述可知\n$$ \\begin{split} \u0026amp;\\frac{RSS}{\\sigma^2}\\ \\sim \\chi_{n-p-1}^{2} \\newline \u0026amp;\\frac{SS_{Reg}}{\\sigma^2} \\sim \\chi_{p}^{2}\\left( \\frac{\\hat{\\mu}^{\\mathrm{T}}P_{X_c}\\hat{\\mu}}{\\sigma^2} \\right) \\end{split} $$\n其中 $\\hat{\\mu}=1\\beta _0+X\\beta$，故而当原假设成立时有 $\\hat{\\mu}=1\\beta _0$，因此\n$$ \\begin{split} \\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\hat{\\mu}\u0026amp;=\\beta _{0}^{2}1^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) 1 \\newline \u0026amp;=\\beta _{0}^{2}\\left( 1^{\\mathrm{T}}1-1^{\\mathrm{T}}1-1^{\\mathrm{T}}P _{X_c}1 \\right) \\newline \u0026amp;=-\\beta _{0}^{2}1^{\\mathrm{T}}P_{X_c}1 \\end{split} $$\n而对 $1^{\\mathrm{T}}P_{X_c}1$ 有\n$$ \\begin{split} 1^{\\mathrm{T}}P_{X_c}1\u0026amp;=1^{\\mathrm{T}}X_c\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}1 \\newline \u0026amp;=1^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}1 \\newline \u0026amp;=\\left( 1^{\\mathrm{T}}-1^{\\mathrm{T}} \\right) X\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}1 \\newline \u0026amp;=0 \\end{split} $$\n所以 $\\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\hat{\\mu}=0$，同理可知 $\\hat{\\mu}^{\\mathrm{T}}P_{X_c}\\hat{\\mu}=0$，故而在原假设成立的条件下有\n$$ \\begin{split} \\frac{RSS}{\\sigma^2}\\ \u0026amp;\\sim \\chi_{n-p-1}^{2} \\newline \\frac{SS_{Reg}}{\\sigma^2} \u0026amp;\\sim \\chi_{p}^{2} \\end{split} $$\n所以，在 $H_0$ 成立的条件下有\n$$ F_0=\\frac{SS_{Reg}/p}{RSS/\\left( n-p-1 \\right)}\\sim F_{p,n-p-1} $$\n这样当给定置信水平 $\\alpha$，在原假设成立的条件下回归平方和 $SS_{Reg}$ 较小，而残差平方和 $RSS$则较大，因此检验统计量 $F_0$ 就应该较小。那么，拒绝原假设的条件就是 $F_0\\ge F_{\\alpha ,p,n-p-1}$。 这里 $F_{\\alpha ,p,n-p-1}$ 表示上侧 $\\alpha$ 分位数（下同）。\n 不带截距项的模型  在这个检验前提下，我们的原假设和备择假设通常是\n$$ \\begin{split} \u0026amp;H_0:\\beta _1=\\beta _2=\\cdots =\\beta _p=0 \\newline \u0026amp;H_1:\\beta _j\\ne 0,\\mathrm{至少对一}个j\\mathrm{成立} \\end{split} $$\n类似的，当原假设成立时有 $\\hat{\\mu}=0$，则\n$$ \\begin{split} \\frac{RSS}{\\sigma^2}\\ \u0026amp;\\sim \\chi_{n-p}^{2} \\newline \\frac{SS_{Reg}}{\\sigma^2} \u0026amp;\\sim \\chi_{p}^{2} \\end{split} $$\n所以，在原假设成立的条件下有\n$$ F_0=\\frac{SS_{Reg}/p}{RSS/\\left( n-p \\right)}~F_{p,n-p} $$\n拒绝原假设的条件同前。\n3.2 单个系数的检验 单个系数的检验通常和从模型中增删变量有关。一般来说，增加一个额外的变量到模型中去，不会使得回归平方和 减小，也不会使得残差平方和 $SS_{Reg}$ 增大。但是，回归平方和 $SS_{Reg}$ 的一点点增大能否充分的保证在模型中引入了一个冗余变量？我们之所以关心，是因为增加一个冗余变量到模型中确实会增大均方误差，因而将模型的可用性降低了。\n检验任何一个单独的回归系数 $\\beta_j$ 的假设通常是\n$$ \\begin{split} \u0026amp;H_0:\\beta _j=0 \\newline \u0026amp;H_1:\\beta _j\\ne 0 \\end{split} $$\n 带截距项的模型  对带截距项的模型有\n$$ \\hat{\\beta}=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}y $$\n因为 $E\\hat{\\beta}=\\beta$ 和 $Var\\left( \\hat{\\beta} \\right) =\\sigma ^2\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}$，显然有\n$$ \\frac{\\hat{\\beta}_j-\\beta _j}{\\sqrt{\\sigma ^2C _{jj}}}\\sim N\\left( 0,1 \\right) $$\n其中 $C_{jj}$ 是矩阵 $\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}$ 第 个对角元。但是由于一般情况下 $\\sigma^2$ 未知，所以我们再考虑 $RSS$ 有\n$$ \\frac{RSS}{\\sigma ^2}\\ \\sim \\chi _{n-p-1}^{2} $$\n所以有\n$$ \\frac{\\left( n-p-1 \\right) s^2}{\\sigma ^2}\\sim \\chi _{n-p-1}^{2} $$\n其中 $s^2$ 是样本的方差，是可以通过样本计算得到的。那么，在原假设成立的条件下有\n$$ t_0 = \\frac{\\hat{\\beta}_j}{\\sqrt{s^2C _{jj}}} \\sim t _{n-p-1} $$\n因此，给定置信水平 $\\alpha$ 在原假设成立的条件下，上式 $t_0$ 的绝对值就不会特别大。那么，拒绝 $H_0$ 的条件就是：$| t_0 |\u0026gt;t_{\\alpha /2,n-p-1}$。\n对于截距项有\n$$ \\frac{\\hat{\\beta}_0-\\beta _0}{\\sqrt{\\sigma ^2\\left( 1/n+\\bar{X}^{\\mathrm{T}}\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\bar{X} \\right)}}\\sim N\\left( 0,1 \\right) $$\n因此\n$$ t_0 = \\frac{\\hat{\\beta}_j}{\\sqrt{s^2(1/n + \\bar{X} ^{\\mathrm{T}}(X _c^{\\mathrm{T}}X _c)^{-1}\\bar{X})}} \\sim t _{n-p-1} $$\n综述，给定置信水平 $\\alpha$ 在原假设 $H_0$ 成立的条件下，拒绝 $H_0$ 的条件就是：$| t_0 |\u0026gt;t_{\\alpha /2,n-p-1}$。\n 不带截距项的模型  对不带截距项的模型有\n$$ \\hat{\\beta}=\\left( X_{}^{\\mathrm{T}}X_{} \\right) ^{-1}X_{}^{\\mathrm{T}}y $$\n类似的，因为 $E\\hat{\\beta}=\\beta $ 和 $E\\hat{\\beta}=\\beta $，所以\n$$ \\frac{\\hat{\\beta}_j-\\beta_j}{\\sqrt{\\sigma ^2C _{jj}}} \\sim N\\left( 0,1 \\right) $$\n其中 $C_{jj}$ 是矩阵 $(X^{\\mathrm{T}}X)^{-1}$ 第 $j$ 个对角元。但是由于一般情况下 $\\sigma^2$ 未知，所以我们再考虑 $RSS$ 有\n$$ \\frac{RSS}{\\sigma ^2}\\ \\sim \\chi _{n-p}^{2} $$\n所以有\n$$ \\frac{\\left( n-p \\right) s^2}{\\sigma ^2} \\sim \\chi _{n-p}^{2} $$\n其中 $s^2$ 是样本的方差。那么在原假设成立的条件下有\n$$ t_0 = \\frac{\\hat{\\beta}_j}{\\sqrt{s^2C _{jj}}} \\sim t _{n-p} $$\n综述，给定置信水平 $\\alpha$ 在原假设 $H_0$ 成立的条件下，拒绝 $H_0$ 的条件就是：$| t_0 |\u0026gt;t_{\\alpha /2,n-p}$。\n4 区间估计 4.1 单个回归系数的置信区间  带截距项的区间估计  根据3.2节的分析和讨论，对于不带截距项的模型有\n$$ \\frac{\\hat{\\beta}_j-\\beta _j}{\\sqrt{s^2C _{jj}}}\\sim t _{n-p-1} $$\n所以，给定置信水平 $\\alpha$ 下 $\\beta_j$ 置信区间为：\n$$ \\hat{\\beta} _{j} - t _{\\alpha /2,n-p-1}\\sqrt{s^2C _{jj}}\\le \\beta _j\\le \\hat{\\beta}_j+t _{\\alpha /2,n-p-1}\\sqrt{s^2C _{jj}} $$\n 不带截距项的区间估计  类似的，给定置信水平 $\\alpha$ 下 $\\beta_j$ 置信区间为\n$$ \\hat{\\beta} _{j} - t _{\\alpha /2,n-p}\\sqrt{s^2C _{jj}}\\le \\beta _j\\le \\hat{\\beta}_j+t _{\\alpha /2,n-p}\\sqrt{s^2C _{jj}} $$\n4.2 回归系数的联合置信区域 前面的置信区间都是针对单个系数进行的，因此置信水平只对一个区间有效。但是，很多问题中需要让置信水平对所有的区间有效，这就是联合置信区域（simultaneous confidence intervals）。\n 带截距项的模型  由3.2节可知\n$$ \\left( \\hat{\\beta}-\\beta \\right) \\sim N\\left( 0,\\sigma ^2\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\right) $$\n所以\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right)}{\\sigma ^2}\\sim \\chi _{p}^{2} $$\n这里 $\\sigma^2$ 未知，为构造检验统计量再考虑 $RSS$ 有\n$$ \\frac{RSS}{\\sigma ^2} \\sim \\chi _{n-p-1}^{2} $$\n所以得到\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right) /p}{RSS/\\left( n-p-1 \\right)} \\sim F_{p,n-p-1} $$\n那么在置信水平 $\\alpha$ 下，所有的回归系数需要满足\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right) /p}{RSS/\\left( n-p-1 \\right)}\\le F_{\\alpha ,p,n-p-1} $$\n 不带截距项的模型  同理可知，对于不带截距项的模型，在给定置信水平 $\\alpha$ 下全部系数需要满足\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right) /p}{RSS/\\left( n-p-1 \\right)}\\le F_{\\alpha ,p,n-p} $$\n对于前述联合置信区域的不等式描述的是一个椭圆约束区域。 当 $p=2$ 时，这个区域相当简单；但是当 $p\\geq 2$ 时，问题就变得复杂的多。\n5 模拟分析 我在简单回归分析中介绍了R求解线性模型的方法，这里我们根据前面的理论分析结果，用R编写系数求解的函数、单个系数的显著性检验函数。这些代码附在最后供参考。\n我们考虑模型\n$$ y=1\\beta _0+X\\beta+\\epsilon $$\n其中 $\\beta _0=3$，$\\beta=\\left( 1,3,2,1,4,5,2,6 \\right) ^{\\mathrm{T}}$，$y\\in \\mathbb{R}^{100}$。根据前述分析，使用最小二乘法进行估计，在R软件中得到结果如下\n可以看出估计得结果很好，模型和单个系数都通过了检验。\n当然，我们可以对相同的数据使用R软件求解带截距的线性模型，得到的结果如下图所示。可以看出，两者是一致的。\n此外，我们也可以对模型考虑不带截距项的估计。这样得到的结果如下图所示\n对比R软件内置函数求解的结果\n首先，两者得到的结果是一样的。其次，我们发现对于一个本身带有截距项的模型使用不带截距项的方法求解，得到的效果就不好。 因为可以看到变量 $x_4$ 的系数估计没有通过检验。\n我们可以通过计算模型得到的残差，如学生化残差，通过分析残差的分布和对应的正态分布之间的异同，从而对回归结果做出分析和判断。如下图所示，是带截距模型的学生化残差分布图（左）以及不带截距的学生化残差分布图（右）\n从图上可以看出来，使用正确的模型得到的残差分布和正态分布较为接近，说明估计的效果好；而使用错误的模型得到的结果，残差分布和正态分布相差较大，估计的效果不好。\n6 自编函数代码 6.1 模型求解 实际使用，主要运行该代码即可。这个代码整合了参数估计、检验等代码，将结果合并输出。\n##求解线性模型\rmylm \u0026lt;- function(x,y,method = \u0026quot;ols\u0026quot;,intercept = T){#\r# 本程序用来求解线性模型参数估计 区间估计和假设检验\r# method可选：ols,max 前者最小二乘 后者极大似然估计\r# intercept为T表示模型带截距 为F表示模型不带截距 # 求解估计量\rb \u0026lt;- mylmestimate(x,y,method = method,intercept = intercept)\r# 求解离差平方和分解\rrsquare \u0026lt;- mylmsquare(x,y,b,intercept = intercept)\r# 单个系数检验\rindivi \u0026lt;- mylmindivi(x,y,b,intercept = intercept)\r# 模型检验\rlmtest \u0026lt;- mylmtest(x,y,b,intercept = intercept)\rif(intercept){\rbname \u0026lt;- c(\u0026quot;(Intercept)\u0026quot;,paste0(rep(\u0026quot;x\u0026quot;,p),1:p))\r}\relse{\rbname \u0026lt;- paste0(rep(\u0026quot;x\u0026quot;,p),1:p)\r}\restimate.test \u0026lt;- data.frame(\r\u0026quot;Beta\u0026quot; = bname,\r\u0026quot;Estimate\u0026quot; = b,\r\u0026quot;t.value\u0026quot; = indivi$Tt,\r\u0026quot;P.value\u0026quot; = indivi$Tp,\r\u0026quot;Sig.\u0026quot; = indivi$Ts\r)\r# 输出结果\rif(intercept){\rcat(\u0026quot;Coefficients:\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rprint(estimate.test)\rcat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;Multiple R-squared:\u0026quot;,(rsquare$sreg/rsquare$syy),\u0026quot;\u0026quot;)\rcat(\u0026quot;Adjusted R-squared:\u0026quot;,(1-(rsquare$rss*(n-1))/(rsquare$syy*(n-p-1))),\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;F-statistic:\u0026quot;,lmtest$f0,\u0026quot;\u0026quot;)\rcat(\u0026quot;on\u0026quot;,p,\u0026quot;\u0026quot;)\rcat(\u0026quot;and\u0026quot;,(n-p-1),\u0026quot;\u0026quot;)\rcat(\u0026quot;DF,\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\u0026quot;)\rcat(\u0026quot;p-value:\u0026quot;,lmtest$p,\u0026quot;\u0026quot;)\r}\relse{\rcat(\u0026quot;Coefficients:\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rprint(estimate.test)\rcat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;Multiple R-squared:\u0026quot;,(rsquare$sreg/rsquare$syy),\u0026quot;\u0026quot;)\rcat(\u0026quot;Adjusted R-squared:\u0026quot;,(1-(rsquare$rss*n)/(rsquare$syy*(n-p))),\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;F-statistic:\u0026quot;,lmtest$f0,\u0026quot;\u0026quot;)\rcat(\u0026quot;on\u0026quot;,p,\u0026quot;\u0026quot;)\rcat(\u0026quot;and\u0026quot;,(n-p),\u0026quot;\u0026quot;)\rcat(\u0026quot;DF,\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\u0026quot;)\rcat(\u0026quot;p-value:\u0026quot;,lmtest$p,\u0026quot;\u0026quot;)\r}\r}\r 6.2 参数估计 ##求解线性模型参数估计\rmylmestimate \u0026lt;- function(x,y,method = \u0026quot;ols\u0026quot;,intercept = T){#\rn \u0026lt;- length(y)\rif(method == \u0026quot;ols\u0026quot;){\rif(intercept){\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\r# 得到最小二乘估计\rb1 \u0026lt;- solve(t(xcenter)%*%xcenter)%*%t(xcenter)%*%y\rb0 \u0026lt;- mean(y) - t(xbar0)%*%b1\rb \u0026lt;- c(b0,b1)\r}\relse{\r# 得到最小二乘估计\rb \u0026lt;- solve(t(x)%*%x)%*%t(x)%*%y\r}\r}\relse{\rif(intercept){\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\r# 得到极大似然估计\rb1 \u0026lt;- solve(t(xcenter)%*%xcenter)%*%t(xcenter)%*%y\rb0 \u0026lt;- mean(y) - t(xbar0)%*%b1\rb \u0026lt;- c(b0,b1)\r}\relse{\r# 得到极大似然估计\rb \u0026lt;- solve(t(x)%*%x)%*%t(x)%*%y\r}\r}\rreturn(b)\r}\r 6.3 假设检验 ##求解线性模型假设检验\rmylmtest \u0026lt;- function(x,y,b,intercept = T){\rn \u0026lt;- dim(x)[1]\rp \u0026lt;- dim(x)[2]\rif(intercept){\rybar \u0026lt;- mean(y)\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值\r# 计算各个离差平方和\rrss \u0026lt;- t((y - yhat))%*%(y - yhat)\rsreg \u0026lt;- t((yhat - ybar))%*%(yhat - ybar)\r# 计算检验统计量\rf0 \u0026lt;- (sreg/p)/(rss/(n - p - 1))\r# 输出p值\rp \u0026lt;- 1 - pf(f0,p,n-p-1)\r}\relse{\ryhat \u0026lt;- x%*%b#计算回归值\r# 计算各个离差平方和\rrss \u0026lt;- t((y - yhat))%*%(y - yhat)\rsreg \u0026lt;- t(yhat)%*%(yhat)\r# 计算检验统计量\rf0 \u0026lt;- (sreg/p)/(rss/(n - p))\r# 输出p值\rp \u0026lt;- 1 - pf(f0,p,n-p)\r}\rmodeltest \u0026lt;- data.frame(\u0026quot;f0\u0026quot; = f0,\u0026quot;p\u0026quot; = p)\rreturn(modeltest)\r}\r 6.4 单个系数检验 ##求解线性模型单个系数检验\rmylmindivi \u0026lt;- function(x,y,b,intercept = T){\rn \u0026lt;- dim(x)[1]\rp \u0026lt;- dim(x)[2]\rif(intercept){\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值\rs \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p-1) #计算MSE\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\rcc \u0026lt;- solve(t(xcenter)%*%xcenter)\rdiagc \u0026lt;- diag(cc)\rTt \u0026lt;- 1:(p+1);Tp \u0026lt;- Tt;Ts \u0026lt;- Tt\rTt[1] \u0026lt;- b[1]/sqrt(s*(1/n + t(xbar0)%*%cc%*%xbar0))\rTp[1] \u0026lt;- 2*(1 - pt(Tt[1],n-p-1))\rif(Tp[1] \u0026lt; 0.001 ){# 判断置信度\rTs[1] \u0026lt;- c(\u0026quot;***\u0026quot;)\r}\relse if(Tp[1] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[1] \u0026gt;= 0.001){\rTs[1] \u0026lt;- c(\u0026quot;**\u0026quot;)\r}\relse if(Tp[1] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[1] \u0026gt;= 0.01){\rTs[1] \u0026lt;- c(\u0026quot;*\u0026quot;)\r}\relse if(Tp[1] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[1] \u0026gt;= 0.05){\rTs[1] \u0026lt;- c(\u0026quot;.\u0026quot;)\r}\relse{\rTs[1] \u0026lt;- c(\u0026quot; \u0026quot;)\r}\rfor(i in 2:(p+1)){\rTt[i] \u0026lt;- b[i]/sqrt(s*diagc[i-1])\rTp[i] \u0026lt;- 2*(1 - pt(Tt[i],n-p-1))\rif(Tp[i] \u0026lt; 0.001 ){# 判断置信度\rTs[i] \u0026lt;- c(\u0026quot;***\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){\rTs[i] \u0026lt;- c(\u0026quot;**\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){\rTs[i] \u0026lt;- c(\u0026quot;*\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){\rTs[i] \u0026lt;- c(\u0026quot;.\u0026quot;)\r}\relse{\rTs[i] \u0026lt;- c(\u0026quot; \u0026quot;)\r}\r}\r}\relse{\ryhat \u0026lt;- x%*%b#计算回归值\rs \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p) #计算MSE\rcc \u0026lt;- solve(t(x)%*%x)\rdiagc \u0026lt;- diag(cc)\rTt \u0026lt;- 1:p;Tp \u0026lt;- Tt;Ts \u0026lt;- Tt\rfor(i in 1:p){\rTt[i] \u0026lt;- b[i]/sqrt(s*diagc[i])\rTp[i] \u0026lt;- 2*(1 - pt(Tt[i],n-p))\rif(Tp[i] \u0026lt; 0.001 ){# 判断置信度\rTs[i] \u0026lt;- c(\u0026quot;***\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){\rTs[i] \u0026lt;- c(\u0026quot;**\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){\rTs[i] \u0026lt;- c(\u0026quot;*\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){\rTs[i] \u0026lt;- c(\u0026quot;.\u0026quot;)\r}\relse{\rTs[i] \u0026lt;- c(\u0026quot; \u0026quot;)\r}\r}\r}\rindivitest \u0026lt;- data.frame(\u0026quot;Tt\u0026quot; = Tt,\u0026quot;Tp\u0026quot; = Tp,\u0026quot;Ts\u0026quot; = Ts)\rreturn(indivitest)\r}\r 6.5 区间估计 ##求解线性模型区间估计\rmylminterval \u0026lt;- function(x,y,b,alpha = 0.05,intercept = T){\rn \u0026lt;- dim(x)[1]\rp \u0026lt;- dim(x)[2]\rif(intercept){\rTi \u0026lt;- matrix(0,nrow = (p+1),ncol = 2)\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值\rs \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p-1) #计算MSE\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\rcc \u0026lt;- solve(t(xcenter)%*%xcenter)\rdiagc \u0026lt;- diag(cc)\rTi[1,1] \u0026lt;- b[1] - qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\rTi[1,2] \u0026lt;- b[1] + qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\rfor(i in 2:(p+1)){\rTi[i,1] \u0026lt;- b[i] - qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\rTi[i,2] \u0026lt;- b[i] + qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\r}\r}\relse{\rTi \u0026lt;- matrix(0,nrow = p,ncol = 2)\ryhat \u0026lt;- x%*%b#计算回归值\rs \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p) #计算MSE\rcc \u0026lt;- solve(t(x)%*%x)\rdiagc \u0026lt;- diag(cc)\rfor(i in 1:p){\rTi[i,1] \u0026lt;- b[i] - qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\rTi[i,2] \u0026lt;- b[i] + qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\r}\r}\rcolnames(Ti) \u0026lt;- c(\u0026quot;LowerBound\u0026quot;,\u0026quot;UpperBound\u0026quot;)\rreturn(Ti)\r}\r 6.6 离差平方和分解 ##求解线性模型离差平方和分解\rmylmsquare \u0026lt;- function(x,y,b,intercept = T){\rif(intercept){\rybar \u0026lt;- mean(y)\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值\r# 计算各个离差平方和\rrss \u0026lt;- t((y - yhat))%*%(y - yhat)\rsreg \u0026lt;- t((yhat - ybar))%*%(yhat - ybar)\rsyy \u0026lt;- t((y - ybar))%*%(y - ybar)\r}\relse{\ryhat \u0026lt;- x%*%b#计算回归值\r# 计算各个离差平方和\rrss \u0026lt;- t((y - yhat))%*%(y - yhat)\rsreg \u0026lt;- t(yhat)%*%(yhat)\rsyy \u0026lt;- t(y)%*%y\r}\rrsquare \u0026lt;- data.frame(\u0026quot;syy\u0026quot; = syy,\u0026quot;rss\u0026quot; = rss,\u0026quot;sreg\u0026quot; = sreg)\rreturn(rsquare)\r}\r 6.7 各种残差 ##求解线性模型各种残差\rmylmresidual \u0026lt;- function(x,y,b,intercept = T){\rn \u0026lt;- dim(x)[1]\rp \u0026lt;- dim(x)[2]\rif(intercept){\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]# 计算回归值\rsig.hat \u0026lt;- t(y - yhat)%*%(y - yhat)/(n-p-1)# 计算方差估计值\r# 计算帽子矩阵对角元\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\rp1 \u0026lt;- matrix(1/n,nrow = n,ncol = n)\rh \u0026lt;- p1 + xcenter%*%solve(t(xcenter)%*%xcenter)%*%t(xcenter)\rdiagh \u0026lt;- diag(h)\r# 计算原始残差\roriginres \u0026lt;- y - yhat\r# 计算standardized residuals\rstandres \u0026lt;- (y - yhat)/sqrt(sig.hat*rep(1,n))\r# 计算student residuals\rstudres \u0026lt;- (y - yhat)/sqrt(sig.hat*(1 - diagh))\r# 计算PRESS residual\rpressres \u0026lt;- (y - yhat)/(1 - diagh)\r}\relse{\ryhat \u0026lt;- x%*%b# 计算回归值\rsig.hat \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p)# 计算方差估计值\r# 计算原始残差\roriginres \u0026lt;- y - yhat\r# 计算帽子矩阵对角元\rh \u0026lt;- x%*%solve(t(x)%*%x)%*%t(x)\rdiagh \u0026lt;- diag(h)\r# 计算standardized residuals\rstandres \u0026lt;- (y - yhat)/sqrt(sig.hat*rep(1,n))\r# 计算student residuals\rstudres \u0026lt;- (y - yhat)/sqrt(sig.hat*(1 - diagh))\r# 计算PRESS residual\rpressres \u0026lt;- (y - yhat)/(1 - diagh)\r}\rlmres \u0026lt;- data.frame(\u0026quot;oringinal.Residual\u0026quot; = originres,\r\u0026quot;Standardized.Residual\u0026quot; = standres,\r\u0026quot;Student.Residual\u0026quot; = studres,\r\u0026quot;PRESS.Residual\u0026quot; = pressres)\rreturn(lmres)\r}\r ","id":10,"section":"posts","summary":"本文总结了线性模型的主要知识点，分别为参数估计，包括最小二乘估计和极大似然估计，区间估计，假设检验。此外，针对每个内容，本文还给出了相应的R","tags":["多元统计","回归分析"],"title":"线性模型的理论与求解","uri":"https://jiandan94.github.io/2018/05/lm/","year":"2018"},{"content":"","id":11,"section":"posts","summary":"","tags":["多元统计"],"title":"非线性模型的参数估计和统计性质","uri":"https://jiandan94.github.io/2018/04/nlm/","year":"2018"},{"content":"惊雷起，\n一霎清明夜雨袭。\n帘外风轻，\n潺潺春语，\n入梦浇睡意。\n心思尽去残花里。\n流水无情空悲寂。\n明朝晓看红湿处，\n杨柳依依，\n梨花风起，\n愁染缙纭堤。\n","id":12,"section":"posts","summary":"惊雷起， 一霎清明夜雨袭。 帘外风轻， 潺潺春语， 入梦浇睡意。 心思尽去残花里。 流水无情空悲寂。 明朝晓看红湿处， 杨柳依依， 梨花风起， 愁染缙纭堤。","tags":["虎溪岁月","诗文"],"title":"夜雨清明","uri":"https://jiandan94.github.io/2018/04/yyqm/","year":"2018"},{"content":"渝州故郡，重庆新府。横断巴楚，环扣水枢。三都之地，嘉陵长江汇韵；四方要冲，名家骚客聚属。都督刘公之雅望，重大建始；校长叶公之懿范，往来鸿儒。禹功有继，文翁留㤖。今哉重大，居985。名师云集，学子仰慕。花香大地，翠拥缙湖。廊腰缦回，林荫杏道真佳境；勾心斗角，诗照长亭好学处。\n余辞乡别离，求学万里。客居虎溪，专业统计。驽钝术浅，常慕鹦鹉之机；乏善可陈，空怀击流之意。所赖数统师亲，传道授业答疑；同窗修睦，直谅多闻明礼。四载光阴荏苒，一处心思凭寄。\n肄业在即，毕设期近。杨虎吾师，富学可亲。教授博导，治学严谨。幸蒙教诲，循循善行。洞火灵犀，醍醐灌顶。师兄师姐，帮助殷殷。评讲文献，指点迷津。心念及此，感激不尽。\n余智术有限，学浅才疏。草创成文，难免谬误。请洒潘江，各倾陆海云尔。\n","id":13,"section":"posts","summary":"渝州故郡，重庆新府。横断巴楚，环扣水枢。三都之地，嘉陵长江汇韵；四方要冲，名家骚客聚属。都督刘公之雅望，重大建始；校长叶公之懿范，往来鸿儒。","tags":["诗文","虎溪岁月"],"title":"本科毕设致谢","uri":"https://jiandan94.github.io/2017/06/bkbszx/","year":"2017"},{"content":"东汉末年，桓灵败政。君失道于百姓；民毋念及“火德”。贪狼盛而紫薇衰，豪强起而暴民乱。权佞当道，途有哀鸿饿殍；英雄逐鹿，有志待价而沽。狼烟四起，烽鼓难息，州吞郡合，而终致天下三分。\n期间名人，不乏有数:盖拥兵自重者，冀州袁绍；勇冠三军者，徐州吕布；衣食祖业者，益州刘璋；风流座客者，荆州刘表。然则公台寡断，奉先乏谋，季玉暗弱，景升志穷……盖以英雄称者，屈指可数矣——江东孙吴，跨江连郡，拒荆刘于江渚，败许曹于赤壁； 汉中刘备，盘踞西蜀，外抚羌夷余力，内安万民有术。此二人宏才雅量，志图天下，可以英雄称焉。然余独爱者，绝非孙刘，唯曹孟德一人而已。\n为操之论，一言难尽。余观夫操之人，亦臣亦君，亦盗亦民；所谓臣君，操之气度，所谓盗民，操之性情。臣君盗民，论其一生，古人云：美玉微瑕。此之余何独慕之。\n先时，宦竖乱朝，外戚干政，黄巾起而荼毒，董贼入而祸乱。惜哉，旧臣不臣，幽泣拥喟于室；新盟虽结，各怀鬼胎于心。操，身非苗裔，本承阉宦之嗣；胸藏鸿图，力尽汉臣之实。所以臣之节气，尽燃于胸，奈何持节守将，徒壁上观。呜呼!谋刀不在，檄文已失；潜龙勿用，养兵屯势。迨及气候稍聚，兵容初整，囊贤达逐汉鹿，挟天子令不臣。柔远能迩，悖(dūn)德允元。若弗尧察舜恭，则音律夺伦，文辞难兴（xìng），四凶作宰，百姓不亲。所以谓操有君度，诚乱世之枭雄。是故英雄不必至贤，而能贤贤，其身未必良将，而善将将。\n及至九锡服銮，出警入跸，左右维诺，庸命卑恭。文若在前，季珪难终，时谓阿瞒窃汉之心昭昭。但帝位尤忝，汉庙未隳。姑妄言之，或操诡谲，扶汉至伪，实则矫诏以便其意欤！抑或留待子嗣，登阁僭位，旧臣切念新恩，以期戮力而为。虽然，盗名之性，穷口莫辩。使操后观司马事，未知其意何如?噫!奈何天不假年，痼疾难消，病笃岌危，梦魇复绕。未尝嘱白帝之托，恍若山野小民，而忧妻妾营生。盖戎马峥峥寿尽，富贵闻达浮云，岂不闻鸟之将死，其鸣也哀；人之将死，其情也善。\n嗟乎， 余辞乡别离，求学万里，毕业将近，投笔无期。驽钝术浅，尝慕鹦鹉之机；乏善可陈，空怀击流之意。今复观三国旧事，聊以赋文，暂长精神而已。\n","id":14,"section":"posts","summary":"东汉末年，桓灵败政。君失道于百姓；民毋念及“火德”。贪狼盛而紫薇衰，豪强起而暴民乱。权佞当道，途有哀鸿饿殍；英雄逐鹿，有志待价而沽。狼烟四起","tags":["虎溪岁月","诗文"],"title":"曹操赋","uri":"https://jiandan94.github.io/2017/03/ccf/","year":"2017"},{"content":"渝州冬早立，近日尚秋衣。\n今明披绣闼，始觉冬意袭。\n瑟瑟西风浸，萧萧梧叶稀。\n是处斑驳景，唯独缙纭堤。\n擎伞盖已去，漏春仍昨夕。\n相对相忘言，只道无和羲。\n","id":15,"section":"posts","summary":"渝州冬早立，近日尚秋衣。 今明披绣闼，始觉冬意袭。 瑟瑟西风浸，萧萧梧叶稀。 是处斑驳景，唯独缙纭堤。 擎伞盖已去，漏春仍昨夕。 相对相忘言，只道无和","tags":["虎溪岁月","诗文"],"title":"冬日过缙纭堤","uri":"https://jiandan94.github.io/2016/11/drgjyd/","year":"2016"},{"content":"蓝桥古驿无踪觅，\n不见崔郎遇云英。\n可怜浆向虽易乞，\n难容相访饮牛津。\n","id":16,"section":"posts","summary":"蓝桥古驿无踪觅， 不见崔郎遇云英。 可怜浆向虽易乞， 难容相访饮牛津。","tags":["虎溪岁月","诗文"],"title":"读纳兰词","uri":"https://jiandan94.github.io/2016/11/dnlc/","year":"2016"},{"content":"银杏叶黄暑气收，\n天凉月高怕登楼。\n怕登楼。\n一见鸿书销眉愁。\n夜浣碧纱掩薄秋，\n累君多奔走，\n愿撷晚霞作兰舟。\n君言舟摇珠帘招，\n风也飘飘雨潇潇。\n雨潇潇。\n赌书廊前有陈雕。\n我抱长琴在虹桥，\n银字流水调，\n不教红樱负绿蕉。\n","id":17,"section":"posts","summary":"银杏叶黄暑气收， 天凉月高怕登楼。 怕登楼。 一见鸿书销眉愁。 夜浣碧纱掩薄秋， 累君多奔走， 愿撷晚霞作兰舟。 君言舟摇珠帘招， 风也飘飘雨潇潇。 雨潇潇。","tags":["虎溪岁月","诗文"],"title":"过银杏道","uri":"https://jiandan94.github.io/2016/10/gyxd/","year":"2016"},{"content":"骤雨如烟喧小楼，\n群芳摇落意无休。\n望断巴山犹未尽，\n点点雨丝点点愁。\n","id":18,"section":"posts","summary":"骤雨如烟喧小楼， 群芳摇落意无休。 望断巴山犹未尽， 点点雨丝点点愁。","tags":["虎溪岁月","诗文"],"title":"松园记雨","uri":"https://jiandan94.github.io/2016/06/syjy/","year":"2016"},{"content":"盛夏偏作晚秋天，\n小晴忽复雨涟涟。\n一任灯花自消落，\n辗转心思顾影怜。\n","id":19,"section":"posts","summary":"盛夏偏作晚秋天， 小晴忽复雨涟涟。 一任灯花自消落， 辗转心思顾影怜。","tags":["虎溪岁月","诗文"],"title":"夏日有感","uri":"https://jiandan94.github.io/2016/05/xryg/","year":"2016"},{"content":" 最喜欢这首诗！也许我天生就是一个懒散的人吧~\n 独坐轩窗向小楼，\n斜阳灼灼木幽幽。\n懒理红尘烦扰事，\n遥映菱花慢梳头。\n","id":20,"section":"posts","summary":"最喜欢这首诗！也许我天生就是一个懒散的人吧~ 独坐轩窗向小楼， 斜阳灼灼木幽幽。 懒理红尘烦扰事， 遥映菱花慢梳头。","tags":["虎溪岁月","诗文"],"title":"无题","uri":"https://jiandan94.github.io/2016/05/wt/","year":"2016"},{"content":"北风呼号雪飘飘，一人在家甚无聊。\n前日刚说有雨雪，明朝又要防寒潮。\n漫将闲书消冷寂，且煮陈茶度寒宵。\n无意庭院凄风紧，腊梅残落竟逍遥。\n","id":21,"section":"posts","summary":"北风呼号雪飘飘，一人在家甚无聊。 前日刚说有雨雪，明朝又要防寒潮。 漫将闲书消冷寂，且煮陈茶度寒宵。 无意庭院凄风紧，腊梅残落竟逍遥。","tags":["虎溪岁月","诗文"],"title":"初雪","uri":"https://jiandan94.github.io/2016/01/cx/","year":"2016"},{"content":"说起虎溪的冬天，第一反应也许就是那些浸淫着寒风冷雨的灰色画面撺掇起来的时节；因为就是在整个渝州，晴朗的日子也是很少遇上的。\n可是今早起床，掀开窗幔，扑面袭来的竟是暖心的惊喜——阳光！连雄姿都快消磨殆尽的松树们也开始抖擞起精神，于我来说，这不可不算作不能和别处的孩子为着第一场雪的到来而振奋的补偿吧。虽然，期末考已经提上日程，七零八碎的琐事充斥在举手投足间，但是眼睁睁地无视这来之不易的阳光，应该堪称“暴殄天物”了！一把椅子，一本卢梭的《社会契约论》，半杯纯咖啡足矣，短暂遗忘了整个世界的我坐在阳台上只对着眼前这暖暖的阳光，不禁想到Shelly 说的那句\u0026quot;A man,to be greatly good,must imagine intensively and comprehensively;he must put himself in the place of another and of many others;the pains and pleasures of his species become his own.\u0026rdquo;\n晚饭特地比往常早吃许久，为的是趁着冬阳的余温重读一番连日阴雨掩藏下的虎溪。我最爱在缙湖湖畔悠然漫步，却不是恋那葱茏依旧的杨柳草木，也不是想那整日怡然自得的天鹅锦鲤，为的是面前的一湖荷花（叶）。\n来虎溪也已经一年有余，习惯了麻辣刺激的重庆火锅，习惯了潮湿阴晦的天气，将将不能习惯这界限不甚分明的春夏秋冬。你看不到时间的更迭，更读不懂生命的意义。只有在这儿——缙湖，从这一池的荷叶中才能看到四季的流转，从“小荷才露尖尖角”到“接天莲叶无穷碧”，再到“荷尽已无擎雨盖”；你能深深地感受着生命的呼吸。也许是耗尽了最后一丝春夏蓄积的精华，当我现在看到他们时，尽管腰杆挺立，但头颅低垂，望着波澜不惊的水面，望着自己曾经成长的印迹。当我正想用“凄凉”或是“伤感”这样的词来形容它们时，不经意的一瞥这整个一湖的静立者们，一瞥与之格格不入的周围的绿柳青草，忽然间觉得我只能望着，满怀敬意，不敢言语。\n我时常找机会来这儿独自漫步，看看这里唯一的生命呼吸者们，也感受着自己生命的呼吸。生命只有在理解死亡时才觉出其中意义，也许，生活只有当乱中取静时才会一品个中滋味。\n","id":22,"section":"posts","summary":"说起虎溪的冬天，第一反应也许就是那些浸淫着寒风冷雨的灰色画面撺掇起来的时节；因为就是在整个渝州，晴朗的日子也是很少遇上的。 可是今早起床，掀开","tags":["虎溪岁月"],"title":"冬日虎溪","uri":"https://jiandan94.github.io/2015/12/winter-huxi/","year":"2015"},{"content":"凉风沁骨别暑天，\n芙蓉燎栗雨涟涟。\n十五中秋明月夜，\n卧听吴郎斧声坚。\n","id":23,"section":"posts","summary":"凉风沁骨别暑天， 芙蓉燎栗雨涟涟。 十五中秋明月夜， 卧听吴郎斧声坚。","tags":["虎溪岁月","诗文"],"title":"中秋有感","uri":"https://jiandan94.github.io/2015/09/zqyg/","year":"2015"},{"content":"最近考试频繁，都没有一个人好好在学校走走了。也不知是渝州受了厄尔尼诺的影响，还是老天眷顾虎溪的莘莘学子，天多半阴沉着，不见重庆孩子口中那往日毒辣的阳光。然而这几天，云淡风轻，艳阳高照，一靠近宿舍楼，远远地就听见呼呼地空调风扇声——入夏了，这次应该是真的！尽管天显得有点热，但好天气还是要出去走走的嘛~\n缙湖今年迎来了几只新宠，一直没时间去问候问候，所以刚刚考完试，疲惫不堪的我得了个多云天气的日子，就迫不及待地向缙湖走去。但当路过情人坡一段时，我不由自主地停下了脚步：好像有什么不对劲？细细观察才发现，原来是之前校工们给这里新换了草皮，那几天情人坡放眼望去，“伤痕累累”，“满目疮痍”，而且刚刚植上的草皮看上去矮小，枯黄······想想浪漫美丽的情人坡原来是这样修炼成长起来的呀！也许是最近雨过天晴，阳光正好，草们似乎卯足了劲儿往上蹿，现在伤疤不见，又是一片“夏”光啊！一下子，我忍不住停下脚步想来端详一下这群小家伙们。\n忽然间，我发现，猛地看上去长势一片大好的草皮实际却不是这样的：假如你稍微拨开一丛生长旺盛的草，你会发现他们的身下掩盖了另一丛草皮——只不过那丛生机不再，稀稀落落、参差不齐，看上去已是气数将尽。原来，生命的美好，是可以建立在其他同伴的尸体上的！当阴绵的雨水褪尽，迎来暖和的阳光时，每一株草都拼命地吸收能量，疯狂生长。要是有一株慢了节拍，或者遇到什么其他“挫折”，其他的草却不会热情地停下脚步等待那些陪伴自己度过苦日子的伙伴，反而踏着他们的躯体继续自己的生长。冰心说，“成功的花，人们只惊羡她现时的明艳！然而当初她的芽儿，浸透了奋斗的泪泉，洒遍了牺牲的血雨。”现在看来那些血雨多半是来自他们同伴的罢。\n想到这些，我不由得叹息：草们究竟不是人类，看我们虽然血液中也有生物原始的自私和贪婪，但是我们创立了文明，我们有，我们有······我们有什么？我楞住了，不是我想不来颂扬人类文明的例子，而是一瞬间闪在脑子里的竟然是人类中的那些“草”！\n然而，人类究竟比草还是高明点。我们发明面具掩饰，依靠“伪善”迷惑，凭借一张巧嘴用来颠倒黑白······看看这些草，像人类的种种野心和阴谋对它们来说竟然在光天化日就进行着；而我们却需要戴上面具，依靠黑夜，假借伪善······我现在站在他们面前嘲笑它们，它们却没有奚落我。我不知道，为什么明明宣扬着“文明”的我们中的我，面对着这群晴空下的阴谋者们，现在脸却红的发烫。\n有时候，我们以为的不是我们可以随便像我们以为的那样去以为；你觉得自己得到了什么，却输给别人半个灵魂！\n","id":24,"section":"posts","summary":"最近考试频繁，都没有一个人好好在学校走走了。也不知是渝州受了厄尔尼诺的影响，还是老天眷顾虎溪的莘莘学子，天多半阴沉着，不见重庆孩子口中那往日","tags":["虎溪岁月"],"title":"晴空下的阴谋","uri":"https://jiandan94.github.io/2015/06/qkym/","year":"2015"},{"content":"这两天忽然又对民国史有了兴趣，无奈囊中羞涩，只能望淘宝而兴叹。总想除了蒋廷黻之流再看看他处的风景，但是连网络也不给力，找不到资源（多半是我技术太差，找不到吧，哈哈！）于是，绞尽脑汁忽然想到重大的图书馆。素日觉得，在图书馆检索到书跟实际找到是两码事的我，而今也且行且搜索着。最后，访书未成，倒看到一本陈寅恪（音‘克’）的小传。我心下寻思，左右倒是看了他的一些故事，但真真的一本传记却实在没读过；仰着他的大名，我慢慢翻了起来……\n从祖宗八代说到父兄金兰，倒也没什么新奇的地方；倒是作者四处援例，字斟句酌，严谨的有点可爱。等到作者说到陈和王国维先生的友谊时，说到，陈只是找到了一个能聊的起话的人……我心里忽然有种莫名的触动。\n通常，一个人说找不到能聊的起话的人时，别人觉得这多半有点矫情。但如果这样的人如此轻易觅得，伯牙又何必摔琴呢？我记得自己倒是之前发过类似的言论，别人或曰自我，或曰极端，或曰空虚寂寞……有人说，你自己主动找别人说话，怎么可能没人跟你聊天呢？我汗颜不止。且不说我非孤僻桀骜者，古今蝇营狗苟、放荡不羁的也不能说没有‘说话’的人，所以此‘聊’肯定非彼众口之‘聊’！再者，先前数日我追过中国宫廷剧、韩剧之流，不少人只道是我‘喜欢’，很少问及为什么。少数的兴许觉得突然发现本人欣赏水平怎么怎么……却哪知呢，事实上我是偶然间看到有人以国人痴迷宫廷剧、韩剧为主题，研究当代国人的心理。我觉得很好玩，向来对喜欢的东西崇尚坐而论道不如格物致知的我，自然得把自己打扮成这类群体，倒要一入虎穴，窥其一二。\n有时我在想，语文考试时各种修辞手法都不敢忘记的我们，为何在活生生的生活中倒完全不记得了？譬如，‘沧海桑田’、‘物换星移’，不是有人这么说话是在矫揉造作，而是告诉你变化好大的确直截了当，但是大家都这样沟通，人和人的交流就没有厚度；其次，揣摩别人的话不是锱珠必较，那太累也没必要，但是绝对不是不值得去思考。这是因为，我觉得说话的方式是阅读一个人很重要的角度。这种观点直接的证明就是，小说家刻画人物的性格特点时，一定不会放过人物的话语。就像《穆斯林的葬礼》中，谢秋思和罗秀竹两个说话方式迥然不同，两人性格也分外突出一样。既然大家都承认这个在文学中的观点，为何在生活中不加注意呢？所以此番想来，看不懂或者懒得想的人兴许终不是陈寅恪传记作者说的那个‘聊的起的人’。\n古人遇到知己，哪怕是弱冠之年都会感慨相见恨晚。想想也是，漫漫人生，长不过百载，能遇到的能有多少人，嗅味相投的能有多少人，相遇相知的又能有多少人？勿怪乎相见恨晚！\n然而，即使遇到本能聊的来的，谈的好的人，遇到了也不一定能成为好朋友。这是因为，每个人都有某种偏见，虽然不一定影响自身的人生观、价值观，但是却影响交朋友！譬如，有人觉得观点不同的人是很难成为好朋友的。但是像爱因斯坦和玻尔，两个人的科学观点不同，几乎每次都是一个有了论断，另一个就要废寝忘食的反对对方，这两个以科学为终生事业而科学观点相悖的却是很要好的的朋友。所以想要交到朋友，有时候还真的得需要‘技巧’。别人不能抛开一些固有的偏见，也许楚人腿断一百次也不能说服对方手中的石头是块美玉。交往中，假如能够适度的放低自己，耐心点，坦诚些，也许就会有不一样的收获！\n偶过重大后山僻静处，见数朵不知名的小花开的自在。也许它们等着愿意欣赏的人来，也许只为自己开放。\n","id":25,"section":"posts","summary":"这两天忽然又对民国史有了兴趣，无奈囊中羞涩，只能望淘宝而兴叹。总想除了蒋廷黻之流再看看他处的风景，但是连网络也不给力，找不到资源（多半是我技","tags":["虎溪岁月"],"title":"秋日偶记","uri":"https://jiandan94.github.io/2014/10/qroj/","year":"2014"},{"content":" 2012年，正在读高二的我看到高三学长和学姐正在举行“百日誓师大会”。他们的喊声不由得触动了我，让我对未来很是憧憬。几日后，便写下了这首诗。\n 江流浮沉风云涌，江畔何人起敛容。\n跣足常趋握发殿，躬身频至玉蟾宫。\n雄心勃勃扫漠北，虎视眈眈向江东。\n待到青梅煮酒日，拔剑示君论英雄。\n","id":26,"section":"posts","summary":"2012年，正在读高二的我看到高三学长和学姐正在举行“百日誓师大会”。他们的喊声不由得触动了我，让我对未来很是憧憬。几日后，便写下了这首诗。","tags":["虎溪岁月","诗文"],"title":"咏曹操","uri":"https://jiandan94.github.io/2012/03/ycc/","year":"2012"}],"tags":[{"title":"r语言","uri":"https://jiandan94.github.io/tags/r%E8%AF%AD%E8%A8%80/"},{"title":"分类算法","uri":"https://jiandan94.github.io/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"},{"title":"回归分析","uri":"https://jiandan94.github.io/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"title":"多元统计","uri":"https://jiandan94.github.io/tags/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1/"},{"title":"影评","uri":"https://jiandan94.github.io/tags/%E5%BD%B1%E8%AF%84/"},{"title":"数学史","uri":"https://jiandan94.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8F%B2/"},{"title":"时政","uri":"https://jiandan94.github.io/tags/%E6%97%B6%E6%94%BF/"},{"title":"相关性分析","uri":"https://jiandan94.github.io/tags/%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90/"},{"title":"算法","uri":"https://jiandan94.github.io/tags/%E7%AE%97%E6%B3%95/"},{"title":"虎溪岁月","uri":"https://jiandan94.github.io/tags/%E8%99%8E%E6%BA%AA%E5%B2%81%E6%9C%88/"},{"title":"诗文","uri":"https://jiandan94.github.io/tags/%E8%AF%97%E6%96%87/"}]}