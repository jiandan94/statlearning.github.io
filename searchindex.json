{"categories":[{"title":"R语言","uri":"https://jiandan94.github.io/categories/r%E8%AF%AD%E8%A8%80/"},{"title":"学术杂谈","uri":"https://jiandan94.github.io/categories/%E5%AD%A6%E6%9C%AF%E6%9D%82%E8%B0%88/"},{"title":"杂文","uri":"https://jiandan94.github.io/categories/%E6%9D%82%E6%96%87/"},{"title":"笔记","uri":"https://jiandan94.github.io/categories/%E7%AC%94%E8%AE%B0/"},{"title":"诗词歌赋","uri":"https://jiandan94.github.io/categories/%E8%AF%97%E8%AF%8D%E6%AD%8C%E8%B5%8B/"},{"title":"随笔","uri":"https://jiandan94.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"posts":[{"content":"1 为什么要做变量选择？  变量选择又叫模型选择，因为选出不同的变量自然就建立出不同的模型。近30年来，变量选择理论繁荣发展并逐渐渗透到各个领域中，成为大数据时代背后的中坚理论之一。\n 随着技术手段的发展和丰富，我们获取数据变得越来越容易，对于一个问题可能搜集到成百上千的变量。有人说，变量越多对一个问题的刻画就越细致，这是好事啊。那么，我们为什么在建立模型时要煞费苦心思考变量选择问题呢？\n一方面，搜集到的变量中可能存在与目标完全无关的变量（冗余变量）。如果将冗余变量纳入模型之中肯定会得到一个错误的模型，用其进行预测自然就会做出误判，这明显与我们的目标相违背。因此，模型能够剔除不相关变量显得很重要。\n另一方面，尽管有些变量已知和目标相关，但实际的影响微乎其微（类似线性模型中变量系数的绝对值近乎为零），把这些变量包含在模型中无疑增加了理解模型的难度。比如，建立一个含有50+变量与一个仅有5个变量的公司员工能力评估模型，如果两者在评估员工能力时效果相当，显然后者更易于理解员工能力的影响因素且更易于指导实际工作。\n所以，研究变量选择是很有现实意义的。\n2 早期的思考 原有的方法若可用，就省的重新造轮子了。然而，经典的最小二乘 (OLS) 没有变量筛选的能力。回忆OLS估计表达式可知，不论变量是否重要，OLS表达式\n$$ \\hat{\\beta}^{ols} = (X^TX)^{-1}X^Ty $$\n没有体现出变量是否重要的差异性。\n因此，大家开始采用的是子集回归法 (subset regression) 而不是OLS方法。\n子集回归的思想很朴素。给定数据集，我们从中选出最佳变量组合建立回归模型即可 (全子集回归)。不过，考虑变量的所有组合，这和变量数 $p$ 呈 $2^p$ 关系。这种指数级的关系使得全子集回归成本高昂、不切实际。因此，大家想到一种折衷的办法——考虑一部分变量组合，找到其中的最优模型。这种思路类似优化理论的局部最优解，不过也可以得到较为满意的结果。相关的方法有逐步回归 (stepwise regression)等。\n子集回归有个严重缺陷：当原始数据中加入或者剔除某些变量后，子集回归得到的新模型可能发生巨大的变动。 比如，研究基因和某个疾病之间的关系。先搜集100个基因用子集回归得到3个重要的基因。后面研究后发现另外几个基因可能也和该疾病有关，但用子集回归重新建模后，原先的3个基因竟然没有出现在新模型中！这让人对该方法的可靠性产生怀疑。\n于是，就轮到LASSO上场了。\n3 LASSO模型 LASSO是Least Absolute Shrinkage and Selection Operator 的缩写。据LASSO的提出者Tibshirani本人说，LASSO是受到Breiman (1995) 提出的Nonnegative Garotte方法启发而得到的。NG方法首先建立原始数据的OLS回归，然后对得到的系数施加一个带有非负权重的约束惩罚，使得某些影响较小的系数被直接压缩到零，从而实现变量选择的目的。\n$$ \\begin{split} NG:\\quad\u0026amp;\\min \\sum _{i=1}^n (y_i - \\sum _{j=1}^p t_j \\hat{\\beta} _j^{ols} x _{ij})^2 \\newline \u0026amp;t_j\\geq 0,\\quad\\sum _{j=1}^p t_j \\leq s \\end{split} $$\nNG方法基于OLS，因而深受OLS影响。换句话说，OLS做不好时，NG也好不到哪里去。因此，Tibshirani抛弃了“两步估计”，直接给系数添加了一个绝对值约束，这就是LASSO模型：\n$$ \\begin{split} LASSO:\\quad \u0026amp;\\min \\sum _{i=1}^n (y_i - \\sum _{j=1}^p \\hat{\\beta}_j x _{ij})^2 \\newline \u0026amp;\\sum _{j=1}^p \\vert \\hat{\\beta}_j \\vert \\leq s \\end{split} $$\n两个模型形式确实相似。不过，LASSO和Hoerl与Kernnard在1970揭示的岭回归 (Ridge Regression) 更为相似，仅仅是罚项由绝对值变成平方项：\n$$ \\begin{split} Ridge:\\quad \u0026amp;\\min \\sum _{i=1}^n (y_i - \\sum _{j=1}^p \\hat{\\beta}_j x _{ij})^2 \\newline \u0026amp;\\sum _{j=1}^p \\hat{\\beta}_j^2 \\leq s \\end{split} $$\n因此，这么一个看似极其微小的改动，究竟神奇在哪里呢？\n- 参数s\n模型的参数s是需要事先给定的一个非负常数。以LASSO模型为例，如果 $s=0$，那么显然每个模型的系数都是零；如果 $s$ 趋于正无穷，那么约束相当于没加，结果就是OLS估计。当 $s$ 取一个不太大的数时，比如OLS系数估计的绝对值之和的一半。显然此时LASSO无法得到OLS估计，它需要降低某些系数的绝对值以满足约束条件。所以，参数 $s$ 实际控制了系数的压缩程度。\n OLS是无偏估计，但在变量数较多时估计量的MSE较大 (估计的系数不可靠)。MSE由估计量的方差和偏度组成，因此牺牲无偏性可以换来估计量方差显著减小，从而得到一个更加可靠的估计。\n - 罚函数\nLASSO约束系数从而提升估计量可靠性，你用这个思路分析岭回归也有同样的结论。那么，我们再来看两者罚函数形式的区别。考虑二变量情形，画出LASSO和Ridge模型的几何示意图有\n两者目标函数都是二次损失，对应一个椭圆。LASSO罚函数采用的是绝对值优化，可行域的边界有很多“尖点” (不可导点)。因此，目标函数优先和尖点触碰，也就形成了稀疏解 (某些系数为零的解)。 然而Ridge罚函数边界光滑，故而难以形成稀疏解。\n于是，我们可以看到LASSO的美妙之处：罚函数的小小改动就可以同时提升模型的性能、估计系数以及实现变量选择。\n LASSO最早发表在统计“四大天王”期刊之一的JRSSB上。原文没有一个理论证明，但仍然影响深远。可见，创新不见得一定要平地起高楼，站在巨人的肩膀上完全可以有卓越的突破。\n 4 LASSO模型的求解 4.1 设计阵正交 一般来说，求解LASSO这种带约束的凸优化问题，常常先考虑相应的拉格朗日函数。对于LASSO，拉格朗日函数为\n$$ LASSO:\\quad \\min \\sum_{i=1}^n (y_i - \\sum_{j=1}^p \\hat{\\beta}_j x_{ij})^2 + \\lambda \\sum_{j=1}^p \\vert \\hat{\\beta}_j \\vert $$\n其中 $\\lambda$ 是非负的拉格朗日乘数。如果我们假设数据矩阵 $X$ 是一个列正交矩阵，我们就能得到LASSO的显示解为\n$$ \\begin{split} \\hat{\\beta} _j^{LASSO} \u0026amp;= sign(\\hat{\\beta} _j^{ols})\\left(\\hat{\\beta} _j^{ols} - \\frac{\\lambda}{2} \\right) _+ \\newline (x) _+ \u0026amp;= \\begin{cases}x,\u0026amp;x\\geq 0 \\newline 0,\u0026amp;x \u0026lt; 0 \\end{cases} \\end{split} $$\n其中sign()表示取符号。在X列正交时，我们顺便求出Ridge的系数估计表达式为\n$$ \\hat{\\beta}_j^{Ridge} = \\frac{1}{1+\\lambda}\\hat{\\beta}_j^{ols} $$\n可见，LASSO的解其实就是对最小二乘的解做了一个压缩：对于较小的那部分系数，直接将其压缩到零；对于较大的系数，作了一个平移压缩， 如左图中橙色的线所示。Ridge估计就是对OLS估计做了一个整体的放缩，并没有直接压缩为零的过程， 如右图中蓝色线所示。\n因此，我们在这里也能看到罚函数的微小差异带来的巨大不同。绝对值惩罚在零点的不光滑性是LASSO产生稀疏性的本质。\n4.1 LARS算法 实际中，数据矩阵X一般不列正交，此时LASSO没有显示解。\n LASSO模型罚函数的奇异性像一柄双刃剑：它可以同时实现系数估计和变量选择，但也让基于梯度的算法失效而令其难以有效计算。事实上，LASSO问世初期没有得到人们关注的最大原因就是不好算！\n 据说，Tibshrani当时为了寻找一个有效的算法，曾四处拜访名家无果。最后，Tibshirani找到了自己的恩师Efron，大师出手一举摆平了此事。这就是著名的最小角回归 (Least Angle Regression, LARS)。\n有了LARS算法，LASSO像开了挂一般，迅速得到了蓬勃的发展。确实，统计模型往往来源于实际问题，如果模型仅仅停留在理论的空中楼阁，往往难以拥有生命力。当然，LARS不仅是一个算法，重要的是其从理论上揭示稀疏性 (Sparsity) 的本质。有兴趣的朋友可以拜读一下大师这篇90多页的大作！\n4.2 坐标下降法 LARS算法将LASSO的计算复杂度降到相当于OLS的复杂度，这基本就将其计算效率提升到极致。不过，这个记录后来被Friedman打破了。Friedman采用的是坐标下降法 (Coordinate Decent Algorithm, CD)。\n我们思考这么一个问题：如果一个函数在某点处各个维度取到了最小值，那么该点是不是函数的最小值点？\n 假如该结论成立，那么全局优化的过程就可以从每个维度进行一维优化即可。这显然极大地提升了模型的求解速度。\n 可惜，上述问题在一般情形下的答案是不一定。幸运的是，在LASSO模型中这个结论是成立的！因此，我们优化LASSO可以从每个维度 (几何上看就是每个坐标) 进行优化，采用迭代计算收敛到最优解即可。这就大大简化了模型的求解过程。\n考虑第 $j$ 个系数，我们根据KKT条件得到该出的最优解满足\n$$ \\begin{split} \\hat{\\beta}_j \u0026amp;= sign(x _j^T r_j)\\frac{(x _j^T r_j - \\frac{\\lambda}{2})_+}{\\Vert x_j\\Vert_2^2} \\newline r_j \u0026amp;= y - \\sum _{k\\neq j}^p x_k\\hat{\\beta}_k \\end{split} $$\n于是，我们便可以利用上式来迭代求解LASSO模型。\n4.3 正则化参数的选取 LASSO模型中的参数 $s$ 控制系数的压缩程度，拉格朗日函数中则由参数 $\\lambda$ 扮演该角色。通常，我们称这种参数为正则化参数，它们需要事先给定。\n正则化参数给大了，系数压缩的过偏，影响模型的表现；正则化参数给小了，系数压缩不明显，提升模型表现的作用不大。\n那么问题来了：怎么设定一个最优参数？\n通常，我们首先选定一系列的正则化候选参数；然后针对每个参数计算某一准则 (如AIC、BIC等) 的结果；最后根据准则确定最优参数的大小。\n5 R与LASSO 在R中，我们可以使用lars进行LASSO求解。使用前需要事先安装lars程序包，它由Efron和Hastie编写。此外，我们还可以使用glmnet求解LASSO模型。使用之前需要事先安装glmnet程序包，它由Tibshirani、Friedman等编写的。它们的基本用法如下\n# lars求解lasso的基本用法\rfit \u0026lt;- lars(x, y, type = \u0026quot;lasso\u0026quot;, intercept = T)\r # glmnet求解lasso的基本用法\rfit \u0026lt;- glmnet(x, y, alpha = 1, lambda = NULL, intercept = T)\rpredict.glmnet(fit, newx)\r 其中x为数据矩阵，y是响应变量，lambda是正则化参数。当alpha=1时，glmnet计算LASSO模型；当alpha=0时，glmnet计算Ridge模型；当0\u0026lt;alpha\u0026lt;1时，glmnet计算Elastic Net模型，一个LASSO的改进模型。此外，模型的截距项参数intercept默认是T，改成F即为无截距。\nglmnet本身优化了模型的计算。对于给定的数据集，glmnet自动计算一系列lambda (默认100个) 对应的系数估计，且每次计算都会利用到上次估计的结果。因此，计算一系列lambda的速度比单独给lambda赋值计算的总耗时要快的多。如果你想用CV (cross validation) 的方式寻找最优参数，也可以自己手动设置参数或者使用cv.glmnet函数。\n6 模拟分析 我们在R当中产生一个含有10个变量和1000个观测的数据矩阵，系数为 (1,0,1,0,\u0026hellip;,1,0) 且模型无截距，具体代码如下\nset.seed(123)\rsamplen \u0026lt;- 1000\rsamplep \u0026lt;- 10\rx \u0026lt;- matrix(rnorm(samplep*samplen), ncol = samplep)\rb \u0026lt;- rep(c(1,0),5)\ry \u0026lt;- x%*%b + rnorm(samplen)\r 6.1 使用lars求解 使用lars函数求解模型，并用plot函数画出系数估计的路径图\nlibrary(lars)\rfit \u0026lt;- lars(x, y, type = \u0026quot;lasso\u0026quot;, intercept = F)\rplot(fit)\r 图中的竖线表示lars算法的迭代次数，非零值对应的即是被选入的变量，图的上刻度数字表示本次迭代选入的变量数，图的右刻度数字表示最终选入的变量名。\n我们可以使用summary函数查看每次迭代的具体情况，可以按照最小 $C_p$ 值得到最优的模型。\n看来 $C_p$ 准则得到的模型和真实模型有点出入。实际估计的系数可以看到，非零系数基本接近1，零系数基本接近零，但 $C_p$ 准则在本案例中稀疏的能力不佳。\n\u0026gt; fit$beta[which.min(fit$Cp),]\r[1] 0.98459718 0.04174324 1.02200983 0.05431813 1.02213893 0.00000000\r[7] 0.95807588 0.01790684 1.02019194 0.04779414\r 6.2使用glmnet求解 使用glmnet求解，再用print函数输出结果：每个lambda对应的模型选入的变量数Df和模型解释的偏差%Dev (R方)。\n\u0026gt; library(glmnet)\r\u0026gt; fit \u0026lt;- glmnet(x, y, intercept = F)\r\u0026gt; print(fit)\rCall: glmnet(x = x, y = y, intercept = F) Df %Dev Lambda\r1 0 0.00000 1.23800\r2 1 0.04079 1.12800\r3 3 0.08998 1.02800\r4 5 0.18710 0.93620\r5 5 0.29830 0.85310\r6 5 0.39060 0.77730\r7 5 0.46730 0.70820\r8 5 0.53090 0.64530\r9 5 0.58370 0.58800\r10 5 0.62760 0.53570\r···\r 选取Df=5和最大%Dev对应得lambda值，利用coef函数得到相应得系数估计结果。\n\u0026gt; coef(fit, s = 0.06305)\r11 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r1\r(Intercept) . V1 0.9243286\rV2 . V3 0.9650517\rV4 . V5 0.9656673\rV6 . V7 0.8985474\rV8 . V9 0.9700144\rV10 .  看来结果不错。不过，我们利用cv.glmnet筛选最优参数发现，最优模型也是对应9个变量。\n\u0026gt; cv_fit \u0026lt;- cv.glmnet(x, y)\r\u0026gt; op_lam \u0026lt;- cv_fit$lambda[which.min(cv_fit$cvm)]\r\u0026gt; coef(fit, s = op_lam)\r11 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r1\r(Intercept) . V1 0.97767537\rV2 0.03679953\rV3 1.01549521\rV4 0.04761603\rV5 1.01535413\rV6 . V7 0.95157838\rV8 0.01017659\rV9 1.01429797\rV10 0.04150092\r 看来我随便生成的这个数据把这两个准则给难住了。\n","id":0,"section":"posts","summary":"1 为什么要做变量选择？ 变量选择又叫模型选择，因为选出不同的变量自然就建立出不同的模型。近30年来，变量选择理论繁荣发展并逐渐渗透到各个领域中","tags":["数学史"],"title":"变量选择与LASSO","uri":"https://jiandan94.github.io/2020/04/lasso-introduction/","year":"2020"},{"content":"1 背景介绍 最近，有朋友跟我说起股票数据处理的问题。由于课程要求，学生需在软件中下载股票指数数据 和对应的成分股收盘价数据，然后对指数和收盘价做一个分析，比如回归分析。\n金融软件导出的数据是指数+所有的成分股的CSV文件，每个CSV文件格式相同，包含时间、开盘价、收盘价、成交量等。那么，问题来了——怎么把这些文件整合到一起去呢？\n准确来说，这个问题要考虑以下这些点：\n 指数和所有成分股要按时间一一对应，每一行代表一个时间点的指数和成分股收盘价数据； 成分股遇到停盘数据便会有缺失，要能够根据最近的收盘价进行填补。  以前也有相应的R处理代码供使用。不过在和同学交流时发现，这个代码运行经常报错。因此，我自己就编了一个。下面说说我编写的思路。\n2 算法设计 我设计这个算法的思路很朴素。首先，指数每天都有不会缺失。因此，我首先把指数都读进来提取出指数的时间点 和收盘信息。 然后，根据成分股的个数创建合适的矩阵先保存好指数数据。如下图所示。\n接着，对第一个成分股进行处理——把时间 和收盘价 提取出来。先不用管它哪里缺失了需不需要填充数据。\n 我们要明确一点，导出的时间段内，指数肯定是最完整的。因此，我们将成分股按照时间跟它匹配肯定可以实现。\n 那么，我们就根据时间点 把第一个成分股匹配到指数所在的矩阵中，也就是下面这样的图像。\n从上图看出，第一支股票在时间点3和时间点4有缺失数据。那么按照填充要求，最近的时间点2有数据，因此可以用来填充，就得到了下面的结果。\n以此类推，我们把有的股票都按照时间匹配到这个矩阵中，并做好缺失数据填充，结果就是下面这样。\n那么最后就简单了——按照木桶原理 把前面没有信息的数据全部砍掉即可，如下图所示。\n3 R 程序代码 这个程序我当时为了赶作业图省事用了很多for循环，因此数据量大的时候，运行有点慢。\n程序分为日线数据处理代码 和分钟线数据处理代码。 显然，可以把这两个合并到一起，不过课已上完我就懒得弄了。\n另外，这是根据西南金点子软件 导出数据结构编写的，其他金融软件导出的数据对应改下应该就没问题了。\n- 代码使用方式\n【1】我桌面是默认的工作目录\r【2】上证指数日线csv数据放在桌面上\r【3】桌面上的data文件夹用来放置50只成分股csv数据（一定要保存成csv格式）\r那么参数就可以写成\rwdir\u0026lt;-\u0026quot;C:/Users/***/Desktop/\u0026quot; ######## (要换成自己的路径)\rfiledir \u0026lt;- \u0026quot;data\u0026quot;\rindex.name \u0026lt;- \u0026quot;sz50.csv\u0026quot;\rdata_org(wdir,filedir,index.name)## 日线数据函数\rminutedata_org(wdir,filedir,index.name)## 分钟线数据函数\r其中出现下面警告直接无视，这只是将字符转换成数字时系统的温馨提示。\rWarning message:\rIn data_org(\u0026quot;C:/Users/tom/Desktop/\u0026quot;, \u0026quot;data\u0026quot;, \u0026quot;sz50.csv\u0026quot;) :\rNAs introduced by coercion\r程序运行后就将分散的指数和成分股数据按照时间整合到一起了，其中缺失数据用前一天补充。\r - 日线处理程序\ndata_org \u0026lt;- function(wdir,filedir,index.name){\r#设定文件所在路径与读入文件\rsetwd(wdir)\rf.names \u0026lt;- list.files(filedir)\rstock_names \u0026lt;- substr(f.names,4,9)\rf.dir \u0026lt;- paste(\u0026quot;./\u0026quot;,filedir,\u0026quot;/\u0026quot;,f.names,sep=\u0026quot;\u0026quot;)\rn.names \u0026lt;- length(f.names)\r# 首先阅读股票指数数据\rindex_data \u0026lt;- read.csv(index.name,skip = 2,header = F,stringsAsFactors = F)\rindex_data \u0026lt;- index_data[1:(nrow(index_data) - 1),c(1,5)]\rcolnames(index_data) \u0026lt;- c(\u0026quot;date\u0026quot;,\u0026quot;index\u0026quot;)\rindex_data$date \u0026lt;- as.Date(index_data$date)# 转换成时间格式\rindex_data$index \u0026lt;- as.numeric(index_data$index)# 将指数数据转换成数值型\rindex_data \u0026lt;- na.omit(index_data)# 去除NA的行\r# 处理成分股数据\rn_zero \u0026lt;- rep(0,n.names)\rstocks \u0026lt;- list()\rfile_data \u0026lt;- index_data\rfor(i in 1:n.names){\r# 读取数据\rstocks[[i]] \u0026lt;- read.csv(file = f.dir[[i]],skip = 2,header = F,stringsAsFactors = F)\rstock_data \u0026lt;- stocks[[i]][1:(nrow(stocks[[i]]) - 1),c(1,5)]\rcolnames(stock_data) \u0026lt;- c(\u0026quot;date\u0026quot;,paste0(\u0026quot;Stock\u0026quot;,stock_names[i]))\rstock_data$date \u0026lt;- as.Date(stock_data$date)# 转换成时间格式\rstock_data[,2] \u0026lt;- as.numeric(stock_data[,2])# 将股票数据转换成数值型\rstock_data \u0026lt;- na.omit(stock_data)# 去除NA的行\r# 根据指数的日期来处理成分股数据\rtemp1 \u0026lt;- rep(0,length(index_data$date))\rfor (t in 1:length(stock_data$date)){\rdate_index \u0026lt;- grep(stock_data$date[t],index_data$date)\rtemp1[date_index] \u0026lt;- stock_data[,2][t]\r}\rfor (k in 2:length(index_data$date)) {\rif(temp1[k] == 0){\rtemp1[k] \u0026lt;- temp1[k - 1]\r}\r}\rn_zero[i] \u0026lt;- which(temp1 \u0026gt; 0)[1]\rtemp2 \u0026lt;- data.frame(temp1)\rcolnames(temp2) \u0026lt;- paste0(\u0026quot;Stock\u0026quot;,stock_names[i])\rfile_data \u0026lt;- cbind(file_data,temp2)\r}\rdata_trunc \u0026lt;- max(n_zero)\rfile_data \u0026lt;- file_data[-(1:data_trunc),]\rwrite.csv(file_data,paste0(\u0026quot;org_\u0026quot;,index.name))\r}\r - 分钟线处理程序\nminutedata_org \u0026lt;- function(wdir,filedir,index.name){\r#设定文件所在路径与读入文件\rsetwd(wdir)\rf.names \u0026lt;- list.files(filedir)\rstock_names \u0026lt;- substr(f.names,4,9)\rf.dir \u0026lt;- paste(\u0026quot;./\u0026quot;,filedir,\u0026quot;/\u0026quot;,f.names,sep=\u0026quot;\u0026quot;)\rn.names \u0026lt;- length(f.names)\r# 首先阅读股票指数数据\rindex_data \u0026lt;- read.csv(index.name,skip = 2,header = F,stringsAsFactors = F)\rindex_data \u0026lt;- index_data[1:(nrow(index_data) - 1),c(1,2,6)]\rcolnames(index_data) \u0026lt;- c(\u0026quot;date\u0026quot;,\u0026quot;minute\u0026quot;,\u0026quot;index\u0026quot;)\rindex_data$dateminute \u0026lt;- paste0(index_data$date,index_data$minute)\rindex_data$index \u0026lt;- as.numeric(index_data$index)# 将指数数据转换成数值型\rindex_data \u0026lt;- na.omit(index_data)# 去除NA的行\r# 处理成分股数据\rn_zero \u0026lt;- rep(0,n.names)\rstocks \u0026lt;- list()\rfile_data \u0026lt;- index_data\rfor(i in 1:n.names){\r# 读取数据\rstocks[[i]] \u0026lt;- read.csv(file = f.dir[[i]],skip = 2,header = F,stringsAsFactors = F)\rstock_data \u0026lt;- stocks[[i]][1:(nrow(stocks[[i]]) - 1),c(1,2,6)]\rcolnames(stock_data) \u0026lt;- c(\u0026quot;date\u0026quot;,\u0026quot;minute\u0026quot;,paste0(\u0026quot;Stock\u0026quot;,stock_names[i]))\rstock_data$dateminute \u0026lt;- paste0(stock_data$date,stock_data$minute)\rstock_data[,3] \u0026lt;- as.numeric(stock_data[,3])# 将股票数据转换成数值型\rstock_data \u0026lt;- na.omit(stock_data)# 去除NA的行\r# 根据指数的日期来处理成分股数据\rtemp1 \u0026lt;- rep(0,length(index_data$dateminute))\rfor (t in 1:length(stock_data$dateminute)){\rdate_index \u0026lt;- grep(stock_data$dateminute[t],index_data$dateminute)\rtemp1[date_index] \u0026lt;- stock_data[,3][t]\r}\rfor (k in 2:length(index_data$dateminute)) {\rif(temp1[k] == 0){\rtemp1[k] \u0026lt;- temp1[k - 1]\r}\r}\rn_zero[i] \u0026lt;- which(temp1 \u0026gt; 0)[1]\rtemp2 \u0026lt;- data.frame(temp1)\rcolnames(temp2) \u0026lt;- paste0(\u0026quot;Stock\u0026quot;,stock_names[i])\rfile_data \u0026lt;- cbind(file_data,temp2)\r}\rdata_trunc \u0026lt;- max(n_zero)\rfile_data \u0026lt;- file_data[-(1:data_trunc),]\rwrite.csv(file_data,paste0(\u0026quot;org_\u0026quot;,index.name))\r}\r ","id":1,"section":"posts","summary":"1 背景介绍 最近，有朋友跟我说起股票数据处理的问题。由于课程要求，学生需在软件中下载股票指数数据 和对应的成分股收盘价数据，然后对指数和收盘价做","tags":["R语言","算法"],"title":"股票数据的合并","uri":"https://jiandan94.github.io/2020/03/stockbind/","year":"2020"},{"content":"最近有读者私信希望介绍R并行计算的方法。在处理较大规模问题时，默认单线程的R显得力不从心。提升运算速度确实在数据分析中意义重大，其中酸爽调参侠定深有感触。\n不过我觉得饭要一口一口吃，如果算法先天不足、设计不合理，指望通过某个函数或程序包暴力提升运算速度，就很难得到满意的结果。\n因此，我打算从一些简单的细节入手，给大家分享R运算速度提升的常见方法和思维误区。\n1. 内置函数与向量化思维 R对常见的运算如矩阵乘积，进行过专门的优化。那么，我们应当有意识将算法中的相关部分转用R优化过的内置函数处理，从而降低计算成本。\n又如，我们在学习模型时，常常看到作者用分量形式展示模型的求解过程——虽然这令人易懂，但让我们在设计算法时极易忽视将其向量化，最后造成计算开销骤增。\n- 案例展示 我们不妨考虑不带截距项的单变量最小二乘模型\n$$\\hat{\\beta}=arg\\min\\sum_{i=1}^n\\left( y_i - x_i\\hat{\\beta}\\right)^2$$\n很容易得到最小二乘解为\n$$\\hat{\\beta} = \\frac{\\sum_{i=1}^nx_iy_i}{\\sum_{i=1}^nx_i^2}$$\n这时，你可能将程序写成\n## 算法1 ols1 \u0026lt;- function(x, y){ a1 \u0026lt;- 0 a2 \u0026lt;- 0 for(i in 1:length(y)){ a1 \u0026lt;- a1 + x[i]*y[i] a2 \u0026lt;- a2 + x[i]^2 } return(a1/a2) }  假如你知道R中*的用法，你可能会将程序写成\n## 算法2 ols2 \u0026lt;- function(x, y){ return(sum(x*y)/sum(x^2)) }  当然你还可以写成向量形式\n## 算法3 ols3 \u0026lt;- function(x, y){ return(c(x%*%y/x%*%x)) }  为了揭示这三者计算效率的差别，我们不妨从标准正态分布中产生1000个观测，重复计算100次的结果如下\n\u0026gt; x \u0026lt;- rnorm(1000) \u0026gt; y \u0026lt;- x*2 + 0.1*rnorm(1000) \u0026gt; system.time(# 算法1 + replicate(100, ols1(x, y)) + ) 用户 系统 流逝 0.02 0.00 0.02 \u0026gt; system.time(# 算法2 + replicate(100, ols2(x, y)) + ) 用户 系统 流逝 0.01 0.00 0.01 \u0026gt; system.time(# 算法3 + replicate(100, ols3(x, y)) + ) 用户 系统 流逝 0 0 0  结果是显然的：向量化的运算比for循环效率明显高。因此，我们要记住 能避免使用for循环就避免使用， 因为for循环在R中很低效。\n善用R内置函数与向量化思维是很多新手易忽视的技巧。这个例子提醒大家须将其铭记于心。\n2. 警惕apply族函数 稍有R常识的读者可能怀疑我题目写错了，因为大家印象中apply族函数可以简化代码和提升效率，所以要广泛使用而不是警惕啊。简化代码我同意，但提升效率真的不一定正确。\n- apply函数 我们先看最常见的apply函数的用法\napply(X, MARGIN, FUN, ...)  其中X表示一个数组或者矩阵；MARGIN表示维度，1表示按行计算，2表示按列计算；FUN是需要施加在X的行或者列上的函数。\n例如，我们希望得到矩阵x每一行的标准差。利用for循环，我们可以将函数写成\nmyrowsd1 \u0026lt;- function(x){ n \u0026lt;- nrow(x) p \u0026lt;- ncol(x) rowsd \u0026lt;- c() for(i in 1:n){ rowsd \u0026lt;- c(rowsd, sqrt(sum((x[i,] - mean(x[i,]))^2)/(p - 1))) } return(rowsd) }  有了前面的向量化意识后，我们可以试着将代码写成\n## 算法2 myrowsd2 \u0026lt;- function(x){ n \u0026lt;- nrow(x) p \u0026lt;- ncol(x) rmean \u0026lt;- rowMeans(x) xbar \u0026lt;- matrix(rep(rmean, p), ncol = p) rowsd \u0026lt;- sqrt(rowSums((x - xbar)^2)/(p - 1)) return(rowsd) }  下面利用apply将sd函数作用到x的每一行上，立即得到我们想要的结果\n## 算法3 myrowsd3 \u0026lt;- function(x){ return(apply(x, 1, sd)) }  从代码上看，apply函数最终的代码确实简洁漂亮。然而，是美与实力兼有还是花瓶一个，需要实践出真知。\n为了比较这三种算法的差异，我们从正态分布中产生1000行10列的矩阵，重复100次后得到\n\u0026gt; x \u0026lt;- matrix(rnorm(10000), ncol = 10) \u0026gt; system.time(# 算法1 + replicate(100, myrowsd1(x)) + ) 用户 系统 流逝 2.08 0.19 2.37 \u0026gt; system.time(# 算法2 + replicate(100, myrowsd2(x)) + ) 用户 系统 流逝 0.03 0.04 0.08 \u0026gt; system.time(# 算法3 + replicate(100, myrowsd3(x)) + ) 用户 系统 流逝 2.52 0.02 2.58  结果令人大跌眼镜——apply函数耗时最多！使用内置函数rowMeans和rowSums的算法2最高效。\n- 并行apply函数 可否让apply函数的简洁与高效并存呢？考虑到apply使用FUN的过程是相互独立的，所以并行计算是个可行的思路。\n在R中已经内置parallel程序包，可以使用类似apply但能多线程运算的parRapply（对矩阵的行进行处理）命令。它的基本调用形式为\ncl \u0026lt;- makeCluster(\u0026lt;size of pool\u0026gt;)# 建立并行线程数 parRapply(cl, x, FUN) stopCluster(cl)# 关闭并行  我们利用并行apply命令重新计算上述问题得到\n\u0026gt; library(parallel) \u0026gt; cl \u0026lt;- makeCluster(2)# 并行两次 \u0026gt; system.time(# 算法3 + replicate(100, parRapply(cl, x, sd)) + ) 用户 系统 流逝 0.39 0.16 2.33 \u0026gt; stopCluster(cl)  可见并行两次的速度确实有提升，超过了for循环但远逊色于算法2的速度——我的surfaceGo只有两个物理核心，如果电脑允许你可以增加并行次数而降低运算时间。\n尽管如此，我想说的是apply函数并不是如我们以为的那样高效，我们在乎计算时间时便要警惕使用该命令！\n不过，这并不意味着apply函数没有存在的价值。在时间开销并不那么重要时，apply得到的代码十分优雅简洁，也容易理解。\n- apply族其他函数 apply族函数除了基本的apply命令外，常用的还有lapply和sapply等。\nlapply(X, FUN, ...)# X通常为向量、数据框或列表  apply命令并不适用于向量，而lapply则显得更加灵活——其中X可取向量、数据框、列表等。例如，我们给定一个向量，然后针对每个元素生成相应数目的均匀随机数，结果用列表储存。\n\u0026gt; x \u0026lt;- c(2,3,4) \u0026gt; lapply(x, runif) [[1]] [1] 0.2485223 0.4080533 [[2]] [1] 0.9620718 0.5607855 0.2601507 [[3]] [1] 0.23525977 0.05190834 0.05177627 0.72261602  如果不喜欢列表储存结果，可以考虑sapply命令，它返回向量或者矩阵。如果FUN得到的结果是一维的，那么sapply就返回一个向量；如果FUN得到的结果是多维的，那么sapply就返回一个矩阵，其每一列对应一个输出结果。\n例如，我们输入一个列表，求其中每一项的均值和方差。\n\u0026gt; x \u0026lt;- list(a = 1:3, b = rnorm(100)) \u0026gt; myfunction \u0026lt;- function(x){ + return(c(mean(x), var(x))) + } \u0026gt; sapply(x, myfunction) a b [1,] 2 -0.1352013 [2,] 1 1.1843918  更多的使用方式和注意细节限于篇幅，不再赘述。\n3. 多线程运算 实际的数据分析中，我们常常需要重复某一过程数十次以上，例如Cross Validation寻找最优参数。一般每次计算相互独立，但计算的规律又极其相似。除了for循环外，我们能否有高效的方法实现该目的呢？\n答案是肯定的：R中的多线程运算就能办到。\n我们这里主要介绍foreach程序包中foreach命令的使用方式。假如我们需要计算100个向量的均值（计算互不影响），for循环意味着依次进行均值计算，这显然低效；foreach多线程则可以同时计算多个向量的均值（同时计算多少取决于你电脑的线程数），自然就显得高效。\n因此，foreach可以看作for循环的加强版。它需要事先安装foreach和doPrarllel两个程序包。\ninstall.packages(\u0026quot;foreach\u0026quot;) install.packages(\u0026quot;doParallel\u0026quot;)  - 基本使用介绍 它的基本用法为\nlibrary(foreach) library(doParallel) cl \u0026lt;- makeCluster(\u0026lt;size of pool\u0026gt;) registerDoParallel(cl)# 注册并行的线程 foreach(..., .combine) %dopar% FUN # 并行计算FUN stopImplicitCluster()# 关闭并行  首先，我们需要确定多线程数并进行注册使用。如果你不知道自己电脑可用的线程数，可以使用detectCores命令获得；通过添加参数logical = F可以得到电脑的实际物理核心数，也即是真正可供调用的实体核心。\n\u0026gt; detectCores() [1] 4 \u0026gt; detectCores(logical = F) [1] 2  可见我的surfaceGo实际有两个物理核心，但可以虚拟两个核心出来。因此，我们可以设置的线程数不超过3个——电脑的运行也需要核心维持，故不能跑满。\n其次，我们解释一下foreach的各个参数。...是函数FUN进行计算的变量，可为一个或多个。.combine则是设置输出的结果形式，如用+连接、用rbind拼接成矩阵等。\n最后，我们使用stopImplicitCluster()关闭并行而不是stopCluster()命令。\n案例分析 我们考虑两个向量对应的的分量和与分量积，输出结果为一个矩阵。那么，程序可以写成\nmyfun \u0026lt;- function(a, b){ return(c(a+b, a*b)) } cl \u0026lt;- makeCluster(2) registerDoParallel(cl) res \u0026lt;- foreach(x = 1:5, y = 6:10, .combine = \u0026quot;rbind\u0026quot;) %dopar% myfun(x, y) stopImplicitCluster()  查看结果为\n\u0026gt; res [,1] [,2] result.1 7 6 result.2 9 14 result.3 11 24 result.4 13 36 result.5 15 50  我在使用foreach包的过程中发现，当问题规模不大、计算过程简单时，foreach调用计算机资源所消耗的时间反而让人得不偿失。 我们用下面的案例来说明问题。\n考虑前面的最小二乘案例，样本观测数固定为1000，重复估计1000次。\n\u0026gt; myfun \u0026lt;- function(){ + x \u0026lt;- rnorm(1000) + y \u0026lt;- x*2 + 0.1*rnorm(1000) + ols3(x, y) + } \u0026gt; system.time(# for循环 + for(i in 1:1000){ + myfun() + } + ) 用户 系统 流逝 0.36 0.00 0.36 \u0026gt; cl \u0026lt;- makeCluster(2) \u0026gt; registerDoParallel(cl) \u0026gt; system.time(# foreach并行 + foreach(i = 1:1000, .combine = \u0026quot;rbind\u0026quot;) %dopar% myfun() + ) 用户 系统 流逝 0.83 0.33 1.45 \u0026gt; stopImplicitCluster()  这也和我们开头所呼应：如果算法先天不足、设计不合理，指望通过某个函数或程序包暴力提升运算速度，就很难得到满意的结果。\n- 一点补充 现在我们编写函数，将foreach得到的100个最小二乘估计的平均值作为最终的解。你的函数若像下面一样编写\nmeanb \u0026lt;- function(){ cl \u0026lt;- makeCluster(2) registerDoParallel(cl) bhat \u0026lt;- foreach(i = 1:10, .combine = \u0026quot;rbind\u0026quot;) %dopar% myfun() stopImplicitCluster() return(mean(bhat)) }  调用后，就现找不到myfun函数的错误提示\n\u0026gt; meanb() Error in myfun() : task 1 failed - \u0026quot;没有\u0026quot;myfun\u0026quot;这个函数\u0026quot;  原因是foreach被嵌套进函数后无法使用函数外部的变量。 因此，我们需要添加.export参数，将需要的外部变量传递到函数内部，也即是\nbhat \u0026lt;- foreach(i = 1:10, .export = c(\u0026quot;ols3\u0026quot;, \u0026quot;myfun\u0026quot;), .combine = \u0026quot;rbind\u0026quot;)  这样，你在外部定义所需变量后再运行meanb函数，就能正常输出结果\n\u0026gt; meanb() [1] 2.001765 }  4. 写在最后 本次我们学习了R提升速度的常见思路和注意事项。最后再次提醒大家，理解自己的问题和程序结构最重要，盲目通过某些命令来暴力提升计算速度，可能会适得其反。\n","id":2,"section":"posts","summary":"最近有读者私信希望介绍R并行计算的方法。在处理较大规模问题时，默认单线程的R显得力不从心。提升运算速度确实在数据分析中意义重大，其中酸爽调参","tags":["R语言","算法"],"title":"提升R的运算速度","uri":"https://jiandan94.github.io/2020/03/rparallel/","year":"2020"},{"content":"1 线性模型的局限性 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题往往更加复杂，$y$ 并不总是满足正态分布的假设。\n例如，在医学诊断中，我们希望通过病人的各项检查数据判断其是否患癌症。这里是否患癌症作为响应变量 $y$ 只有两个可能的取值：患和不患。 显然，$y$ 不服从正态分布。\n类似的例子还有很多。在气象领域中，通过观测到的气象数据判断是否会下雨；在金融领域中，根据当前的金融指标判断是否抛售股票；在图像领域，综合输入的人脸数据判断是否通过验证\u0026hellip;\u0026hellip;\n上述问题的共同点是，$y$ 的取值是离散的，我们关心的不再是预测具体数值而是变成判断分类。\n为了讨论方便，我们以下考虑二分类问题。\n机灵的同学可能想到，依然对数据建立线性模型，然后再设定一个阈值 $a$，当 $y\u0026gt;a$ 时，预测数据归为一类；当 $y\u0026lt;a$ 时，预测数据归为另一类。\n这么做确实可以得到一个分类模型(上图红实线)。但是，如果我们在原有数据上额外得到一个新观测且远离原始数据时， 线性思路得到的新模型就会出现明显的变动(上图蓝虚线)。\n2 逻辑回归(Logistic Regression) 究竟哪里出现问题了呢？我们回忆所学的线性模型的形式：\n$$ y_i = x_i^T\\beta + \\epsilon_i, \\quad\\epsilon_i~N(0, \\sigma^2) $$\n对模型两边取期望和方差可得：\n$$ E(y_i) = x_i^T\\beta, \\quad Var(y_i) = \\sigma^2 $$\n显然可以看出：(1)线性模型对y的估计，本质是对 $y$ 期望的估计；(2)线性模型中 $y$ 的期望和方差两者互相独立。\n接着来看二分类问题，一般我们考虑 $y_i$ 独立同分布于成功概率为 $p_i$ 的伯努利分布(Bernouilli Distribution)：\n$$ y_i \\overset{iid}{\\sim} B(p_i) = p_i^{y_i}(1 - p_i)^{1 - y_i} $$\n两边取期望和方差可得：\n$$ E(y_i) = p_i, \\quad Var(y_i) = p_i(1 - p_i) $$\n此时的期望和方差两者之间存在函数关系且期望取值在 $[0,1]$ 之内。 因此，在二分类问题中完全套用线性模型的框架显然是有问题。\n该怎么办呢？\n试想 $y_i$ 是一个随机变量，在不知道额外信息的前提下，用它的期望作为估计还是挺合理的。因此，我们可以继续采用期望估计的思路。 但是二分类问题中 $y_i$ 的期望有0-1的约束，这就需要我们寻找一种满足该要求的函数。通常，这样的函数选取成Logistic函数：\n$$ E(y_i) = \\frac{1}{1 + \\exp(-x_i^T\\beta)} $$\n很容易验证Logistic函数满足前面的条件。选取这种函数形式的深层原因，从统计的角度来看，源自于指数族分布。 有兴趣的朋友可以翻阅我关于广义线性模型相关理论的介绍以及其他参考资料。\n于是，我们终于等到本次的主角——逻辑回归模型(Logistic Regression Model)：\n$$ y_i \\overset{iid}{\\sim} B(p_i),\\quad E(y_i) = \\frac{1}{1 + exp(-x_i^T\\beta)} $$\n我们之所以称其为回归模型，是因为线性回归深入人心，在其之上生长发展的理论便继承了这个名称。事实上，逻辑回归更多的是面向分类问题。\n3 逻辑回归的求解 3.1 极大似然法 最小二乘常被用来求解线性模型中的系数估计值，这在逻辑回归中失效了——我们无法得到逻辑回归的最小二乘损失函数的具体形式。\n然而我们知道问题的概率分布信息，因此可以得到相应的似然函数：\n$$ \\mathscr{L} = \\prod_{i = 1}^n \\left(\\frac{1}{1 + \\exp(-x_i^T\\hat{\\beta})} \\right)^{y_i} \\left(1 - \\frac{1}{1 + \\exp(-x_i^T\\hat{\\beta})} \\right)^{1 - y_i}$$\n再进而得到对数似然函数并进行优化即可。不过，与线性模型具有显示解不同，逻辑回归没有解析解，故而需要结合梯度下降法等算法进行求解，可参考广义线性模型中的相关方法。限于篇幅，不在赘述。\n3.2 R求解逻辑回归模型 R中内置glm函数可以求解逻辑回归模型，它的使用方式如下所示。\nlogmodel \u0026lt;- glm(formula, data, family = binomial(link = \u0026quot;logit\u0026quot;)) predict.glm(logmodel, newdata)# 预测模型  与线性模型求解函数lm区别在于，glm需要添加link参数。我们使用该函数对前面的模型再次求解并做图，如下所示。\n可见，逻辑回归很好的刻画了模型的主要特征，且不会因为新数据的出现模型发生巨大变动。\n4 案例分析 4.1 数据描述 我们从UCI机器学习数据库中获取到威斯康辛乳腺癌数据(Breast Cancer Wisconsin)共计569个观测，包含1个字符型分类变量，取值B表示良性肿瘤，取值M表示恶性肿瘤；30个数值型检测指标，如肿块厚度、大小、形态等。\n4.2 建立逻辑回归模型 我们使用R读入数据并进行分析。由于数据没有特定顺序，我们选择前70%作为训练集，剩下的作为测试集。为了方便展示结果，我们仅选取前三个变量 进行模型拟合，对应的代码如下所示。\n## 逻辑回归模型处理WDBC数据 wdbc \u0026lt;- read.csv(\u0026quot;wdbc.csv\u0026quot;, header = F, stringsAsFactors = F) colnames(wdbc) \u0026lt;- c(\u0026quot;y\u0026quot;, paste0(\u0026quot;x\u0026quot;, 1:30)) wdbc$y \u0026lt;- ifelse(wdbc$y==\u0026quot;M\u0026quot;, 1, 0) # 划分训练集和测试集 wdbc_train \u0026lt;- wdbc[1:400, 1:4] wdbc_test \u0026lt;- wdbc[401:569, 1:4] # 训练模型 wdbc_fit \u0026lt;- glm(y~., data = wdbc_train, family = binomial(link = \u0026quot;logit\u0026quot;))  4.3 评价模型 首先，我们使用summary()函数查看模型相关的统计指标，具体如下所示。\n可见，三个系数都非常显著。\n其次，我们使用predict.glm()查看模型的内预测效果，得到混淆矩阵(Confusion Matrix)如下\n    真实M 真实B 总计     预测M 215 18 233   预测B 12 155 167   总计 227 173 400    可以算出模型的预测准确率为(155+215)/400=0.925，仅三个变量就可以很好的预测病人肿瘤的状态。\n不过分类问题中，大家比较关心模型的敏感性和特异性。 敏感性指真阳性率，而特异性指真阴性率。以本案例而言，模型的敏感性为0.947，特异性为0.896。这么看来，我们的模型假阳性率为0.053，还是不错的。\nlibrary(pROC) roclong \u0026lt;- plot.roc(wdbc_train$y, lty = 1, train_pre, grid = T, percent=TRUE,ci=TRUE,col=\u0026quot;red\u0026quot;, print.auc = T,print.auc.cex = 0.8,print.auc.x = 60,print.auc.y = 50, main = \u0026quot;逻辑回归的ROC曲线和AUC值\u0026quot;)  进一步分析敏感性和特异性，可以利用ROC曲线 和AUC值 来刻画。我们在R中使用pROC包画出ROC曲线以及进行AUC的计算和检验，结果参见下图。\nAUC达到了92.2%， 说明模型能够很好的刻画我们的数据。\n最后，我们对剩下的测试集进行验证，得到预测准确率为88.76%，还是挺理想的——毕竟我们仅使用三个变量进行建模。关于逻辑回归更多的内容，还请读者自己了解。\n参考文献\n[1] Myers, Raymond H., et al. Generalized linear models: with applications in engineering and the sciences. 2012.\n","id":3,"section":"posts","summary":"1 线性模型的局限性 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题往往更加复杂，$y$ 并不总是满足正态分布的假设。 例","tags":["多元统计","分类算法","r语言"],"title":"逻辑回归模型","uri":"https://jiandan94.github.io/2020/03/logistic-regression/","year":"2020"},{"content":"最近，外国人永久居住条例在年轻人群体中掀起不小的风波。估计有关部门可能都没想到这个条例是个大雷，会有这么的能量。\n围观微博一圈，大家对于该条例基本呈强烈反对的态度。不过，那些为中国亿万女青年未来幸福纷纷哀呼的抨击者，窃以为大概率应该没有把条例完整看完。当然，将所有的人打成无脑喷，显然置信度不够。\n所以，我也实在好奇这个条例，更好奇大家的群情激愤，便谈谈自己简单的看法。\n作为一名典型理科男，对于移民政策知之甚少，但是通读条例几遍后也没理解出网友认为的居住条例的颁布等于大量的外来人口涌入。 不过，既然这是大家的一个痛点，不妨就顺着这个没道理的阅读理解答案分析一下。\n1 假装很理性 在认为条例的颁发必然导致外来人口大量涌入的假设下，作为全球人口数量最大的国家的民众，一个直接的质疑便是，我们不缺人啊？为什么还要吸引众多的移民？是不是有阴谋呢？\n人口第一这个结论自然毋庸置疑，但缺不缺人这个论断还要仔细斟酌。因为发展建设不能只盯着眼前，未来也很重要。\n我从中国统计年鉴中搜集了1982年到2018年的人口数据，这期间的变化趋势图如下所示。\n1982年，我国将“计划生育”写入宪法使之成为一项基本国策，到2016年国家正式施行“二胎”政策。从图中看到，“计划生育”实施后约20年，人口增长的趋势明显变缓才开始显露——所以，政策的影响并不是想当然的立竿见影。\n虽然2016年施行了“二胎”政策，但是这对人口增长的刺激显得有点力不从心。从下图的人口自然增长率可以看出，我们国家的人口增长率常年处于下滑状态。并且，在2016年“二胎政策”后还出现了一个陡降。\n 直觉有时候并不可靠，这在统计学中并不是一个新鲜的话题。\n 所以，现在不缺人，但不太久的将来不一定。我们再来看下图所示的2018年全国人口年龄结构柱状图。\n粗略的计算，60岁以上的人群占比17.88%，约2.5亿人；10年后将退休的的人群占比14.56%，而相应的参加工作（按20岁开始参加工作算）占比10.53%，也就是说\n10年后，约减少0.56亿人的主要劳动力，约为韩国的总人口数。\n所以，建设社会的主要劳动力数目确实以超乎我们想象的速度在减少。 那么，单纯的根据人口数目而深陷“阴谋论”实在不可取。\n不过，上面粗略的分析可以说明不需要引入移民 是一厢情愿的想法，但还不足以让激愤者冷静思考。\n 因为人口减少了，我们想当然的会认为就业机会将增加，落到个人头上便是更加富足。\n 事实果真如此么？\n我在网上搜集到了2018年全球各个国家的人口数以及相应的失业率数据。我们选择人口数前50的国家画出相应的散点图，如下所示。\n瞟一眼难以发现这两者有相关关系。对数据敏感的人可能发现右下角两个离群点：中国和印度，两个人口大国。是不是离群点使得数据信息淹没了呢？不妨移除他们，得到下图。\n这时数据呈现出某种倾向，难以直接断定两者无关。我们不妨对两者做皮尔逊相关性检验，得到P值为0.7141，不显著。\n因此，我们也不能轻易的认为，人口变少，意味着大家更容易就业。\n2 假装在说教 网上经常有很多所谓的流量大V关于某些时事恶意带节奏，如果我们按照他们的思维引导，很容易因为“直觉”被带跑偏而成为网络暴民。理性是个好东西，既然都立足在国家建设、民族发展的高度了，难道不应该去看看条例具体内容，不应该去切实分析具体问题么？\n用自己的想当然去抨击别人在想当然，自然得不到真理。\n要知道我们前面的分析是在一个不合理的假设下进行的，得到的结果虽不足以说明条例合理，却实实在在可以说明“无脑喷子”让人害怕。事实上，条例并没有哪里表明将要大量的引入外来人口，遑论中国男人娶不到老婆。\n3 写在最后 关于条例，我的着眼点其实不是它的条文是否合理，因为不懂相关领域，就不能胡言乱语。那么可以做什么呢？\n我们以往很少直接对国家大政方针建言献策，这次司法部向社会公开征询，让我们以为事关重大，造成神经紧张。因此我们完全可以借这次公开征询的机会慢慢将民众的监督职能进行完善。 某种程度上，这更加重要。\n","id":4,"section":"posts","summary":"最近，外国人永久居住条例在年轻人群体中掀起不小的风波。估计有关部门可能都没想到这个条例是个大雷，会有这么的能量。 围观微博一圈，大家对于该条例","tags":["时政","相关性分析"],"title":"关于外国人永久居住条例的简单看法","uri":"https://jiandan94.github.io/2020/03/immigration-policy/","year":"2020"},{"content":"远行不经意，归途暮雨袭。\n阑风入沉阁，竹斋听雨息。\n乱蝉声渐渐，梧桐叶槭槭。\n雁阵来时路，年年有归期。\n","id":5,"section":"posts","summary":"远行不经意，归途暮雨袭。 阑风入沉阁，竹斋听雨息。 乱蝉声渐渐，梧桐叶槭槭。 雁阵来时路，年年有归期。","tags":["虎溪岁月","诗文"],"title":"偶遇秋雨","uri":"https://jiandan94.github.io/2019/11/oyqy/","year":"2019"},{"content":"没想到易烊千玺的演技这么在线，看完《少年的你》感觉还是挺不错的。不过客观来说，我对电影主题略感模糊，感觉少了一点深度。\n开头那会儿突然就出现霸凌了，有点摸不着头脑的感觉。转念想到近几年也报到过很多次类似的新闻，这样切入也勉强可以吧。\n爱情线路是各类题材不可缺少的一条主线，设计的好也确实能够带动观影人的情绪，比如《少年的你》，但我觉得这也是这个电影跑偏的地方。我记得在观影时，看到小北保护陈念时的义无反顾，其他人都发出羡慕的赞声。尤其是小北把烟头弹向魏莱时，简直帅出天际。\n然而，电影究竟是想在屈辱和挣扎中揭露霸凌现象，还是想在霸凌背景下表达希望和美好？我认为这两者是完全不一样的，看完电影后我觉得主题在这之间模糊了。因为，我发现大家更多的在回味男主女主之间的信任和扶持等等。\n其实，在成长的阶段，很多人都有过委屈，有过忍气吞声。所以，谁不期待在自己孤独无助的时候，有个小北可以义无反顾、风雨无阻得站在自己身后？ 在霸凌的背景下，这样的信任，这样的依靠，这样的担当，无一不能引起观众内心中那根柔软的心弦——尽管大家并未都经历过霸凌。从这点来说，电影是成功的。\n不过，说它揭露清楚了霸凌，我不太认可。\n镜头更多得是展现被霸凌的人的屈辱和无奈，对于那些霸凌产生的根源所提甚少。电影也就浮光掠影得几个镜头，不是原生家庭的支离破碎，就是原生家庭的骄傲无礼，符合普罗大众的\u0026quot;社会映像\u0026quot;嘛。可是，这样就表明挖掘的还不够。 找不到根源就不能\u0026quot;于病视神，未有形而除之\u0026rdquo;，不能让一些施暴者在成为施爆者前得以挽救，比如魏莱的一些跟班姐妹。\n我看电影的时候，不自觉就想起以前的初中。那也是小混混不少，远近闻名。我自己也被小混混拦截抢过钱，也被别人恐吓过，吓得好几天战战兢兢。所以，当我看到女警官质疑陈念为什么在被欺负后不找大人求助，陈念沉默不语的神情时，我挺有感触的。\n 因为，大人们不是小北，不会风雨无阻得站在我的背后。而跟大人们说了，就等于和小混混公开宣战，那肯定会被打的很惨。所以，我会选择沉默和忍耐。\n 我的好朋友当时也被喊到厕所差点扇耳光。很多人可能没见过厕所外排了一条长队，一个一个走过去给里面的老大扇耳光的场景吧。\n我印象中，我知道和他们一起混的那几个同学，家里不是支离破碎，也没有富甲一方。就是普普通通的家庭，但是父母对孩子管教不上心。有的想法就是，小孩欺负人总比被欺负好，饿不死就行。这种情况不是少数，但比起破碎的家庭，富人的傲慢与偏见等等，少了吸引大众眼球的能力。作为商业电影，我觉得导演这么设定无可厚非。\n只是，我想如果一部揭露社会现象的电影，在关众还未看之前，就已经猜到了现象的设定，那它究竟揭露了什么？或者，在众所周知的原因下，它展现了什么更为深刻的反思？小混混，肯定是家破人亡？我在初中那会儿成绩不错，但我们班主任一直说和xxx比差远了。后面听别人说，xxx初一那会儿被小混混抢了好几回钱，开始班主任送他回家，但送了几次后就没继续了，因为自己也忙啊。后面xxx又被抢了几次。最后，xxx直接选择他们一起混了，而且还凶的很。xxx的爸妈是工人，家境还不错。听说这件事后，就说哪里没一口饭吃，管不了。\n我记得初中那会儿打算写一本小说，要狠狠的揭露这种现象。最后好像写了两章，我还拿去给我爸看了，我爸说有些用词故作老成，实际不够深刻，哈哈哈。无奈文笔不佳，后来不了了之了。现在想来，也是令人感慨。\n校园霸凌的确存在，需要关注，需要更多的思考，更需要从源头认真整治。\n","id":6,"section":"posts","summary":"没想到易烊千玺的演技这么在线，看完《少年的你》感觉还是挺不错的。不过客观来说，我对电影主题略感模糊，感觉少了一点深度。 开头那会儿突然就出现霸","tags":["虎溪岁月","影评"],"title":"看完《少年的你》后的一点想法","uri":"https://jiandan94.github.io/2019/10/yp-sndn/","year":"2019"},{"content":" 如果随便逮到一个统计专业的学生问他“统计方法谁家强”，相信大部分人会异口同声得说“最小二乘法”。\n 的确，最小二乘法是一种非常重要的统计方法，它的重要性不仅仅体现在对问题求解的自然、简单、有效层面，其背后所蕴含的“最小二乘思想”更在不同领域、不同问题中应用广泛。\n虽然以现在的眼光来看最小二乘法出发点朴素而又自然，但是它的产生也是历经波折。本文我们一起来了解一下最小二乘法的“心路历程”吧！\n1 天文和测地学 早期，在天文和测地学中经常会遇到这么一种数据分析情况：我们有若干个可以测量的量 $x_1$, $x_2$,\u0026hellip;, $x_p$ 和 $y$，他们之间呈现一种线性关系\n$$ y = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_px_p $$\n这里 $\\beta_1$,\u0026hellip;, $\\beta_p$ 都是未知参数，需要我们运用一定的方法估计出来从而应用到实际问题中去。\n现在稍有统计背景的都知道这用最小二乘可以轻易求解，不过当时还没有最小二乘的概念呢，因此求解就是一个令人头疼的问题。\n 可能有人说根据线性方程理论这也不是一个很困难的问题啊！只要测量得到 $p$ 组数据，那么应用线性方程的知识不就将未知参数求解出来了么？\n 可是在实际天文和测地学中同一问题研究人员会测量多组数据以降低测量过程中产生的误差，而相应的数据量基本都是大于未知参数的个数的 (其实就是超定线性方程组)，这就使得问题的求解比较棘手。\n我们希望的是，一方面对于方程的个数大于未知参数个数的问题，求解方法应该尽可能的简单、有效；另一方面，由于实际问题中测量误差不可避免，求解方法在理论上应该对误差控制有一个保证。所以总结起来就是四个字：又快又稳！\n2 早期工作 在最小二乘还未诞生的时代，各路英雄豪杰面对该问题也是使劲浑身解数。不过他们的方法核心思路是从大量的方程中挑选或者组合恰当数量的方程来进行参数求解。因此，各家各派使得都是挑选和组合的功夫。\n例如在1750年，天文学家梅耶发表了一种测定航海船只经度的方法，其中要从27个方程中求解出3个未知参数。他主要采用分组法将27个方程分成3组分别相加得到3个方程，最终求解出未知参数。 这个方法曾经一度流行，被冠以梅耶的名字。\n 此外，拉普拉斯和欧拉也在天文学中研究过类似的问题，但是解法极其繁杂而且杂乱无章。 他们二人这样的大数学家一生不知道解决过多少数学中的“疑难杂症”，对于这么一个并不是很困难的问题竟然束手无策。这确实让人感到不可思议。\n 3 勒让德和最小二乘 勒让德是法国大数学家，在数学的很多领域包括椭圆几分、数论和几何等方面都有着重大的贡献。\n$$ (\\hat{\\beta}, \\hat{\\beta}_0) = \\min \\sum _{i=1}^n \\left( y_i - x_i^T\\hat{\\beta} - \\hat{\\beta}_0 \\right)^2 $$\n最小二乘法最早于1805年勒让德公开发表的文章《计算彗星轨道的新方法》中问世。在这本著作的附录中，勒让德描述了最小二乘法的思想、具体做法和优缺点。\n 最小二乘法一经提出，由于其思想自然合理、操作简单有效，很快就得到欧洲一些国家的天文和测地工作者的广泛使用。据不完全统计，自1805年到1864年，有关这一方法的研究论文约有250篇。\n 尽管勒让德的工作没有涉及到最小二乘的误差分析理论，但是他也注意到了各个方程因为误差不独立而不能直接运用最小二乘法， 这的确难能可贵。最小二乘法的“快”勒让德已经说明了，关于它的“稳”则是高斯的工作了。\n4 高斯的工作 1809年，高斯在《绕日天体的运动理论》的一节中讨论了“数据结合”的问题，实际就是误差分布的确立问题。假设真值为 $\\theta$，有 $n$ 个独立的测量值 $x_1$,\u0026hellip;, $x_n$，高斯将后者的概率定为\n$$ \\mathscr{L}(\\theta) = f(x_1 - \\theta)\\cdots f(x_n - \\theta) $$\n其中 $f$ 就是待定的误差密度函数。在确立密度函数形式过程中，高斯有两个创新点。\n  一是他没有采取已有的贝叶斯推理方法，而是直接将 $\\mathscr{L}(\\theta)$ 的最大值——极大似然的思想 ——定为 $\\theta$ 的估计值。\n  二是他先承认了观测值 $x_1$,\u0026hellip;, $x_n$ 的算数平均值为 $θ$ 的估计值，然后再去找误差的密度函数来迎合这一点——在这样的 $f$ 下，$\\theta$ 的估计值就是算数平均值。最后他得出只有在\n  $$ f(x) = \\frac{1}{\\sqrt{2\\pi}h}e^{-\\frac{1}{2h^2}} $$\n时才成立。这就是均值为 $\\theta$ 标准差为 $h$ 的正态分布.\n 使用正态分布就可以对最小二乘给出一种解释，也就是可以对其误差做出理论上的分析，保证了这种方法的优越性。后世将最小二乘的发明权归功于它，也正是因为这一项工作。\n 尽管高斯讨论最小二乘法的文章发表较晚，但是他声称自己很早之前就运用勒让德的最小二乘法来解决问题。这也导致两人后来最小二乘的首创权争论。\n不过在高斯的证明中有点循环论证 的感觉，先承认算数平均值估计的优越性，再得到误差正态密度函数形式，然后再说明算术平均值作为估计的合理性。这一缺陷在拉普拉斯运用其发现的中心极限定理得以解决。 他指出现实中的误差可以看成很多量的叠加，那么根据他的中心极限定理，误差的分布就是正态分布。\n5 写在最后 从最小二乘法的发展历史来看，一项科学理论的发展并无坦途， 尽管这项理论看起来朴素而又简单。\n","id":7,"section":"posts","summary":"如果随便逮到一个统计专业的学生问他“统计方法谁家强”，相信大部分人会异口同声得说“最小二乘法”。 的确，最小二乘法是一种非常重要的统计方法，它","tags":["数学史"],"title":"漫谈最小二乘法","uri":"https://jiandan94.github.io/2019/08/forum-ols/","year":"2019"},{"content":"1 导言 回归分析是一个古老的话题。一百多年前，英国的统计学家高尔顿 (F. Galton，1822-1911) 和他的学生皮尔逊 (K. Pearson，1857-1936) 在研究父母和其子女身高的遗传关系问题中，统计了1078对夫妇的平均身高以及他们的一个成年儿子的身高数据。\n他们将孩子的身高作为自变量 $x$，父母的平均身高作为因变量 $y$，然后将两者画在同一张直角坐标系上。结果，他们发现这些数据点“惊人的”位于一条直线的附近，并且经过计算得到了直线的拟合方程:\n$$ y = 33.73 + 0.516x $$\n 这个结果看起来是违背直觉的。因为统计的结果表明，高个子父母的子女有低于父母身高的趋势；而矮个子的子女则有高于父母的趋势。高尔顿解释说，自然界存在某种约束力将人的身高向某个“平均数”靠拢——或者说是“回归”——也即是统计学上回归的涵义。\n 那么本文的主题便是了解线性回归模型并通过R来解决线性回归分析中的若干问题。\n2 基础回顾 回归的概念来源于实际问题，那么现在我们所说的线性回归分析问题具体指的是什么呢？一般说来，如果我们研究的问题中的 $p$ 个自变量 $x_1$, $x_2$, \u0026hellip;, $x_p$ 和因变量 $y$ 的关系形式如下所示\n$$ y_i = \\beta_0 + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip} + \\epsilon_i $$\n那么我们就说这是一个线性回归问题，其中 $\\epsilon_i$ 是随机误差项，$i$ 表示第 $i$ 个观测。在线性回归问题中我们的核心任务就是估计出未知参数 $\\beta_0$, $\\beta_1$, $\\cdots$, $\\beta_p$ 的值。\n 注意，线性回归问题的确定并不是通过自变量的形式，而是问题中待估计的未知参数最高次都为一次且关于未知参数呈线性关系。例如 $y = \\beta_0 + \\beta_1x_1^2 + \\epsilon$；$y = \\beta_0 + \\beta_1x_1x_2 + \\epsilon$ 都是线性回归问题。\n 通常在线性回归中估计未知参数方法是最小二乘法（OLS），而为了保证估计值能够很好的解释模型，我们又有如下前提条件：\n 正态性：$\\epsilon_i$ 服从正态分布； 独立性：$\\epsilon_i$ 之间是独立的； 线性性：$x$ 和 $y$ 必须线性相关； 同方差性：$\\epsilon_i$ 的方差不变。  这些条件又被称为高斯—马尔可夫条件， 它们保证了在经典线性回归中最小二乘估计的优越性。\n3 求解线性回归模型函数 3.1 极大似然法 最小二乘法和极大似然法都可以用来求解线性回归模型，我们在往期文章中讨论过最小二乘法，这里对似然法进行简单介绍。\n假设我们得到下面一组观测数据：\n$$ (x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n) $$\n那么根据高斯-马尔可夫假设，我们可以得到残差估计的似然函数 为\n$$ \\mathscr{L} = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(y_i - x_i^T\\hat{\\beta} - \\hat{\\beta}_0)^2}{2\\sigma^2}\\right] $$\n 这个式子的成立还需要假设残差分布的均值为0，标准差为 $\\sigma$。这个假设是可行的。因为残差如果均值不为零，可以将其移到模型的截距项里。\n 如何通过上面的函数得到系数的估计值呢？极大似然的思想便是，让这些估计值使得似然函数达到最大！ 这个想法很朴素：每个观测数据随机且互相独立，我们一次搜集便得到眼前的数据，那么自然而然认为这些数据组合出现的概率是最大的。\n不过，数据已经搜集好便不能改动。我们自然想到，系数的估计值便是让这些数据对应的概率可能性最大——也即是似然函数最大。\n现在假装大家已经理解了极大似然的原理，下面我们来求解它。直接最大化不太可行，我们通常对似然函数取对数得到对数似然函数\n$$ \\ln\\mathscr{L} = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\sum_{i = 1}^n\\frac{(y_i - x_i^T\\hat{\\beta} - \\hat{\\beta}_0)^2}{2\\sigma^2} $$\n然后再分别对各个参数进行优化。限于篇幅，不再赘述。\n3.2 R求解线性回归模型 我们可以利用现有软件进行模型求解。在R中求解线性回归问题的最基本函数就是lm()，其格式为：\nmyfit \u0026lt;- lm(formula, data)\r# formula 是要拟合的模型形式，用一个R公式表示\r# data 就是模型的数据构成的数据框\r 下面我们解释一下formula具体的形式，首先看下表总结的formula中常用的符号\n   符号 说明     ~ 分隔符号，左边为因变量，右边为自变量   + 分隔自变量   : 自变量的交互项，如 xz 可以表示成 x:z   * 自变量的所有交互项，如 x*z*w 展开即为 x+z+w+x:z+x:w+z:w+x:z:w   ^ 交互项可以达到某个次数，如(x+z+w)^2展开即为x+z+w+x:z+x:w+z:w   . 除因变量外的所有自变量   -1 删除截距项   I() 如x+I((z+w)^2)等价于x+h，h是z+w平方构成的新变量    如果自变量为 $x_1$, $x_2$ 和 $x_3$ 而预测变量为 $y$，我们假定的线性模型形式为：\n$$ y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + \\epsilon $$\n那么formula可以写成：\ny ~ x1 + x2 + x3\r# 或者为 y ~ .\r 其他形式的模型formula的表达式还请读者自行琢磨。\n当模型拟合成功后，我们使用summary()函数来得到拟合的具体结果。而其他常用的获取线性回归模型拟合结果的函数如下表所示。\n   函数 说明     summary() 拟合情况概述，包括系数显著性、模型显著性等   coefficients() 拟合系数   fitted() 因变量拟合值   residuals() 残差值   plot() 画出模型诊断图    4 实例分析 下面我们将用实例具体介绍lm()函数的使用方法。\n4.1 简单线性回归 本例中我们使用基础安装中的数据集women数据，它记录了15个年龄在30~39岁间女性的身高和体重信息，我们现在来探究体重关于身高的关系。\nmyfit \u0026lt;- lm(weight~height, data = women)\rsummary(myfit) # 展示拟合详细结果\r 程序的输出结果如下所示\n这里主要给读者解释这么几项指标的含义：\n  Residuals 体重预测值和真实值之差的统计信息，从左到右分别为最小值、下四分位数、中位数、上四分位数和最大值。\n  Coefficients 第一列Estimate中Intercept对应的数值为截距项，height对应的即为身高变量前的估计系数。\n  Multiple R-squared 介于0-1之间，越接近1说明线性关系越强。\n  p-value 模型的F检验统计量的p值，值越小说明模型越可靠。\n  因此本例中体重和身高的回归方程为：\n$$ \\hat{Weight} = -87.51667 + 3.45\\times Height $$\n根据R方 (Multiple R-squared) 和p值 (p-value) 可知模型是可靠的。此外，我们可以作图观察最终的拟合结果。\n4.2 具有交互项的线性回归 继续考虑上例，如果模型中存在一个交互项比如一个平方项，那么即有：\nmyfit \u0026lt;- lm(weight~height + I(height^2), data = women)\rsummary(myfit) # 展示拟合详细结果\r 程序的输出结果如下所示。\n可以看到通过比较R方、p值，添加了平方项的线性模型效果更好。我们同样可以做出相应的图像。 5 写在最后 本文主要介绍了R中线性回归分析的简单操作方法。不过，这里仅仅涉及线性回归分析的冰山一角，关于线性回归问题中的回归诊断和异常点的判断等内容，限于篇幅这里就不做介绍了。有兴趣的读者可以学习《R in action》第8章中关于回归的讲解。\n","id":8,"section":"posts","summary":"1 导言 回归分析是一个古老的话题。一百多年前，英国的统计学家高尔顿 (F. Galton，1822-1911) 和他的学生皮尔逊 (K. Pearson，185","tags":["多元统计","r语言","回归分析"],"title":"简单回归分析的R实现","uri":"https://jiandan94.github.io/2019/07/simplelm/","year":"2019"},{"content":"远山疏林轻雾笼，暖阳探窗听晨钟。\n庭前初落银杏雨，芳华凋零又几重。\n不期秋霜侵柳绿，可怜金风老梧桐。\n昨日阆苑尚新蕊，行人行色行匆匆。\n","id":9,"section":"posts","summary":"远山疏林轻雾笼，暖阳探窗听晨钟。 庭前初落银杏雨，芳华凋零又几重。 不期秋霜侵柳绿，可怜金风老梧桐。 昨日阆苑尚新蕊，行人行色行匆匆。","tags":["虎溪岁月","诗文"],"title":"久雨初晴有感","uri":"https://jiandan94.github.io/2018/11/jycqyg/","year":"2018"},{"content":"晴岚裙腰山染黛，\n空谷幽幽暗香来。\n时人剪却好风光，\n月下说与娇娘猜。\n","id":10,"section":"posts","summary":"晴岚裙腰山染黛， 空谷幽幽暗香来。 时人剪却好风光， 月下说与娇娘猜。","tags":["虎溪岁月","诗文"],"title":"七夕","uri":"https://jiandan94.github.io/2018/08/qx/","year":"2018"},{"content":"本文总结了广义估计方程中的参数估计，并推导了用于求解模型的Gauss—Newton迭代法。此外，针对每个内容，本文还给出了相应的R软件求解算法，并做了相应的模拟。\n1 广义估计方程 假定观测数据中有 $y_1,\\cdots ,y_n$ 个体，对于个体 $y_i$ 有 $y_{i1},\\cdots ,y_{im}$ 个观测。其中对于 $y_1,\\cdots ,y_n$ 个体之间是相互独立的，而每个个体内的观测 $y_{i1},\\cdots ,y_{im}$ 则是具有相关性的。对于观测 $y_{ij}$ 对应有 $\\eta_{ij}$，满足 $\\eta _{ij}=x_{ij_1}\\beta _1+\\cdots +x _{ij_p}\\beta _p$。相应的对 $y_{ij}$ 有假设 $\\mu _{ij}=Ey _{ij}$，$\\mathrm{Var}y_{ij}=a\\left( \\phi \\right) \\mathrm{Var}_{\\mu _{ij}}$，并且 $\\eta_{ij}$ 和 $\\mu_{ij}$ 之间存在链接函数 $g(x)$ 使得 $\\eta _{ij}=g\\left( \\mu _{ij} \\right) =x _{ij}^{\\mathrm{T}}\\beta $。那么，估计参数 $\\beta$ 就是广义估计方程中的一项重要任务。\n2 IEE 由于纵向数据中组内观测之间存在相关性，所以不能直接离用GLM的结果。为了推导出GEE的参数估计的表达式，首先假设纵向数据组内也是相互独立的。\n 这么做，是先通过一个特殊的清醒理解广义估计方程，然后再将得到的方法进行一般化推广，使之可以适用于广义估计方程。\n 另一方面，我们注意到在广义估计方程中没有分布的信息，只有均值和方差的假设。因而，极大似然法在这里不能直接使用。（没有线性关系的保证和正态分布假设，最小二乘显然也不能使用）\n这里，我们利用拟似然的方法来求解模型。 似然方法只利用到了前二阶矩信息，那么在给定均值和方差的条件下，我们可以仿照极大似然的思路构造似然函数，这就是拟似然的思想。\n首先构造 $S_{ij}\\left( \\mu _{ij} \\right) $\n$$ S_{ij}\\left( \\mu _{ij} \\right) =\\frac{y_{ij}-\\mu_{ij}}{\\mathrm{Var}y_{ij}}=\\frac{y_{ij}-\\mu _{ij}}{a\\left( \\phi \\right) \\mathrm{Var} _{\\mu _{ij}}} $$\n那么相应的有\n$$ \\theta_{ij}\\left( \\mu_{ij} \\right) =\\int_{y_{ij}}^{\\mu _{ij}}{S _{ij}\\left( t \\right) \\mathrm{d}t} $$\n接着根据上述结果得到得分函数\n$$ \\begin{split} S\\left( \\beta \\right) \u0026amp;=\\frac{\\partial \\theta \\left( \\mu \\right)}{\\partial \\beta} = \\sum_j{\\sum_j \\frac{\\partial \\theta_{ij}}{\\partial \\beta}} = \\sum_j{\\sum_j \\frac{\\partial \\mu_{ij}}{\\partial \\beta}S_{ij}} \\newline \u0026amp;= \\sum_j{\\sum_j \\frac{\\partial \\mu_{ij}}{\\partial \\beta}\\cdot \\frac{y_{ij} - \\mu_{ij}}{a(\\phi)\\mathrm{Var}\\mu_{ij}}} \\newline \u0026amp;= \\sum_i {D_i^T V_i^{-1}(y_i - \\mu_i)} \\end{split} $$\n其中 $D_{i}^{\\mathrm{T}}=\\Delta _iX_i$，这里 $X_i$ 表示第 $i$ 次的观测矩阵，$\\Delta_i$ 则为对角阵\n$$ \\Delta _i=\\left( \\begin{matrix} \\dot{h}\\left( \\eta _{i1} \\right)\u0026amp;\t\u0026amp;\t\u0026amp;\t\\newline \u0026amp;\t\\dot{h}\\left( \\eta _{i2} \\right)\u0026amp;\t\u0026amp;\t\\newline \u0026amp;\t\u0026amp;\t\\ddots\u0026amp;\t\\newline \u0026amp;\t\u0026amp;\t\u0026amp;\t\\dot{h}\\left( \\eta _{im} \\right)\\newline \\end{matrix} \\right) $$\n3 GEE 根据上述求解的思想，对于GEE我们有\n$$ S\\left( \\beta \\right) =\\sum_i{D_{i}^{\\mathrm{T}}\\left( \\mathrm{Cov}y_i \\right) ^{-1}\\left( y_i-\\mu _i \\right)} $$\n其中 $\\mathrm{Cov}y_i=A_{i}^{1/2}R_iA_{i}^{1/2}$。显然，$\\beta$ 要计算参数 我们需要知道 $R_i$（组内相关结构），显然问题中 $R_i$ 未知，故而为了估计得以进行须给定 $R_{i0}$，称之为working correlation matrix。\n这样，我们令\n$$ D=\\left( \\begin{array}{c} D_1\\newline \\vdots\\newline D_n\\newline \\end{array} \\right) , \\quad V=\\left( \\begin{matrix} \\mathrm{Cov}y_1\u0026amp;\t\u0026amp;\t\\newline \u0026amp;\t\\ddots\u0026amp;\t\\newline \u0026amp;\t\u0026amp;\t\\mathrm{Cov}y_n\\newline \\end{matrix} \\right), \\quad y=\\left( \\begin{array}{c} y_1\\newline \\vdots\\newline y_n\\newline \\end{array} \\right), \\quad \\mu =\\left( \\begin{array}{c} \\mu _1\\newline \\vdots\\newline \\mu _n\\newline \\end{array} \\right) $$\n那么 $S(\\beta)$ 可以写成\n$$ S\\left( \\beta \\right) =D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) $$\n则相应的加权迭代最小二乘法可以写成\n$$ \\beta =\\left( D^{\\mathrm{T}}V^{-1}D \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\Delta \\left[ \\Delta ^{-1}\\left( y-\\mu \\right) +X\\beta \\right] $$\n4 数据模拟 根据前面的分析可以编写求解模型的R代码，这些都附在最后。\n本实验中我们考虑10个观测个体，每个个体观测100次，共有1000次观测。数据按照如下方式产生：设计矩阵 $X$ 是服从 $[0,0.1]$ 均匀分布，系数给定为 $\\beta =\\left( 1,3,2,4,5 \\right) ^{\\mathrm{T}}$，链接函数 $g(x) = \\ln(x)$，组内协方差矩阵先生成 $100\\times100$ 的服从 $[0,1]$ 均匀分布的矩阵，在将之和其转置相乘，取乘积的0.4倍作为组内协方差矩阵。这样，使用GEE模型，初始迭代向量取 $\\left( 1.2,3.5,2.1,4.2,5.5 \\right)$ 进行求解模型得到如下结果\n可见最终的效果还是可以的。\n5 程序代码 ##This program is quasi-likelihood method\rmygee \u0026lt;- function(x,y,beta1,v.inverse,N = 5000,e = 1e-10){#\r# x is the design matrix\r# b is the starting value of the iteration\r# N is the upper bound of the times of the iteration\r# e is the convergence criteria\r# v.inverse is the inverse of the covariance function of y\rn \u0026lt;- length(x)\rx \u0026lt;- as.matrix(x)\ry \u0026lt;- as.matrix(y)\rh \u0026lt;- expression(exp(eta))# the inverse of the link funtion\rdh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h\rk \u0026lt;- 1\rbeta0 \u0026lt;- beta1 + 1\rwhile(sum((beta0 - beta1)^2) \u0026gt;= e){\rbeta0 \u0026lt;- beta1\r# compute the D matrix\reta \u0026lt;- x%*%beta1\rdelta \u0026lt;- diag(as.vector(eval(dh)))\rD \u0026lt;- delta%*%x\r# compute v.inverse—the inverse matrix of the covariance matrix of y\rmu \u0026lt;- eval(h)\rp1 \u0026lt;- solve(t(D)%*%v.inverse%*%D)\rp2 \u0026lt;- t(D)%*%v.inverse%*%delta\rp3 \u0026lt;- x%*%beta1 + solve(delta)%*%(y - mu)\rbeta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates\r# check if it is divergent\rif(k \u0026gt; N){\rcat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rprint(beta1)\rbreak\r}\relse{\rk \u0026lt;- k + 1\r}\r}\rcolnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;)\rrownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1))\r# eta \u0026lt;- x%*%beta1\r# y.fit \u0026lt;- eval(h)# compute the fitting values of y\rquasi.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1)\rreturn(quasi.result)\r}\r ","id":11,"section":"posts","summary":"本文总结了广义估计方程中的参数估计，并推导了用于求解模型的Gauss—Newton迭代法。此外，针对每个内容，本文还给出了相应的R软件求解算","tags":["多元统计","纵向数据分析"],"title":"广义估计方程简介","uri":"https://jiandan94.github.io/2018/06/gee/","year":"2018"},{"content":"1 指数族分布与广义线性模型 1.1 引入指数族分布 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题中 $y$ 并不总是满足正态分布的假设。因此，我们考虑更加一般的指数族分布。\n指数族分布定义如下：\n$$ f\\left( y;\\theta ,\\phi \\right) =\\exp \\left[ \\frac{y\\theta -b\\left( \\theta \\right)}{a\\left( \\phi \\right)}+c\\left( y,\\phi \\right) \\right] $$\n其中 $a(\\cdot)$，$b(\\cdot)$ 和 $c(\\cdot)$ 是已知给定的函数。参数 $\\theta$ 为分布族的位置参数（location parameter），参数 $\\phi$ 通常被称为分散参数（dispersion parameter）。一般来说，函数 $a(\\phi)$ 通常有形式 $a(\\phi)=\\phi\\cdot\\omega$，其中 $\\omega$ 是一个已知的常数。指数族分布包括常见的二项分布、泊松分布、正态分布和指数分布等。\n1.2 联系回归问题 前面直接给出指数族分布，有点让人一时难以和回归问题构建联系。我们不妨回忆线性模型的内容，此时响应变量 $y$ 满足下面的正态分布\n$$ N(x^T\\beta, \\sigma^2) $$\n我们写出它的密度函数具体表达式\n$$ f\\left( y \\right) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[ -\\frac{(y - x\\beta)^2}{2\\sigma^2} \\right] = \\exp\\left[ -\\frac{(y^2 - 2yx\\beta + \\beta^Tx^Tx\\beta)}{2\\sigma^2} - \\frac{1}{2} \\ln(2\\pi\\sigma^2) \\right] $$\n整理一下就得到\n$$ f\\left( y \\right) = \\exp\\left[ \\frac{yx\\beta - 1/2\\beta^Tx^Tx\\beta}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2} \\ln(2\\pi\\sigma^2) \\right] $$\n原来线性回归问题就是 $\\theta = x\\beta$ 和 $\\phi = \\sigma^2$ 的指数族问题。 因此，我们保留线性模型中的线性结构假设，把正态分布约束推广成指数族分布，这就得到的更为一般的广义线性模型。\n对应线性模型的假设，广义线性模型也有若干前提假设：\n 观测 $y_1,\\cdots ,y_n$ 是相互独立的，且对应的均值为 $\\mu _1,\\mu _2,\\cdots ,\\mu _n$； 每个观测 $y_i$ 具有指数族分布； 模型建立在线性预测因子 $\\eta _1,\\cdots ,\\eta _n$ 上，其中 $\\eta _i=x _{i}^{\\mathrm{T}}\\beta $，$x_i$ 是设计矩阵第 $i$ 个行向量； 模型通过链接函数（link function）建立，其中：$\\eta _i=g\\left( \\mu _i \\right) ,\\ i=1,2,\\cdots ,n$； 链接函数是单调可微的（其反函数存在）。  不难看出待求参数 $\\beta$ 和指数族分布之间的关系链接：\n$$ \\beta \\overset{\\eta _i=g\\left( \\mu _i \\right)}{\\leftrightarrow}\\mu _i\\overset{\\mu _i=\\dot{b}\\left( \\theta _i \\right)}{\\leftrightarrow}\\theta _i $$\n上述这种关系对我们逐步得到参数 $\\beta$ 的求解公式意义重大。\n2 广义线性模型的参数估计 在线性模型的假设下，最小二乘法和极大似然法都能用于参数的求解。在广义线性模型中，我们无法写出二乘形式的优化函数。因此，我们根据分布信息利用极大似然法来估计参数。 模型的似然函数 $\\mathscr{L}(\\theta;Y)$ 为\n$$ \\mathscr{L}(\\theta;Y) = \\prod _{i=1}^n \\exp \\left[ \\frac{y_i\\theta_i -b\\left( \\theta-i \\right)}{a\\left( \\phi_i \\right)}+c\\left( y_i,\\phi_i \\right) \\right] $$\n为了方便理解算法的具体推导过程，下面首先介绍指数族的两个重要结论，然后再具体推导求解算法。\n2.1 两个重要的结论 对于指数族分布，首先讨论两个重要的结论。根据密度函数可以得到相应的对数似然函数：\n$$ \\ln \\mathscr{L}\\left( \\theta ;Y \\right) =\\sum_{i=1}^n{\\frac{y_i\\theta _i-b\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}}+\\sum_{i=1}^n{c\\left( y_i,\\phi _i \\right)} $$\n这里仅假定 $y_i$ 是独立的。则似然函数对 $\\theta$ 求导有：\n$$ \\frac{\\partial \\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta}=\\sum_{i=1}^n{\\frac{y_i-\\dot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}}=\\sum_{i=1}^n{S_i} $$\n似然函数对 $\\theta$ 的二阶导为：\n$$ \\frac{\\partial ^2\\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta ^2}=\\sum_{i=1}^n{\\frac{\\ddot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}} $$\n其中 $\\dot{b}(\\theta)$ 和 $\\ddot{b}(\\theta)$ 是 $b(\\theta)$ 对 $\\theta$ 的一阶导和二阶导（下同）。那么，大部分指数族分布（要求密度函数积分号和求导号可以交换顺序）满足：\n$$ E\\left( \\frac{\\partial \\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta} \\right) =0, \\quad E\\left( \\frac{\\partial \\ln \\mathscr{L}\\left( \\theta ;Y \\right)}{\\partial \\theta} \\right) =0 $$\n显然，根据上述两条性质可以推出：\n$$ \\mu _i=Ey_i=\\dot{b}\\left( \\theta _i \\right) $$\n和\n$$ Var\\left( y_i \\right) =\\ddot{b}\\left( \\theta _i \\right) a\\left( \\phi \\right) =\\frac{\\mathrm{d}\\mu _i}{\\mathrm{d}\\theta _i}a\\left( \\phi \\right) = Var _{\\mu _i}a\\left( \\phi \\right) $$\n2.2 极大似然估计 根据极大似然思想，对数似然函数对 $\\beta$ 求导得到：\n$$ \\begin{split} S\\left( \\beta \\right) \u0026amp;=\\frac{\\partial \\ln \\mathscr{L}\\left( \\beta ;Y \\right)}{\\partial \\beta}=\\sum_{i=1}^n{\\frac{\\partial}{\\partial \\theta _i}\\left( \\frac{y_i\\theta _i-b\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)} \\right) \\cdot \\frac{\\partial \\theta _i}{\\partial \\mu _i}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}} \\newline \u0026amp;=\\sum_{i=1}^n{\\frac{y_i-\\dot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}\\cdot \\frac{\\partial \\theta _i}{\\partial \\mu _i}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}}=\\sum_{i=1}^n{\\frac{y_i-\\dot{b}\\left( \\theta _i \\right)}{a\\left( \\phi _i \\right)}\\cdot \\frac{1}{\\frac{\\partial \\mu _i}{\\partial \\theta _i}}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}} \\newline \u0026amp;=\\sum_{i=1}^n{\\left( y_i-\\dot{b}\\left( \\theta _i \\right) \\right) \\frac{1}{a\\left( \\phi _i \\right) \\frac{\\partial \\mu _i}{\\partial \\theta _i}}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}}=\\sum_{i=1}^n{\\left( y_i-\\mu _i \\right) \\frac{1}{Var\\left( y_i \\right)}\\cdot \\frac{\\partial \\mu _i}{\\partial \\beta}} \\newline \u0026amp;=\\left( \\frac{\\partial \\mu _1}{\\partial \\beta},\\cdots ,\\frac{\\partial \\mu _n}{\\partial \\beta} \\right) \\left( \\begin{matrix} \\frac{1}{Var\\left( y_1 \\right)}\u0026amp;\t\u0026amp;\t\\newline \u0026amp;\t\\ddots\u0026amp;\t\\newline \u0026amp;\t\u0026amp;\t\\frac{1}{Var\\left( y_n \\right)}\\newline \\end{matrix} \\right) \\left( \\begin{array}{c} y_1-\\mu _1\\newline \\newline y_n-\\mu _n\\newline \\end{array} \\right) \\newline \u0026amp;=\\frac{\\partial \\mu ^{\\mathrm{T}}}{\\partial \\beta}V^{-1}\\left( y-\\mu \\right) =\\left( \\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\newline \u0026amp;=D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\end{split} $$\n显然，这和之前求解得到的得分函数形式是一致的。为了得到具体的表达式，需要进一步求解 $D$ 的具体形式。\n因为 $D=\\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}}$，而 $x_{i}^{\\mathrm{T}}\\beta =\\eta _i=g\\left( \\mu _i \\right) $，考虑到链接函数 $g(\\cdot)$ 是单调可微函数，则其反函数存在，不妨设为 $h(\\cdot)$，所以\n$$ \\frac{\\partial \\mu _i}{\\partial \\beta}=\\frac{\\partial}{\\partial \\beta}h\\left( x _{i}^{\\mathrm{T}}\\beta \\right) =\\dot{h}\\left( x _{i}^{\\mathrm{T}}\\beta \\right) x_i $$\n进而\n$$ \\begin{split} D\u0026amp;=\\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}}=\\left( \\frac{\\partial \\mu}{\\partial \\beta _1},\\cdots ,\\frac{\\partial \\mu}{\\partial \\beta _n} \\right) =\\left( \\dot{h}\\left( x _{1}^{\\mathrm{T}}\\beta \\right) x_1,\\cdots ,\\dot{h}\\left( x _{n}^{\\mathrm{T}}\\beta \\right) x _n \\right) \\newline \u0026amp;=\\left( \\begin{matrix} \\dot{h}\\left( x _{1}^{\\mathrm{T}}\\beta \\right)\u0026amp;\t\u0026amp;\t\\newline \u0026amp;\t\\ddots\u0026amp;\t\\newline \u0026amp;\t\u0026amp;\t\\dot{h}\\left( x _{n}^{\\mathrm{T}}\\beta \\right)\\newline \\end{matrix} \\right) \\left( x _1,\\cdots ,x _n \\right) \\newline \u0026amp;=\\Delta X \\end{split} $$\n所以得到 $S(\\beta)$ 的最终表达式为\n$$ S\\left( \\beta \\right) =\\left( \\Delta X \\right) ^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) =X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) $$\n为了得到极大似然估计，我们接着求解对应的Fisher信息矩阵。根据定义有：\n$$ I\\left( \\beta \\right) =E\\left( \\frac{\\partial}{\\partial \\beta ^{\\mathrm{T}}}S\\left( \\beta \\right) \\right) =E\\left( \\frac{\\partial ^2\\ln \\mathscr{L}}{\\partial \\beta ^{\\mathrm{T}}\\partial \\beta} \\right) $$\n根据指数族的性质有\n$$ E\\left( \\frac{\\partial ^2\\ln \\mathscr{L}}{\\partial \\beta ^{\\mathrm{T}}\\partial \\beta} \\right) =-E\\left( SS^{\\mathrm{T}} \\right) $$\n考虑其第 $(i,j)$ 个元素 $S_{ij}=S_iS_j$，则\n$$ \\begin{split} E\\left( S_iS_j \\right) \u0026amp;=E\\left[ x _{i}^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) x _{j}^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\right] \\newline \u0026amp;=E\\left[ x _{i}^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\left( y-\\mu \\right) ^{\\mathrm{T}}V^{-1}\\Delta x _j \\right] \\newline \u0026amp;=x _{i}^{\\mathrm{T}}\\Delta V^{-1}\\Delta x _j \\end{split} $$\n所以\n$$ I\\left( \\beta \\right) =X^{\\mathrm{T}}\\Delta V^{-1}\\Delta X $$\n这时，再回到头考虑得分函数，根据极值的必要条件有\n$$ S\\left( \\hat{\\beta} \\right) =X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) =0 $$\n将上式在真值 $\\beta$ 处进行Taylor展开\n$$ S\\left( \\hat{\\beta} \\right) =S\\left( \\beta \\right) +\\frac{\\partial}{\\partial \\beta ^{\\mathrm{T}}}S\\left( \\beta \\right) \\left( \\hat{\\beta}-\\beta \\right) =0 $$\n所以\n$$ \\hat{\\beta}=\\beta +\\left( -\\frac{\\partial S\\left( \\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{-1}S\\left( \\beta \\right) $$\n其中括号内的一项过于复杂，在实际求解中可以根据大数定律用其期望代替，由前面的讨论可知其期望即为 $I(\\beta)$。这样，模型的极大似然估计可以近似成\n$$ \\begin{split} \\hat{\\beta}\u0026amp;\\doteq \\beta +\\left( X^{\\mathrm{T}}\\Delta V^{-1}\\Delta X \\right) ^{-1}X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\newline \u0026amp;=\\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\end{split} $$\n2.3 渐近正态性 根据得到的估计量的近似表达式，其中等式右侧只有 $y$ 是服从正态分布的随机变量，那么得到的参数估计可以看成正态随机变量一个线性组合，显然估计量也服从正态分布，故而只需要求其均值和方差即可。\n$\\hat{\\beta}$ 的均值\n$$ E\\hat{\\beta}=E\\left[ \\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\right] =\\beta $$\n$\\hat{\\beta}$ 的方差\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left[ \\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\right] \\newline \u0026amp;=Var\\left[ I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\right] \\newline \u0026amp;=I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}Var\\left( y-\\mu \\right) V^{-1}DI\\left( \\beta \\right) ^{-1} \\newline \u0026amp;=I\\left( \\beta \\right) ^{-1}\\left( D^{\\mathrm{T}}V^{-1}D \\right) I\\left( \\beta \\right) ^{-1} \\newline \u0026amp;=I\\left( \\beta \\right) ^{-1} \\end{split} $$\n所以，可知估计量服从 $N\\left( \\beta ,I\\left( \\beta \\right) ^{-1} \\right)$。有了渐近分布，我们就能构造显著性检验了。\n2.4 拟似然方法 广义线性模型中对于分布有前提假设，但是实际问题中并不知道具体的分布是什么。考虑到在求解问题中一个重要的信息就是信息函数，它和分布前两阶矩有关，故而可以假设分布的前两阶矩存在，从而推出不含分布信息的拟似然方法。\n假定响应变量 $y_i$ 的均值为 $\\mu_i$，方差函数为 $\\mathrm{Var}y_i=a\\left( \\phi \\right) \\mathrm{Var} _{\\mu _i}$，并且假定均值 $\\mu_i$ 和 $x_i^T\\beta$ 之间存在链结函数，链结函数的性质和广义线性模型的链结函数性质一样。据此，根据广义线性模型的思想实施拟似然方法。\n构造逆得分函数\n$$ S_i\\left( \\mu _i \\right) =\\frac{y_i-\\mu _i}{\\mathrm{Var}y_i} $$\n它满足\n$$ \\begin{cases} ES_i\\left( \\mu _i \\right) =0\u0026amp;\t\\newline ES _{i}^{2}=E\\left( -\\frac{\\partial S_i}{\\partial \\mu _i} \\right)\u0026amp;\t\\newline \\end{cases} $$\n其中第二条性质是因为\n$$ \\begin{split} E\\left( -\\frac{\\partial S_i}{\\partial \\mu _i} \\right) \u0026amp;=-E\\left( \\frac{\\partial}{\\partial \\mu _i}\\left( \\frac{y_i-\\mu _i}{\\mathrm{Var}y_i} \\right) \\right) \\newline \u0026amp;=\\frac{1}{\\mathrm{Var}y_i}=E\\left( S _{i}^{2} \\right) \\end{split} $$\n所以根据广义线性模型极大似然思想有\n$$ \\theta _i=\\int _{y_i}^{\\mu _i}{S_i\\left( t \\right) \\mathrm{d}t}, \\quad \\theta \\left( \\mu \\right) =\\sum _{i=1}^n{\\theta \\left( \\mu _i \\right)}=\\theta \\left( \\beta \\right) $$\n那么拟似然方法的得分函数可以写成\n$$ S\\left( \\beta \\right) =\\frac{\\partial}{\\partial \\beta}\\theta \\left( \\beta \\right) =\\sum_{i=1}^n{\\frac{\\partial \\theta _i}{\\partial \\beta}=\\sum_{i=1}^n{\\frac{\\partial \\mu _{i}^{\\mathrm{T}}}{\\partial \\beta}\\frac{\\partial \\theta _i}{\\partial \\mu _i}=D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right)}} $$\n类似的可以得到Fisher信息矩阵为\n$$ I\\left( \\beta \\right) =D^{\\mathrm{T}}V^{-1}D $$\n3 迭代求解算法 考虑模型的极大似然估计，其中括号内的一项过于复杂，在实际求解中可以根据大数定律用其期望代替，由前面的讨论可知其期望即为 $I(\\beta)$。这样，模型的极大似然估计可以近似成\n$$ \\begin{split} \\hat{\\beta}\u0026amp;\\doteq \\beta +\\left( X^{\\mathrm{T}}\\Delta V^{-1}\\Delta X \\right) ^{-1}X^{\\mathrm{T}}\\Delta V^{-1}\\left( y-\\mu \\right) \\newline \u0026amp;=\\beta +I\\left( \\beta \\right) ^{-1}D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) \\end{split} $$\n这样就得到了估计量的求解迭代公式。\n4 模拟分析 在R中，我们有glm函数求解广义线性模型。这里，我结合前面的分析自己编写的相应的参数估计和假设检验的函数，包括二项分布（逻辑回归）、Poisson分布。这些代码附在最后供参考。\n以Poisson分布举例，考虑符合Poisson分布的观测，其中链接函数为\n$$ \\eta _i=\\ln \\mu _i $$\n我们生成 $\\mathbb{R}^{1000\\times 5}$ 的设计矩阵 $X$，系数 $\\beta =\\left( 1,3,2,4,5 \\right) ^{\\mathrm{T}}$，使用自编的广义线性模型求解算法求解该模型得到\n这里的myglm是我自编的广义线性模型求解函数，其中囊括了线性模型、二项分布模型（逻辑回归）和Poisson模型的参数估计和显著性检验。对比一下R中的glm包求解的结果\n可以看到结果几乎是一样的。\n5 自编函数代码 5.1 模型求解 实际使用，主要运行该代码即可。这个代码整合了参数估计、检验等代码，将结果合并输出。\n## This program is to solve the generalized linear models: logistic, poisson myglm \u0026lt;- function(x,y,b0,alpha = 0.05,family = \u0026quot;poisson\u0026quot;){# options(digits = 4) # family has two choice: logistic and poisson # alpha is the significance of the interval estimate of the coefficients # solve the model if(family == \u0026quot;logistic\u0026quot;){ glm.result \u0026lt;- myglmlogistic(x,y,beta1 = b0) } else if(family == \u0026quot;poisson\u0026quot;){ glm.result \u0026lt;- myglmpoisson(x,y,beta1 = b0) } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic,poisson之一\u0026quot;)) } b \u0026lt;- glm.result$模型的解 y.fit \u0026lt;- glm.result$拟合值 num \u0026lt;- glm.result$算法迭代次数 # hypothesis testing indivi \u0026lt;- myglmindivi(x,y,b,expr = family) # confidence interval inter.sig \u0026lt;- myglminterval(x,y,b,alpha = alpha,expr = family) bname \u0026lt;- paste0(rep(\u0026quot;beta\u0026quot;,length(b)),1:length(b)) # the residuls # glm.residual \u0026lt;- y - y.fit # res1 \u0026lt;- summary(glm.residual) # names(res1) \u0026lt;- c(\u0026quot;最小值\u0026quot;,\u0026quot;下四分位数\u0026quot;,\u0026quot;中位数\u0026quot;,\u0026quot;均值\u0026quot;,\u0026quot;上四分位数\u0026quot;,\u0026quot;最大值\u0026quot;) estimate.test \u0026lt;- data.frame( \u0026quot;估计值\u0026quot; = b, \u0026quot;下界\u0026quot; = inter.sig[,2], \u0026quot;上界\u0026quot; = inter.sig[,3], \u0026quot;z值\u0026quot; = indivi$Tu, \u0026quot;P值(\u0026gt;|z|)\u0026quot; = indivi$Tp, \u0026quot;置信度\u0026quot; = indivi$Ts,check.names = F ) # output the results cat(\u0026quot;\\n\u0026quot;) cat(\u0026quot;Call: 这是不带截距项的\u0026quot;,family,\u0026quot;模型,\u0026quot;,\u0026quot;下面是模型的分析结果：\u0026quot;,seq = \u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;\\n\u0026quot;) cat(\u0026quot;参数估计结果:\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;\\n\u0026quot;) print(estimate.test) cat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;置信度: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;区间估计的置信水平为：\u0026quot;,alpha,\u0026quot;\\n\u0026quot;) cat(\u0026quot;---\u0026quot;,\u0026quot;\\n\u0026quot;) cat(\u0026quot;Fisher信息矩阵的迭代次数为:\u0026quot;,num,\u0026quot;次\u0026quot;,\u0026quot;\\n\u0026quot;) }  5.2 Logistic模型的解 ##本程序用来求解广义线性模型中——logistic模型的解 myglmlogistic \u0026lt;- function(x,y,beta1,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) x \u0026lt;- as.matrix(x) y \u0026lt;- as.matrix(y) g \u0026lt;- expression(log(mu/(1 - mu)))# the link function h \u0026lt;- expression(1/(1 + exp(-eta)))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(log(1 + exp(theta)))# the b(theta) function of the pdf db \u0026lt;- D(b,\u0026quot;theta\u0026quot;)# the first derivative funtion of b ddb \u0026lt;- D(db,\u0026quot;theta\u0026quot;)# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu/(1 - mu)))# the inverse of db k \u0026lt;- 1 beta0 \u0026lt;- beta1 + 1 while(sum((beta0 - beta1)^2) \u0026gt;= e){ beta0 \u0026lt;- beta1 # compute the D matrix eta \u0026lt;- x%*%beta1 delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x # compute v.inverse—the inverse matrix of the covariance matrix of y mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb))) p1 \u0026lt;- solve(t(D)%*%v.inverse%*%D) p2 \u0026lt;- t(D)%*%v.inverse%*%delta p3 \u0026lt;- x%*%beta1 + v.inverse%*%(y - mu) beta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(beta1) break } else{ k \u0026lt;- k + 1 } } colnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1)) eta \u0026lt;- x%*%beta1 y.fit \u0026lt;- eval(h)# compute the fitting values of y glmlogistic.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1,\u0026quot;拟合值\u0026quot; = y.fit) return(glmlogistic.result) }  5.3 Poisson模型的解 ##本程序用来求解广义线性模型中——Poisson模型的解 myglmpoisson \u0026lt;- function(x,y,beta1,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) x \u0026lt;- as.matrix(x) y \u0026lt;- as.matrix(y) g \u0026lt;- expression(log(mu))# the link function h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(exp(theta))# the b(theta) function of the pdf db \u0026lt;- expression(exp(theta))# the first derivative funtion of b ddb \u0026lt;- expression(exp(theta))# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu))# the inverse of db k \u0026lt;- 1 beta0 \u0026lt;- beta1 + 1 while(sum((beta0 - beta1)^2) \u0026gt;= e){ beta0 \u0026lt;- beta1 # compute the D matrix eta \u0026lt;- x%*%beta1 delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x # compute v.inverse—the inverse matrix of the covariance matrix of y mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb))) p1 \u0026lt;- solve(t(D)%*%v.inverse%*%D) p2 \u0026lt;- t(D)%*%v.inverse%*%delta p3 \u0026lt;- x%*%beta1 + v.inverse%*%(y - mu) beta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(beta1) break } else{ k \u0026lt;- k + 1 } } colnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1)) eta \u0026lt;- x%*%beta1 y.fit \u0026lt;- eval(h)# compute the fitting values of y glmpoisson.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1,\u0026quot;拟合值\u0026quot; = y.fit) return(glmpoisson.result) }  5.4 拟似然 ## ##This program is quasi-likelihood method ## myquasimle \u0026lt;- function(x,y,beta1,family,N = 5000,e = 1e-10){# # x is the design matrix # b is the starting value of the iteration # N is the upper bound of the times of the iteration # e is the convergence criteria n \u0026lt;- length(x) x \u0026lt;- as.matrix(x) y \u0026lt;- as.matrix(y) v.mu \u0026lt;- expression(mu^2)# The covariance function of y if(family == \u0026quot;possion\u0026quot;){ h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h } else if(family == \u0026quot;logistic\u0026quot;){ h \u0026lt;- expression(1/(1 + exp(-eta))) dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;) } else{ print(\u0026quot;分布参数错误, 请选择logistic或者possion分布！！！！\u0026quot;) } k \u0026lt;- 1 beta0 \u0026lt;- beta1 + 1 while(sum((beta0 - beta1)^2) \u0026gt;= e){ beta0 \u0026lt;- beta1 # compute the D matrix eta \u0026lt;- x%*%beta1 delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x # compute v.inverse—the inverse matrix of the covariance matrix of y mu \u0026lt;- eval(h) v.inverse \u0026lt;- diag(1/as.vector(eval(v.mu))) p1 \u0026lt;- solve(t(D)%*%v.inverse%*%D) p2 \u0026lt;- t(D)%*%v.inverse%*%delta p3 \u0026lt;- x%*%beta1 + v.inverse%*%(y - mu) beta1 \u0026lt;- p1%*%p2%*%p3# the kth estimates # check if it is divergent if(k \u0026gt; N){ cat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;) cat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;) print(beta1) break } else{ k \u0026lt;- k + 1 } } colnames(beta1) \u0026lt;- c(\u0026quot;估计值\u0026quot;) rownames(beta1) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(beta1)),1:length(beta1)) eta \u0026lt;- x%*%beta1 y.fit \u0026lt;- eval(h)# compute the fitting values of y quasi.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = beta1,\u0026quot;算法迭代次数\u0026quot; = k-1,\u0026quot;拟合值\u0026quot; = y.fit) return(quasi.result) }  5.5 单个系数检验 ## 求解广义线性模型单个系数检验 myglmindivi \u0026lt;- function(x,y,b,expr){# # expr 表示模型：logistic, poisson # b 表示由广义线性模型估计得到的估计量 n \u0026lt;- length(x) p \u0026lt;- length(b) beta.hat \u0026lt;- b D \u0026lt;- matrix(0, nrow = n, ncol = p) # get the D matrix if(expr == \u0026quot;logistic\u0026quot;){ h \u0026lt;- expression(1/(1 + exp(-eta)))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(log(1 + exp(theta)))# the b(theta) function of the pdf db \u0026lt;- D(b,\u0026quot;theta\u0026quot;)# the first derivative funtion of b ddb \u0026lt;- D(db,\u0026quot;theta\u0026quot;)# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu/(1 - mu)))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else if(expr == \u0026quot;poisson\u0026quot;){ h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h db \u0026lt;- expression(exp(theta))# the first derivative funtion of b ddb \u0026lt;- expression(exp(theta))# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic,poisson之一\u0026quot;)) } cc \u0026lt;- solve(t(D)%*%v.inverse%*%D) diagc \u0026lt;- diag(cc) Tt \u0026lt;- 1:p;Tp \u0026lt;- Tt;Ts \u0026lt;- Tt for(i in 1:p){ Tt[i] \u0026lt;- beta.hat[i]/sqrt(diagc[i]) Tp[i] \u0026lt;- 2*(1 - pnorm(Tt[i])) if(Tp[i] \u0026lt; 0.001 ){# 判断置信度 Ts[i] \u0026lt;- c(\u0026quot;***\u0026quot;) } else if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){ Ts[i] \u0026lt;- c(\u0026quot;**\u0026quot;) } else if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){ Ts[i] \u0026lt;- c(\u0026quot;*\u0026quot;) } else if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){ Ts[i] \u0026lt;- c(\u0026quot;.\u0026quot;) } else{ Ts[i] \u0026lt;- c(\u0026quot; \u0026quot;) } } indivitest \u0026lt;- data.frame(\u0026quot;Tu\u0026quot; = Tt,\u0026quot;Tp\u0026quot; = Tp,\u0026quot;Ts\u0026quot; = Ts) return(indivitest) }  5.6 区间估计 ## the confidence interval of the generalized linear model myglminterval \u0026lt;- function(x,y,b,expr,alpha = 0.05){ n \u0026lt;- length(x) p \u0026lt;- length(b) beta.hat \u0026lt;- b D \u0026lt;- matrix(0, nrow = n, ncol = p) # get the D matrix if(expr == \u0026quot;logistic\u0026quot;){ h \u0026lt;- expression(1/(1 + exp(-eta)))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h b \u0026lt;- expression(log(1 + exp(theta)))# the b(theta) function of the pdf db \u0026lt;- D(b,\u0026quot;theta\u0026quot;)# the first derivative funtion of b ddb \u0026lt;- D(db,\u0026quot;theta\u0026quot;)# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu/(1 - mu)))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else if(expr == \u0026quot;poisson\u0026quot;){ h \u0026lt;- expression(exp(eta))# the inverse of the link funtion dh \u0026lt;- D(h,\u0026quot;eta\u0026quot;)# the first derivative funtion of h db \u0026lt;- expression(exp(theta))# the first derivative funtion of b ddb \u0026lt;- expression(exp(theta))# the second derivative funtion of b db.inverse \u0026lt;- expression(log(mu))# the inverse of db eta \u0026lt;- x%*%beta.hat delta \u0026lt;- diag(as.vector(eval(dh))) D \u0026lt;- delta%*%x# the D matrix mu \u0026lt;- eval(h) theta \u0026lt;- eval(db.inverse) v.inverse \u0026lt;- diag(1/as.vector(eval(ddb)))# the inverse of the covariance matrix of y } else{ stop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic, gompertz, weibull之一\u0026quot;)) } Ti \u0026lt;- matrix(0,nrow = p,ncol = 2) cc \u0026lt;- solve(t(D)%*%v.inverse%*%D) diagc \u0026lt;- diag(cc) for(i in 1:p){ Ti[i,1] \u0026lt;- beta.hat[i] - qnorm((1-alpha/2))*sqrt(diagc[1]) Ti[i,2] \u0026lt;- beta.hat[i] + qnorm((1-alpha/2))*sqrt(diagc[1]) } out \u0026lt;- cbind(beta.hat,Ti) colnames(out) \u0026lt;- c(\u0026quot;Estimator\u0026quot;,\u0026quot;LowerBound\u0026quot;,\u0026quot;UpperBound\u0026quot;) return(out) }  ","id":12,"section":"posts","summary":"1 指数族分布与广义线性模型 1.1 引入指数族分布 在线性模型中，一个重要的条件便是响应变量 $y$ 须服从正态分布。然而，实际问题中 $y$ 并不总是满足正态分布的","tags":["多元统计","回归分析"],"title":"广义线性模型及其一般求解方式","uri":"https://jiandan94.github.io/2018/06/glm/","year":"2018"},{"content":"这个笔记总结了非线性模型的极大似然估计量和渐进性质，并推导了用于求解模型的Gauss—Newton迭代法。此外，针对每个内容，我还给出了相应的R软件求解算法，并做了相应的模拟。\n1 非线性模型 线性模型建立在自变量和响应变量之间呈线性关系的基础上，但实际数据并不总是如此。当我们没有额外信息认为两者之间的关系为线性时，非线性模型便成了一种选择。\n考虑非线性模型\n$$ y=f\\left( X,\\beta \\right) +\\epsilon $$\n其中 $y\\in \\mathbb{R}^n$，$X\\in \\mathbb{R}^{n\\times p}$，$n$ 表示观测数，$p$ 表示变量数。$\\beta \\in \\mathbb{R}^n$ 表示回归系数。$\\epsilon$ 一般假设成为独立同分布的 $N\\left( 0,\\sigma ^2I_n \\right) $ 高斯随机变量。\n2 模型的极大似然估计 如果我们假设模型的随机误差项是独立同分于均值为0方差为常数 $\\sigma^2$ 的正态分布，那么可以考虑极大似然估计法来估计模型的解。\n首先，根据模型和假设得到似然函数\n$$ \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =\\frac{1}{\\left( 2\\pi \\sigma ^2 \\right) ^{n/2}}\\exp \\left[ -\\frac{1}{2\\sigma ^2}\\left( y-f\\left( X,\\beta \\right) \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) \\right] $$\n对上式取对数，得到对数似然函数\n$$ \\ln \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =-\\frac{n}{2}\\ln \\left( 2\\pi \\sigma ^2 \\right) -\\frac{1}{2\\sigma ^2}\\left( y-f\\left( X,\\beta \\right) \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) $$\n将对数似然方程对 $\\beta$ 进行求偏导，根据极值必要条件令为零，从而得到得分方程（score equation）\n$$ \\begin{split} \\frac{\\partial \\ln \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right)}{\\partial \\beta}\u0026amp;=\\frac{\\partial}{\\partial \\beta}\\left[ -\\frac{1}{2\\sigma ^2}\\left( y-f\\left( X,\\beta \\right) \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) \\right] \\newline \u0026amp;=-\\frac{1}{2\\sigma ^2}\\frac{\\partial \\left( y-f\\left( X,\\beta \\right) \\right) ^{\\mathrm{T}}}{\\partial \\beta}\\left( y-f\\left( X,\\beta \\right) \\right) \\newline \u0026amp;=-\\frac{1}{\\sigma ^2}\\frac{\\partial f\\left( X,\\beta \\right) ^{\\mathrm{T}}}{\\partial \\beta}\\left( y-f\\left( X,\\beta \\right) \\right) \\newline \u0026amp;=-\\frac{1}{\\sigma ^2}\\left( \\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) \\newline \u0026amp;=0 \\end{split} $$\n上式利用了矩阵微分中的结论\n$$ \\frac{\\mathrm{d}f\\left( Y\\left( X \\right) \\right)}{\\mathrm{d}X}=\\frac{\\mathrm{d}Y^{\\mathrm{T}}}{\\mathrm{d}X}\\cdot \\frac{\\mathrm{d}f\\left( Y \\right)}{\\mathrm{d}Y} $$\n其中 $X\\in \\mathbb{R}^{n\\times 1}$，$Y\\in \\mathbb{R}^{n\\times 1}$，$f\\in \\mathbb{R}$ 的结论。进一步，上式等价于\n$$ \\left( \\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{\\mathrm{T}}\\left( y-f\\left( X,\\beta \\right) \\right) =0 $$\n由于 $f(X, \\beta)$ 表示均值函数，将其记作 $\\mu$，并用 $\\hat{\\mu}$ 表示 $f(X,\\beta)$ 中 $\\beta$ 被其估计量 $b$ 替换的结果。那么根据上式可以推导出模型的估计量满足\n$$ \\left( \\frac{\\partial \\hat{\\mu}}{\\partial \\beta ^{\\mathrm{T}}} \\right) ^{\\mathrm{T}}\\left( y-\\hat{\\mu} \\right) =0 $$\n这时我们记 $\\frac{\\partial \\hat{\\mu}}{\\partial \\beta ^{\\mathrm{T}}}=D\\in \\mathbb{R}^{n\\times p}$，其中 $D_{ij}=\\frac{\\partial f\\left( X_i,\\beta \\right)}{\\partial \\beta _j}$，那么就有\n$$ D^{\\mathrm{T}}\\left( y-\\hat{\\mu} \\right) =0 $$\n 这里可以看出极大似然估计量和最小二乘估计量的形式是一致的。 之所以使用似然法，是因为在线性模型、广义线性模型中似然法更普适。\n 3 迭代求解算法 在非线性回归模型中，求解最小二乘估计量（极大似然估计量）的一个广泛应用的方法是将期望函数线性化，然后利用Gauss—Newton迭代法进行求解。\n考虑非线性模型，将其在 点处进行Taylor展开\n$$ y=f\\left( X,\\beta \\right) +\\epsilon =f\\left( X,b_0 \\right) +\\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}}\\left( \\beta -b_0 \\right) +\\epsilon $$\n令 $y_0=y-f\\left( X,b_0 \\right) =y-f_0$，$\\theta _0=\\beta -b_0$，$D_0=\\left[ \\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}} \\right] _{\\beta =b_0}$，则上式可以写成\n$$ y_0=D_0\\theta _0+\\epsilon $$\n那么 $\\theta_0$ 的最小二乘估计量为\n$$ \\begin{split} \\hat{\\theta}_0 \u0026amp;=\\left( D _{0}^{\\mathrm{T}}D_0 \\right) ^{-1}D _{0}^{\\mathrm{T}}y_0 \\newline \u0026amp;=\\left( D _{0}^{\\mathrm{T}}D_0 \\right) ^{-1}D _{0}^{\\mathrm{T}}\\left( y-f_0 \\right) \\end{split} $$\n因为 $\\theta _0=\\beta -b_0$，我们用\n$$ b_1=b_0+\\hat{\\theta}_0 $$\n作为未知参数 $\\beta$ 的一个修正估计。那么，我们就按照这个逻辑不停得修正 $\\beta$，也就是说有\n$$ \\begin{split} b_{k+1}\u0026amp;=b_k+\\hat{\\theta}_k \\newline \u0026amp;=b_k+\\left( D_{k}^{\\mathrm{T}}D_k \\right) ^{-1}D_{k}^{\\mathrm{T}}\\left( y-f_k \\right) \\end{split} $$\n当这种修正直到收敛（前后两个估计的改变量非常小）时，即\n$$ \\frac{\\lVert b_{k+1}-b_k \\rVert}{\\lVert b_k \\rVert}\u0026lt;\\delta $$\n迭代结束，其中 $\\delta$ 是某个很小的数，比如说 $10^{-6}$。\n4 估计量的渐近正态性 根据前面分析可知 $\\beta$ 估计量为\n$$ \\hat{\\beta}\\doteq \\beta +\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( y-f \\right) $$\n其中 $D=\\frac{\\partial \\mu}{\\partial \\beta ^{\\mathrm{T}}}=\\frac{\\partial f\\left( X,\\beta \\right)}{\\partial \\beta ^{\\mathrm{T}}}$。对上式进行分析，等式右侧的随机项只有 $y$ 一项，这时我们考察有\n$$ \\begin{split} E\\hat{\\beta}\u0026amp;=\\beta +E\\left[ \\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( y-f \\right) \\right] \\newline \u0026amp;=\\beta +\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}E\\left( y-f \\right) \\newline \u0026amp;=\\beta +\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( Ey-f \\right) \\newline \u0026amp;=\\beta \\end{split} $$\n和\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left[ \\beta +\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( y-f \\right) \\right] \\newline \u0026amp;=Var\\left[ \\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}\\left( y-f \\right) \\right] \\newline \u0026amp;=\\left( D^{\\mathrm{T}}D \\right) ^{-1}D^{\\mathrm{T}}Var\\left( y-f \\right) D\\left( D^{\\mathrm{T}}D \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( D^{\\mathrm{T}}D \\right) ^{-1} \\end{split} $$\n进一步，上式求出了得分函数\n$$ S\\left( \\beta \\right) =\\frac{1}{\\sigma ^2}D^{\\mathrm{T}}\\left( y-\\mu \\right) $$\n这是一个 $\\mathbb{R}^{p\\times 1}$ 中的列向量，并且\n$$ ES=\\frac{1}{\\sigma ^2}ED^{\\mathrm{T}}\\left( y-\\mu \\right) =D^{\\mathrm{T}}\\left( Ey-\\mu \\right) =0 $$\n所以\n$$ \\begin{split} Cov\\left( S \\right) \u0026amp;=\\frac{1}{\\sigma ^4}E\\left( S-ES \\right) \\left( S-ES \\right) ^{\\mathrm{T}}=\\frac{1}{\\sigma ^4}ESS^{\\mathrm{T}} \\newline \u0026amp;=\\frac{1}{\\sigma ^4}E\\left[ D^{\\mathrm{T}}\\left( y-\\mu \\right) \\left( y-\\mu \\right) ^{\\mathrm{T}}D \\right] \\newline \u0026amp;=\\frac{1}{\\sigma ^4}D^{\\mathrm{T}}E\\left[ \\left( y-\\mu \\right) \\left( y-\\mu \\right) ^{\\mathrm{T}} \\right] D \\newline \u0026amp;=\\frac{1}{\\sigma ^4}D^{\\mathrm{T}}Var\\left( y \\right) D=\\frac{1}{\\sigma ^2}D^{\\mathrm{T}}D \\end{split} $$\n上式得到的协方差矩阵称之为Fisher信息矩阵，记做 $I(\\beta)$。所以，$\\hat{\\beta}$ 的协方差矩阵也可以写成\n$$ Cov\\left( \\beta \\right) =I^{-1}\\left( \\beta \\right) $$\n重新考察 $\\hat{\\beta}$ 的近似表达式，由于 $\\hat{\\beta}$ 是近似关于 $y$ 的一个线性组合，而 $y$ 是服从正态分布的，那么 $\\hat{\\beta}$ 也近似服从正态分布，根据所求的结果有\n$$ \\hat{\\beta}\\ \\sim N\\left( \\beta ,I^{-1}\\left( \\beta \\right) \\right) $$\n如果 $y$ 不是独立的，协方差矩阵为 $V$，可以推出下面结果\n$$ S\\left( \\beta \\right) =D^{\\mathrm{T}}V^{-1}\\left( y-\\mu \\right) $$\n且\n$$ \\hat{\\beta}\\ \\dot{\\sim} N\\left( \\beta ,I^{-1}\\left( \\beta \\right) \\right) $$\n5 模拟和实例 根据前面的分析可以自己用R编写非线性模型求解函数，以及显著性检验的函数。相关代码附在最后供参考。\n 迭代算法毕竟是局部最优，模拟的初始值最好取在真实值附近。如果是实际问题，可以根据经验、利用OLS估计等最为初始值。\n 5.1 Logistic模型 考虑Logistic增长模型\n$$ y=\\frac{\\beta _1}{1+\\beta _2e^{-\\beta _3x}}+\\varepsilon $$\n在本例中取 $\\beta_1=5$，$\\beta_2=6$，$\\beta_3 = 3$；并且 服从标准正态分布，样本观测数取5000。 $\\epsilon$ 是服从标准正态分布的噪声。在模拟中，使用Gauss—Newton迭代法给定初始迭代值 $b_0=\\left( 4.5,5.2,3.5 \\right) ^{\\mathrm{T}}$，程序迭代5次后收敛，得到系数估计如下图所示\n可见算法的效果还是不错的。\n5.2 Gompertz模型 考虑Gompertz模型\n$$ y=\\beta _1\\exp \\left( -\\beta _2e^{-\\beta _3x} \\right) +\\varepsilon $$\n在本例中取 $\\beta_1=30$，$\\beta_2=14$，$\\beta_3 = 3$；并且 $x$ 服从标准正态分布，样本观测数取5000。 $\\epsilon$ 是服从标准正态分布的噪声。在模拟中，使用Gauss—Newton迭代法给定初始迭代值 $b_0=\\left( 24.5,15.2,3.5 \\right) ^{\\mathrm{T}}$，程序迭代6次后收敛，得到系数估计如下图所示\n可见算法的效果还是不错的。\n5.3 Weibull模型 考虑Weibull模型\n$$ y=\\beta _1-\\beta _2\\exp \\left( -\\beta _3x^{\\beta _4} \\right) +\\varepsilon $$\n在本例中取 $\\beta_1=5$，$\\beta_2=1$，$\\beta_3 = 3$，$\\beta_4=2$；并且 $x$ 服从 $[0,1]$ 的均匀分布，样本观测数取5000。$\\epsilon$ 是服从标准正态分布的噪声。在模拟中，使用Gauss—Newton迭代法给定初始迭代值 ，程序迭代6次后收敛，得到系数估计如下图所示\n可见算法的效果还是不错的。\n5.4 Michaelis-Menten模型 考虑Michaelis-Menten模型\n$$ y=\\frac{\\beta _1x}{\\beta _2+x}+\\varepsilon $$\n对puromycin数据\n   Concentration ($y$) Velocity ($x_1$) Velocity ($x_2$)     0.02 47 76   0.06 97 107   0.11 123 139   0.22 152 159   0.56 191 201   1.10 200 207    考虑上述Michaelis-Menten模型，使用Gauss—Newton迭代法给定初始迭代值 $b_0=\\left( 205.00,0.08 \\right) ^{\\mathrm{T}}$，程序迭代5次后收敛，得到系数估计\n$$ b=\\left( 212.6837,0.0641 \\right) ^{\\mathrm{T}} $$\n利用得到的参数估计计算拟合值，画出图像如下所示\n图中圆点表示真实数据，虚线是得到的Michaelis-Menten模型估计方程。从图中看出，拟合的效果还是不错的。\n5.5 渐近正态性验证 考虑5.1中的Logistic模型，在本次试验中分别取观测数目为50,500,1000和5000来进行模拟，每次模拟都做500次，然后画出每次参数估计中 $\\beta_1$ 的核密度曲线，并与对应的正态密度曲线进行对比，结果如下所示\n从图中可以看出，$\\beta_1$ 的渐近正态性得到了验证。\n6 自编函数代码 6.1 Logistic模型系数求解 mynlmlogistic \u0026lt;- function(x,y,b,N = 5000,e = 1e-10){#\r# x is the design matrix\r# b is the starting value of the iteration\r# N is the upper bound of the times of the iteration\r# e is the convergence criteria\rn \u0026lt;- length(x)\rmylogistic \u0026lt;- expression(a1/(1 + a2*exp(-a3*t)))\rpartial.a1 \u0026lt;- D(mylogistic,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(mylogistic,\u0026quot;a2\u0026quot;)\rpartial.a3 \u0026lt;- D(mylogistic,\u0026quot;a3\u0026quot;)\rk \u0026lt;- 1\rbk \u0026lt;- b\rb \u0026lt;- b + 1# to enter the iteration\rwhile(sum((bk - b) ^ 2) / sum(b ^ 2) \u0026gt;= e) {\r# get the D matrix\rD \u0026lt;- matrix(0, nrow = n, ncol = 3)\ra1 \u0026lt;- bk[1]\ra2 \u0026lt;- bk[2]\ra3 \u0026lt;- bk[3]\rfor (i in 1:n) {\rt \u0026lt;- x[i]\rD[i, 1] \u0026lt;- eval(partial.a1)\rD[i, 2] \u0026lt;- eval(partial.a2)\rD[i, 3] \u0026lt;- eval(partial.a3)\r}\rt \u0026lt;- x\rf \u0026lt;- eval(mylogistic)# compute the value of f\rb \u0026lt;- bk# denote the (k)th valuve by b\rbk \u0026lt;- bk + solve(t(D)%*%D)%*%t(D)%*%(as.matrix(y - f))# denote the (k+1)th value by bk\r# check if it is divergent\rif(k \u0026gt; N){\rcat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rprint(bk)\rbreak\r}\relse{\rk \u0026lt;- k + 1\r}\r}\rcolnames(bk) \u0026lt;- c(\u0026quot;估计值\u0026quot;)\rrownames(bk) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(bk)),1:length(bk))\rlogistic.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = bk,\u0026quot;算法迭代次数\u0026quot; = k-1)\rreturn(logistic.result)\r}\r 6.2 Gompertz模型系数求解 mynlmgompertz \u0026lt;- function(x,y,b,N = 5000,e = 1e-10){#\r# x is the design matrix\r# b is the starting value of the iteration\r# N is the upper bound of the times of the iteration\r# e is the convergence criteria\rn \u0026lt;- length(x)\rmygompertz \u0026lt;- expression(a1*exp(-a2*exp(-a3*t)))\rpartial.a1 \u0026lt;- D(mygompertz,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(mygompertz,\u0026quot;a2\u0026quot;)\rpartial.a3 \u0026lt;- D(mygompertz,\u0026quot;a3\u0026quot;)\rk \u0026lt;- 1\rbk \u0026lt;- b\rb \u0026lt;- b + 1# to enter the iteration\rwhile(sum((bk - b) ^ 2) / sum(b ^ 2) \u0026gt;= e) {\r# get the D matrix\rD \u0026lt;- matrix(0, nrow = n, ncol = 3)\ra1 \u0026lt;- bk[1]\ra2 \u0026lt;- bk[2]\ra3 \u0026lt;- bk[3]\rfor (i in 1:n) {\rt \u0026lt;- x[i]\rD[i, 1] \u0026lt;- eval(partial.a1)\rD[i, 2] \u0026lt;- eval(partial.a2)\rD[i, 3] \u0026lt;- eval(partial.a3)\r}\rt \u0026lt;- x\rf \u0026lt;- eval(mygompertz)# compute the value of f\rb \u0026lt;- bk# denote the (k)th valuve by b\rbk \u0026lt;- bk + solve(t(D)%*%D)%*%t(D)%*%(as.matrix(y - f))# denote the (k+1)th value by bk\r# check if it is divergent\rif(k \u0026gt; N){\rcat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rprint(bk)\rbreak\r}\relse{\rk \u0026lt;- k + 1\r}\r}\rcolnames(bk) \u0026lt;- c(\u0026quot;估计值\u0026quot;)\rrownames(bk) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(bk)),1:length(bk))\rgompertz.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = bk,\u0026quot;算法迭代次数\u0026quot; = k-1)\rreturn(gompertz.result)\r}\r 6.3 Weibull模型系数求解 mynlmweibull \u0026lt;- function(x,y,b,N = 5000,e = 1e-10){#\r# x is the design matrix\r# b is the starting value of the iteration\r# N is the upper bound of the times of the iteration\r# e is the convergence criteria\rn \u0026lt;- length(x)\rmyweibull \u0026lt;- expression(a1 - a2*exp(-a3*(t^a4)))\rpartial.a1 \u0026lt;- D(myweibull,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(myweibull,\u0026quot;a2\u0026quot;)\rpartial.a3 \u0026lt;- D(myweibull,\u0026quot;a3\u0026quot;)\rpartial.a4 \u0026lt;- D(myweibull,\u0026quot;a4\u0026quot;)\rk \u0026lt;- 1\rbk \u0026lt;- b\rb \u0026lt;- b + 1# to enter the iteration\rwhile(sum((bk - b) ^ 2) / sum(b ^ 2) \u0026gt;= e) {\r# get the D matrix\rD \u0026lt;- matrix(0, nrow = n, ncol = 4)\ra1 \u0026lt;- bk[1]\ra2 \u0026lt;- bk[2]\ra3 \u0026lt;- bk[3]\ra4 \u0026lt;- bk[4]\rfor (i in 1:n) {\rt \u0026lt;- x[i]\rD[i, 1] \u0026lt;- eval(partial.a1)\rD[i, 2] \u0026lt;- eval(partial.a2)\rD[i, 3] \u0026lt;- eval(partial.a3)\rD[i, 4] \u0026lt;- eval(partial.a4)\r}\rt \u0026lt;- x\rf \u0026lt;- eval(myweibull)# compute the value of f\rb \u0026lt;- bk# denote the (k)th valuve by b\rbk \u0026lt;- bk + solve(t(D)%*%D)%*%t(D)%*%(as.matrix(y - f))# denote the (k+1)th value by bk\r# check if it is divergent\rif(k \u0026gt; N){\rcat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rprint(bk)\rbreak\r}\relse{\rk \u0026lt;- k + 1\r}\r}\rcolnames(bk) \u0026lt;- c(\u0026quot;估计值\u0026quot;)\rrownames(bk) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(bk)),1:length(bk))\rweibull.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = bk,\u0026quot;算法迭代次数\u0026quot; = k-1)\rreturn(weibull.result)\r}\r 6.4 Michaelis-Menten模型系数求解 mynlmmicmen \u0026lt;- function(x,y,b,N = 5000,e = 1e-10){#\r# x is the design matrix\r# b is the starting value of the iteration\r# N is the upper bound of the times of the iteration\r# e is the convergence criteria\rn \u0026lt;- length(x)\rmymicmen \u0026lt;- expression(a1*t/(a2 + t))\rpartial.a1 \u0026lt;- D(mymicmen,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(mymicmen,\u0026quot;a2\u0026quot;)\rk \u0026lt;- 1\rbk \u0026lt;- b\rb \u0026lt;- b + 1# to enter the iteration\rwhile(sum((bk - b) ^ 2) / sum(b ^ 2) \u0026gt;= e) {\r# get the D matrix\rD \u0026lt;- matrix(0, nrow = n, ncol = 2)\ra1 \u0026lt;- bk[1]\ra2 \u0026lt;- bk[2]\rfor (i in 1:n) {\rt \u0026lt;- x[i]\rD[i, 1] \u0026lt;- eval(partial.a1)\rD[i, 2] \u0026lt;- eval(partial.a2)\r}\rt \u0026lt;- x\rf \u0026lt;- eval(mymicmen)# compute the value of f\rb \u0026lt;- bk# denote the (k)th valuve by b\rbk \u0026lt;- bk + solve(t(D)%*%D)%*%t(D)%*%(as.matrix(y - f))# denote the (k+1)th value by bk\r# check if it is divergent\rif(k \u0026gt; N){\rcat(\u0026quot;算法不收敛，已达到最大迭代次数：\u0026quot;,N,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;此时得到的解为：\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rprint(bk)\rbreak\r}\relse{\rk \u0026lt;- k + 1\r}\r}\rcolnames(bk) \u0026lt;- c(\u0026quot;估计值\u0026quot;)\rrownames(bk) \u0026lt;- paste0(rep(\u0026quot;系数\u0026quot;,length(bk)),1:length(bk))\rmymicmen.result \u0026lt;- list(\u0026quot;模型的解\u0026quot; = bk,\u0026quot;算法迭代次数\u0026quot; = k-1)\rreturn(mymicmen.result)\r}\r 6.5 单个系数检验 ## 求解非线性模型单个系数检验\rmynlmindivi \u0026lt;- function(x,y,b,expr){#\r# expr 表示模型：logistic, gompertz, weibull\rn \u0026lt;- length(x)\rp \u0026lt;- length(b)\rt \u0026lt;- x\rD \u0026lt;- matrix(0, nrow = n, ncol = p)\r# get the D matrix\rif(expr == \u0026quot;logistic\u0026quot;){\rmylogistic \u0026lt;- expression(a1/(1 + a2*exp(-a3*t)))\rpartial.a1 \u0026lt;- D(mylogistic,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(mylogistic,\u0026quot;a2\u0026quot;)\rpartial.a3 \u0026lt;- D(mylogistic,\u0026quot;a3\u0026quot;)\ra1 \u0026lt;- b[1]\ra2 \u0026lt;- b[2]\ra3 \u0026lt;- b[3]\rD[, 1] \u0026lt;- eval(partial.a1)\rD[, 2] \u0026lt;- eval(partial.a2)\rD[, 3] \u0026lt;- eval(partial.a3)\r}\relse if(expr == \u0026quot;gompertz\u0026quot;){\rmygompertz \u0026lt;- expression(a1*exp(-a2*exp(-a3*t)))\rpartial.a1 \u0026lt;- D(mygompertz,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(mygompertz,\u0026quot;a2\u0026quot;)\rpartial.a3 \u0026lt;- D(mygompertz,\u0026quot;a3\u0026quot;)\ra1 \u0026lt;- b[1]\ra2 \u0026lt;- b[2]\ra3 \u0026lt;- b[3]\rD[, 1] \u0026lt;- eval(partial.a1)\rD[, 2] \u0026lt;- eval(partial.a2)\rD[, 3] \u0026lt;- eval(partial.a3)\r}\relse if(expr == \u0026quot;weibull\u0026quot;){\rmyweibull \u0026lt;- expression(a1 - a2*exp(-a3*(t^a4)))\rpartial.a1 \u0026lt;- D(myweibull,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(myweibull,\u0026quot;a2\u0026quot;)\rpartial.a3 \u0026lt;- D(myweibull,\u0026quot;a3\u0026quot;)\rpartial.a4 \u0026lt;- D(myweibull,\u0026quot;a4\u0026quot;)\ra1 \u0026lt;- b[1]\ra2 \u0026lt;- b[2]\ra3 \u0026lt;- b[3]\ra4 \u0026lt;- b[4]\rD[, 1] \u0026lt;- eval(partial.a1)\rD[, 2] \u0026lt;- eval(partial.a2)\rD[, 3] \u0026lt;- eval(partial.a3)\rD[, 4] \u0026lt;- eval(partial.a4)\r}\relse{\rstop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic, gompertz, weibull之一\u0026quot;))\r}\rcc \u0026lt;- solve(t(D)%*%D)\rdiagc \u0026lt;- diag(cc)\rTt \u0026lt;- 1:p;Tp \u0026lt;- Tt;Ts \u0026lt;- Tt\rfor(i in 1:p){\rTt[i] \u0026lt;- b[i]/sqrt(diagc[i])\rTp[i] \u0026lt;- 2*(1 - pnorm(Tt[i]))\rif(Tp[i] \u0026lt; 0.001 ){# 判断置信度\rTs[i] \u0026lt;- c(\u0026quot;***\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){\rTs[i] \u0026lt;- c(\u0026quot;**\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){\rTs[i] \u0026lt;- c(\u0026quot;*\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){\rTs[i] \u0026lt;- c(\u0026quot;.\u0026quot;)\r}\relse{\rTs[i] \u0026lt;- c(\u0026quot; \u0026quot;)\r}\r}\rindivitest \u0026lt;- data.frame(\u0026quot;Tu\u0026quot; = Tt,\u0026quot;Tp\u0026quot; = Tp,\u0026quot;Ts\u0026quot; = Ts)\rreturn(indivitest)\r}\r 6.6 区间估计 ##求解非线性模型区间估计\rmynlminterval \u0026lt;- function(x,y,b,expr,alpha = 0.05){\rn \u0026lt;- length(x)\rp \u0026lt;- length(b)\rt \u0026lt;- x\rD \u0026lt;- matrix(0, nrow = n, ncol = p)\r# get the D matrix\rif(expr == \u0026quot;logistic\u0026quot;){\rmylogistic \u0026lt;- expression(a1/(1 + a2*exp(-a3*t)))\rpartial.a1 \u0026lt;- D(mylogistic,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(mylogistic,\u0026quot;a2\u0026quot;)\rpartial.a3 \u0026lt;- D(mylogistic,\u0026quot;a3\u0026quot;)\ra1 \u0026lt;- b[1]\ra2 \u0026lt;- b[2]\ra3 \u0026lt;- b[3]\rD[, 1] \u0026lt;- eval(partial.a1)\rD[, 2] \u0026lt;- eval(partial.a2)\rD[, 3] \u0026lt;- eval(partial.a3)\r}\relse if(expr == \u0026quot;gompertz\u0026quot;){\rmygompertz \u0026lt;- expression(a1*exp(-a2*exp(-a3*t)))\rpartial.a1 \u0026lt;- D(mygompertz,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(mygompertz,\u0026quot;a2\u0026quot;)\rpartial.a3 \u0026lt;- D(mygompertz,\u0026quot;a3\u0026quot;)\ra1 \u0026lt;- b[1]\ra2 \u0026lt;- b[2]\ra3 \u0026lt;- b[3]\rD[, 1] \u0026lt;- eval(partial.a1)\rD[, 2] \u0026lt;- eval(partial.a2)\rD[, 3] \u0026lt;- eval(partial.a3)\r}\relse if(expr == \u0026quot;weibull\u0026quot;){\rmyweibull \u0026lt;- expression(a1 - a2*exp(-a3*(t^a4)))\rpartial.a1 \u0026lt;- D(myweibull,\u0026quot;a1\u0026quot;)\rpartial.a2 \u0026lt;- D(myweibull,\u0026quot;a2\u0026quot;)\rpartial.a3 \u0026lt;- D(myweibull,\u0026quot;a3\u0026quot;)\rpartial.a4 \u0026lt;- D(myweibull,\u0026quot;a4\u0026quot;)\ra1 \u0026lt;- b[1]\ra2 \u0026lt;- b[2]\ra3 \u0026lt;- b[3]\ra4 \u0026lt;- b[4]\rD[, 1] \u0026lt;- eval(partial.a1)\rD[, 2] \u0026lt;- eval(partial.a2)\rD[, 3] \u0026lt;- eval(partial.a3)\rD[, 4] \u0026lt;- eval(partial.a4)\r}\relse{\rstop(cat(\u0026quot;!!!!!!模型参数错误，请选择给定模型logistic, gompertz, weibull之一\u0026quot;))\r}\rTi \u0026lt;- matrix(0,nrow = p,ncol = 2)\rcc \u0026lt;- solve(t(D)%*%D)\rdiagc \u0026lt;- diag(cc)\rfor(i in 1:p){\rTi[i,1] \u0026lt;- b[i] - qnorm((1-alpha/2))*sqrt(diagc[1])\rTi[i,2] \u0026lt;- b[i] + qnorm((1-alpha/2))*sqrt(diagc[1]) }\rout \u0026lt;- cbind(b,Ti)\rcolnames(out) \u0026lt;- c(\u0026quot;Estimator\u0026quot;,\u0026quot;LowerBound\u0026quot;,\u0026quot;UpperBound\u0026quot;)\rreturn(out)\r}\r ","id":13,"section":"posts","summary":"这个笔记总结了非线性模型的极大似然估计量和渐进性质，并推导了用于求解模型的Gauss—Newton迭代法。此外，针对每个内容，我还给出了相应","tags":["多元统计","回归分析"],"title":"非线性模型的参数估计和统计性质","uri":"https://jiandan94.github.io/2018/05/nlm/","year":"2018"},{"content":"本文总结了线性模型的主要知识点，分别为参数估计，包括最小二乘估计和极大似然估计，区间估计，假设检验。此外，针对每个内容，本文还给出了相应的R软件求解算法，并做了相应的模拟。\n设在线性模型中：$y\\in \\mathbb{R}^n$，$X\\in \\mathbb{R}^{n\\times p}$，$n$ 表示观测数，$p$ 表示变量数。$\\beta \\in \\mathbb{R}^n$ 表示回归系数。$\\epsilon \\sim N\\left( 0,\\sigma ^2I_n \\right) $，是为独立同分布的高斯随机变量。\n1 估计量的表达式 1.1 最小二乘估计量  有截距项的参数估计：  首先令带有截距项的线性模型表达式为：\n$$ y=1\\beta _0+X\\beta+\\epsilon $$\n根据最小二乘思想可知目标函数为 $f\\left( \\beta _0,\\beta \\right) =\\lVert y-1\\beta _0-X\\beta \\rVert _{2}^{2} $，所以将 $f\\left( \\beta _0,\\beta \\right) $ 展开有：\n$$ \\begin{split} f\\left( \\beta _0,\\beta \\right) =\u0026amp;y^{\\mathrm{T}}y+\\beta _01^{\\mathrm{T}}1\\beta _0+2\\beta ^{\\mathrm{T}}X^{\\mathrm{T}}1\\beta _0 \\newline \u0026amp;+\\beta ^{\\mathrm{T}}X^{\\mathrm{T}}X\\beta -2\\beta _01^{\\mathrm{T}}y-2y^{\\mathrm{T}}X\\beta \\end{split} $$\n然后分别对 $\\beta_0$，$\\beta$ 求偏导并令为零：\n$$ \\begin{split} \\frac{\\partial f}{\\beta _0}\u0026amp;=2\\cdot 1^{\\mathrm{T}}1\\beta _0+2\\beta ^{\\mathrm{T}}X ^{\\mathrm{T}}1-2y ^{\\mathrm{T}}1=0 \\newline \\frac{\\partial f}{\\beta }\u0026amp;=2X^{\\mathrm{T}}1\\beta _0+2X^{\\mathrm{T}}X\\beta -2X ^{\\mathrm{T}}y=0 \\end{split} $$\n由上式第1式求得：$\\hat{\\beta}_0=\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta =\\bar{y}-\\bar{x}^{\\mathrm{T}}\\hat{\\beta} $，其中 $\\bar{X}=\\left( \\bar{x}_1,\\cdots ,\\bar{x}_p \\right) ^{\\mathrm{T}}$ 表示由 $X$ 的每一列的均值组成的列向量。然后将求得的结果带入到第2式中，有：\n$$ X^{\\mathrm{T}}1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-X^{\\mathrm{T}}1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta +X^{\\mathrm{T}}X\\beta -X^{\\mathrm{T}}y=0 $$\n整理可得：\n$$ -X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] y+X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] X\\beta =0 $$\n注意到矩阵 $I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}$ 是对称幂等的，并且 $\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X$ 可以看成是 $X$ 的每个元素减去所在列的均值得到的新矩阵（就是每一列元素进行中心化）。则记\n$$ X_c=\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] X $$\n所以可以得到\n$$ X_{c}^{\\mathrm{T}}X_c\\beta =X_{c}^{\\mathrm{T}}y $$\n故而得到有截距的最小二乘参数估计为：\n$$ \\begin{split} \\hat{\\beta}_0\u0026amp;=\\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\newline \\hat{\\beta}\u0026amp;=\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n对所得到的估计进行分析，显然有：\n$$ \\begin{split} E\\hat{\\beta}\u0026amp;=E\\left[ \\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}y \\right] \\newline \u0026amp;=E\\left[ \\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}\\left( 1\\beta _0+X\\beta +\\epsilon \\right) \\right] \\newline \u0026amp;=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{}^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) \\left( 1\\beta _0+X\\beta \\right) \\newline \u0026amp;=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{}^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X\\beta \\newline \u0026amp;=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}X_c\\beta \\newline \u0026amp;=\\beta \\end{split} $$\n这里利用了 $\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) $ 对称幂等的性质。同理有\n$$ \\begin{split} E\\hat{\\beta}_0\u0026amp;=E\\left( \\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\right) \\newline \u0026amp;=\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}Ey-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta \\newline \u0026amp;=\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}\\left( 1\\beta _0+X\\beta \\right) -\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\beta \\newline \u0026amp;=\\beta _0 \\end{split} $$\n因此 $\\beta_0$，$\\beta$ 都是无偏估计量。 下面考虑他们的方差，则有\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left( \\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}y \\right) \\newline \u0026amp;=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}Var\\left( y \\right) X_{c}^{}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\end{split} $$\n和\n$$ \\begin{split} Var\\left( \\hat{\\beta}_0 \\right) \u0026amp;=Var\\left( \\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\right) \\newline \u0026amp;=Var\\left[ \\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right] \\newline \u0026amp;=\\frac{\\sigma ^2}{n^2}\\left[ 1^{\\mathrm{T}}1-0-0+1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X^{\\mathrm{T}}1 \\right] \\newline \u0026amp;=\\sigma ^2\\left( \\frac{1}{n}+\\bar{X}^{\\mathrm{T}}\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\bar{X} \\right) \\end{split} $$\n上式只需注意到 $1^{\\mathrm{T}}X_{c}=1^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X=0$ 即可。最后，计算 $\\beta_0$ 和 $\\beta$ 之间的协方差：\n$$ \\begin{split} Cov\\left( \\hat{\\beta}_0,\\hat{\\beta} \\right) \u0026amp;=Cov\\left( \\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta},\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right) \\newline \u0026amp;=\\sigma ^2\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}\\left( I-X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\right) X _{c}\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}\\left( X _{c}\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1}-X\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1} \\right) \\newline \u0026amp;=-\\sigma ^2\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1} \\newline \u0026amp;=-\\sigma ^2\\bar{X}^{\\mathrm{T}}\\left( X _{c}^{\\mathrm{T}}X _c \\right) ^{-1} \\end{split} $$\n 不带截距项的参数估计：  假设 $\\hat{\\beta}$ 是参数 $\\beta$ 的最小二乘估计量，$\\hat{y}$ 是相应的 $y$ 的最小二乘估计，则根据最小二乘估计的思想，最小化以下目标函数即可：\n$$ \\hat{\\beta}=\\mathrm{arg}\\ \\mathop {\\min} \\limits_\\beta\\lVert y-X\\beta \\rVert _{2}^{2} $$\n令 $f\\left( \\beta \\right) =\\lVert y-X\\beta \\rVert _{2}^{2}$，对 $\\beta$ 进行求导有：\n$$ \\begin{split} \\frac{\\partial f\\left( \\beta \\right)}{\\partial \\beta}\u0026amp;=-2X^{\\mathrm{T}}y+2X^{\\mathrm{T}}X\\beta \\newline \\frac{\\partial ^2f\\left( \\beta \\right)}{\\partial \\beta ^2}\u0026amp;=2X^{\\mathrm{T}}X \\end{split} $$\n根据极值的必要条件，令上式为零有：\n$$ \\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y $$\n显然在求解中并不需要 假定 $\\epsilon$ 一定服从正态分布。\n对所得到的估计进行分析，显然有：\n$$ \\begin{split} E\\hat{\\beta}\u0026amp;=E\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\newline \u0026amp;=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}X\\beta \\newline \u0026amp;=\\beta \\end{split} $$\n因此 $\\hat{\\beta}$ 是无偏估计量。 下面考虑他的方差：\n$$ \\begin{split} Var\\left( \\hat{\\beta} \\right) \u0026amp;=Var\\left( \\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\right) \\newline \u0026amp;=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}Var\\left( y \\right) X\\left( X^{\\mathrm{T}}X \\right) ^{-1} \\newline \u0026amp;=\\sigma ^2\\left( X^{\\mathrm{T}}X \\right) ^{-1} \\end{split} $$\n1.2 极大似然估计量 首先给出多元正态联合密度函数公式：\n$$ f\\left( x|\\mu ,\\Sigma \\right) =\\left[ \\left( 2\\pi \\right) ^{\\mathrm{T}}|\\Sigma |^{-\\frac{1}{2}} \\right] \\exp \\left[ -\\frac{1}{2}\\left( x-\\mu \\right) ^{\\mathrm{T}}\\Sigma ^{-\\frac{1}{2}}\\left( x-\\mu \\right) \\right] $$\n其中 $x$ 服从 $N(\\mu, \\Sigma)$ 分布。在本问题中，我们假定 $\\Sigma =\\sigma ^2I$，也就是随机变量是独立同分布于高斯分布的。\n因此，根据极大似然原理得到似然函数：\n$$ \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =\\left( 2\\pi \\sigma ^2 \\right) ^{-\\frac{n}{2}}\\exp \\left[ -\\frac{1}{2\\sigma ^2}\\left( y-X\\beta \\right) ^{\\mathrm{T}}\\left( y-X\\beta \\right) \\right] $$\n取对数得到对数似然函数：\n$$ \\ln \\mathscr{L}\\left( \\beta ,\\sigma ^2 \\right) =-\\frac{n}{2}\\ln \\left( 2\\pi \\sigma ^2 \\right) -\\frac{1}{2\\sigma ^2}\\left( y-X\\beta \\right) ^{\\mathrm{T}}\\left( y-X\\beta \\right) $$\n上式分别对 $\\beta$ 和 $\\sigma^2$ 求偏导：\n$$ \\begin{split} \\frac{\\partial \\ln \\mathscr{L}}{\\partial \\beta}\u0026amp;=-\\frac{1}{2\\sigma ^2}\\left( -2X^{\\mathrm{T}}y+X^{\\mathrm{T}}X\\beta \\right) =0 \\newline \\frac{\\partial \\ln \\mathscr{L}}{\\partial \\sigma ^2}\u0026amp;=-\\frac{n}{2\\sigma ^2}+\\frac{1}{2\\sigma ^4}\\left( y-X\\beta \\right) ^{\\mathrm{T}}\\left( y-X\\beta \\right) \\end{split} $$\n显然根据第1式得到 $\\beta$ 的极大似然估计量为：\n$$ \\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y $$\n并且我们还可以得到 $\\sigma^2$ 的估计量：\n$$ \\hat{\\sigma}^2=\\frac{RSS}{n} $$\n其中 $RSS=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) $ 是残差平方和。\n 从这里可以看出，在正态分布的假设下，最大似然法和最小二乘法的得到的估计量是一致的。 因此，线性模型下使用最小二乘法或者极大似然法估计系数都可以。然而，这种等价性在其他分布类型的回归问题中不成立。\n 2 方差分析 2.1 带截距项的方差分解 首先给出模型总的离差平方和（$SYY$），回归平方和（$SS_{Reg}$）和残差平方和（$RSS$）的定义：\n$$ \\begin{split} SYY\u0026amp;=\\left( y-\\bar{y}1 \\right) ^{\\mathrm{T}}\\left( y-1\\bar{y} \\right) \\newline SS_{Reg}\u0026amp;=\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right) \\newline RSS\u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\end{split} $$\n则对 $SYY$ 有：\n$$ \\begin{split} SYY\u0026amp;=\\left( y-\\hat{y}+\\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y}+\\hat{y}-1\\bar{y} \\right) \\newline \u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) +\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right)+2\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right) \\newline \u0026amp;=SS_{\\mathrm{Re}g}+RSS+2\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\end{split} $$\n考虑带有截距项的模型的参数估计：\n$$ \\begin{split} \\hat{\\beta}_0\u0026amp;=\\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\newline \\hat{\\beta}\u0026amp;=\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n由此可以得到 $\\hat{y}$ 为：\n$$ \\begin{split} \\hat{y}\u0026amp;=1\\hat{\\beta}_0+X\\hat{\\beta} \\newline \u0026amp;=1\\left[ \\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y-\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\right]+X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \u0026amp;=1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y+\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \u0026amp;=1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}y+X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n所以：\n$$ \\begin{split} y-\\hat{y}\u0026amp;=\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] y-X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \\hat{y}-1\\bar{y}\u0026amp;=X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n所以：\n$$ \\begin{split} \\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \u0026amp;=y^{\\mathrm{T}}X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] y \\newline \u0026amp;\\quad-y^{\\mathrm{T}}X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n而 $\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] $ 对称幂等，则有：\n$$ \\begin{split} X _{c}^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] \u0026amp;=X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] ^2 \\newline \u0026amp;=X^{\\mathrm{T}}\\left[ I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right] \\newline \u0026amp;=X _{c}^{\\mathrm{T}} \\end{split} $$\n所以得到 $\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right)$。这样我们就得到一个重要的分解恒等式：\n$$ SYY=RSS+SS_{\\mathrm{Reg}} $$\n注意，在按照前面各个方差的定义下，只有带截距项的模型才使得上式成立。 通常的模型都是带有截距的，因此在实际运用中，按照前面定义的不同离差是比较常见的。\n 在本节不同离差的定义下，不带截距项的模型分解恒等式不一定成立。\n 2.2 通用型方差分解式 除了以上形式的方差类型的定义方式外，还有一种模型方差分解的定义：\n$$ \\begin{split} SYY\u0026amp;=y^{\\mathrm{T}}y \\newline SS_{Reg}\u0026amp;=\\hat{y}^{\\mathrm{T}}\\hat{y} \\newline RSS\u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\end{split} $$\n将上式中的 $RSS$ 乘开得到：\n$$ RSS=y^{\\mathrm{T}}y+\\hat{y}^{\\mathrm{T}}\\hat{y}-2y^{\\mathrm{T}}\\hat{y} $$\n考虑不带截距项的模型有 $\\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y$，所以 $\\hat{y}=X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y$，故而有：\n$$ \\begin{split} \\hat{y}^{\\mathrm{T}}\\hat{y}\u0026amp;=y^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\newline \u0026amp;=y^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\newline y^{\\mathrm{T}}\\hat{y}\u0026amp;=y^{\\mathrm{T}}X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y=\\hat{y}^{\\mathrm{T}}\\hat{y} \\end{split} $$\n所以将上式结果带入到 $RSS$ 中得到：\n$$ \\begin{split} RSS\u0026amp;=y^{\\mathrm{T}}y+\\hat{y}^{\\mathrm{T}}\\hat{y}-2y^{\\mathrm{T}}\\hat{y}=y^{\\mathrm{T}}y+\\hat{y}^{\\mathrm{T}}\\hat{y}-2\\hat{y}^{\\mathrm{T}}\\hat{y} \\newline \u0026amp;=y^{\\mathrm{T}}y-\\hat{y}^{\\mathrm{T}}\\hat{y} \\newline \u0026amp;=SYY-SS_{\\mathrm{Re}g} \\end{split} $$\n这样就证明了方差分解公式成立。\n 在本节方差的定义下，带截距项模型的分解恒等式也成立。\n 通过上述分析可以看出，如果是带有截距项的模型，一般按照第一种定义进行分解；如果是不带截距项的模型，则按照式第二种定义进行分解。\n2.3 方差分析表  带截距项的模型  对带截距项的模型有：\n$$ \\begin{split} \\hat{\\beta}_0\u0026amp;=\\bar{y}-\\bar{X}^{\\mathrm{T}}\\hat{\\beta} \\newline \\hat{\\beta}\u0026amp;=\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\end{split} $$\n根据2.1节各个离差的定义，假定设计矩阵 $X$ 列满秩且 $\\epsilon \\sim N\\left( 0,\\sigma ^2I_n \\right)$。\n虑总离差平方和 $SYY$：\n$$ \\begin{split} SYY\u0026amp;=\\left( y-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( y-1\\bar{y} \\right) \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) ^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) y \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) y \\end{split} $$\n注意到 $P_1=1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}}$ 对称、幂等且 $I-P_1$ 秩为 $n-1$。对 $y=1\\beta _0+X\\beta+\\epsilon $，容易得到 $y\\sim N\\left( \\hat{\\mu},\\sigma ^2I_n \\right) $，其中 $\\hat{\\mu}=1\\beta _0+X\\beta$，故而：\n$$ \\frac{SYY}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}\\left( I-P_1 \\right) \\left( y/\\sigma \\right) \\sim \\chi _{n-1}^{2}\\left( \\frac{\\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1 \\right) \\hat{\\mu}}{\\sigma ^2} \\right) $$\n考虑残差平方和 $RSS$：\n$$ \\begin{split} RSS \u0026amp;=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \\newline \u0026amp;=\\left( y-1\\hat{\\beta}_0-X\\hat{\\beta} \\right) ^{\\mathrm{T}}\\left( y-1\\hat{\\beta}_0-X\\hat{\\beta} \\right) \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-P_1-P _{X_c}+P_1P _{X_c}+P _{X_c}P_1 \\right) y \\end{split} $$\n其中 $P_{X_c}$ 秩为 $p$ 且满足：\n$$ \\begin{split} P_{X_c}\u0026amp;=X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\newline \u0026amp;=\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\newline \u0026amp;=\\left( I-P_1 \\right) X\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}} \\end{split} $$\n所以有 $\\left( I-P_1 \\right) P_{X_c}=P_{X_c}$，同理可以验证 $P_{X_c}\\left( I-P_1 \\right) =P_{X_c}$ 也成立。则对 $\\left( I-P_1-P_{X_c}+P_1P_{X_c}+P_{X_c}P_1 \\right) $ 有：\n$$ \\begin{split} \u0026amp;I-P_1-P_{X_c}+P_1P_{X_c}+P_{X_c}P_1 \\newline \u0026amp;=I-\\left( I-P_1 \\right) P_{X_c}-P_1+P _{X_c}P_1 \\newline \u0026amp;=I-P_{X_c}-P_1+P_{X_c}P_1 \\newline \u0026amp;=I-P_1-P_{X_c}\\left( I-P_1 \\right) \\newline \u0026amp;=I-P_1-P_{X_c} \\end{split} $$\n可以验证 $I-P_1-P_{X_c}$ 是对称、幂等的且秩为 $n-p-1$，因此\n$$ \\frac{RSS}{\\sigma ^2}=\\left( \\frac{y}{\\sigma} \\right) ^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\left( \\frac{y}{\\sigma} \\right) \\sim \\chi _{n-p-1}^{2} $$\n这里的卡方分布中心没有偏移是因为\n$$ \\begin{split} \\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \u0026amp;=\\left( 1\\beta _0+X\\beta \\right) ^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\newline \u0026amp;=\\left( \\beta _01^{\\mathrm{T}}-\\beta _01^{\\mathrm{T}}P_1 \\right) -\\beta _01^{\\mathrm{T}}P_{X_c}+\\beta^{\\mathrm{T}}X^{\\mathrm{T}}-\\beta^{\\mathrm{T}}X^{\\mathrm{T}}P_1-\\beta^{\\mathrm{T}}X^{\\mathrm{T}}P_{X_c} \\newline \u0026amp;=0-0+\\beta^{\\mathrm{T}}X^{\\mathrm{T}}\\left( I-P_1 \\right) -\\beta^{\\mathrm{T}}X^{\\mathrm{T}}P_{X_c} \\newline \u0026amp;=0 \\end{split} $$\n考虑回归平方和 $SS_{Reg}$：\n$$ \\begin{split} SS_{Reg}\u0026amp;=\\left( \\hat{y}-1\\bar{y} \\right) ^{\\mathrm{T}}\\left( \\hat{y}-1\\bar{y} \\right) \\newline \u0026amp;=y^{\\mathrm{T}}X_c\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}X _{c}^{\\mathrm{T}}y \\newline \u0026amp;=y^{\\mathrm{T}}P_{X_c}y \\end{split} $$\n结合 $y\\sim N\\left( \\hat{\\mu},\\sigma ^2I_n \\right)$，$\\hat{\\mu}=1\\beta _0+X\\beta+\\mu$，故而\n$$ \\frac{SS_{Reg}}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}P_{X_c}\\left( y/\\sigma \\right) \\sim \\chi _{p}^{2}\\left( \\frac{\\tilde{\\mu}^{\\mathrm{T}}P _{X_c}\\tilde{\\mu}}{\\sigma ^2} \\right) $$\n 观察到 $\\left( I-P_1-P_{X_c} \\right) \\times P_{X_c}=0$，故而 $RSS$ 和 $SS_{Reg}$ 是独立同分布的。\n 综合上述可得到带截距项的模型方差分析表\n   方差来源 平方和 自由度 $df$ 均方     回归 $SS_{Reg}$ $p$ $SS_{Reg}/p$   残差 $RSS$ $n-p-1$ $RSS/(n-p-1)$   总离差 $SYY$ $n-1$      不带截距的模型  对不带截距项的模型有：\n$$ \\hat{\\beta}=\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y $$\n根据式2.2各个离差的定义，假定设计矩阵 $X$ 列满秩且 $\\epsilon \\sim N\\left( 0,\\sigma ^2I_n \\right) $。\n考虑总离差平方和 $SYY$：\n其中 $y=X\\beta +\\epsilon \\sim \\left( \\tilde{\\mu},\\sigma ^2I_n \\right) $，$\\tilde{\\mu}=X\\beta $，故而\n$$ \\frac{SYY}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}\\left( y/\\sigma \\right) \\sim \\chi _{n}^{2}\\left( \\frac{\\tilde{\\mu}^{\\mathrm{T}}\\tilde{\\mu}}{\\sigma ^2} \\right) $$\n考虑残差平方和 $RSS$\n$$ \\begin{split} RSS=\\left( y-\\hat{y} \\right) ^{\\mathrm{T}}\\left( y-\\hat{y} \\right) \u0026amp;=\\left( y-X\\hat{\\beta} \\right) ^{\\mathrm{T}}\\left( y-X\\hat{\\beta} \\right) \\newline \u0026amp;=\\left( y-X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\right) ^{\\mathrm{T}}\\left( y-X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}y \\right) \\newline \u0026amp;=y^{\\mathrm{T}}\\left( I-P_X \\right) y \\end{split} $$\n其中 $P_X=X\\left( X^{\\mathrm{T}}X \\right) ^{-1}X^{\\mathrm{T}}$ 是空间 $\\mathscr{R}\\left( X \\right) $ 的正交投影算子。根据矩阵知识， $I-P_X$ 对称、幂等且秩为 $n-p$，所以\n$$ \\frac{RSS}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}\\left( I-P_X \\right) \\left( y/\\sigma \\right) \\sim \\chi _{n-p}^{2} $$\n这里的卡方分布也是没有中心偏移的。\n考虑回归平方和 $SS_{Reg}$\n$$ SS_{Reg}=\\hat{y}^{\\mathrm{T}}\\hat{y}=y^{\\mathrm{T}}P_Xy $$\n结合 $y=X\\beta +\\epsilon \\sim N\\left( \\tilde{\\mu},\\sigma ^2I_n \\right)$，$\\tilde{\\mu}=X\\beta +\\mu $，故而\n$$ \\frac{SS_{Reg}}{\\sigma ^2}=\\left( y/\\sigma \\right) ^{\\mathrm{T}}P_X\\left( y/\\sigma \\right) \\sim \\chi _{p}^{2}\\left( \\frac{\\tilde{\\mu}^{\\mathrm{T}}P_X\\tilde{\\mu}}{\\sigma ^2} \\right) $$\n 通过上述分析，观察到 $\\left( I-P_X \\right) \\times P_X=0$，故而 $RSS$ 和 $SS_{Reg}$ 是独立同分布的。\n 最后，综合上述可得到不带截距项的模型方差分析表\n   方差来源 平方和 自由度 $df$ 均方     回归 $SS_{Reg}$ $p$ $SS_{Reg}/p$   残差 $RSS$ $n-p$ $RSS/(n-p)$   总离差 $SYY$ $n$     3 统计显著性检验 3.1 模型显著性  带截距项的模型  在这个检验前提下，我们的原假设和备择假设通常是\n$$ \\begin{split} \u0026amp;H_0:\\beta=0 \\newline \u0026amp;H_1:\\beta\\ne 0 \\end{split} $$\n根据前述可知\n$$ \\begin{split} \u0026amp;\\frac{RSS}{\\sigma^2}\\ \\sim \\chi_{n-p-1}^{2} \\newline \u0026amp;\\frac{SS_{Reg}}{\\sigma^2} \\sim \\chi_{p}^{2}\\left( \\frac{\\hat{\\mu}^{\\mathrm{T}}P_{X_c}\\hat{\\mu}}{\\sigma^2} \\right) \\end{split} $$\n其中 $\\hat{\\mu}=1\\beta _0+X\\beta$，故而当原假设成立时有 $\\hat{\\mu}=1\\beta _0$，因此\n$$ \\begin{split} \\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\hat{\\mu}\u0026amp;=\\beta _{0}^{2}1^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) 1 \\newline \u0026amp;=\\beta _{0}^{2}\\left( 1^{\\mathrm{T}}1-1^{\\mathrm{T}}1-1^{\\mathrm{T}}P _{X_c}1 \\right) \\newline \u0026amp;=-\\beta _{0}^{2}1^{\\mathrm{T}}P_{X_c}1 \\end{split} $$\n而对 $1^{\\mathrm{T}}P_{X_c}1$ 有\n$$ \\begin{split} 1^{\\mathrm{T}}P_{X_c}1\u0026amp;=1^{\\mathrm{T}}X_c\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}1 \\newline \u0026amp;=1^{\\mathrm{T}}\\left( I-1\\left( 1^{\\mathrm{T}}1 \\right) ^{-1}1^{\\mathrm{T}} \\right) X\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}1 \\newline \u0026amp;=\\left( 1^{\\mathrm{T}}-1^{\\mathrm{T}} \\right) X\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}1 \\newline \u0026amp;=0 \\end{split} $$\n所以 $\\hat{\\mu}^{\\mathrm{T}}\\left( I-P_1-P_{X_c} \\right) \\hat{\\mu}=0$，同理可知 $\\hat{\\mu}^{\\mathrm{T}}P_{X_c}\\hat{\\mu}=0$，故而在原假设成立的条件下有\n$$ \\begin{split} \\frac{RSS}{\\sigma^2}\\ \u0026amp;\\sim \\chi_{n-p-1}^{2} \\newline \\frac{SS_{Reg}}{\\sigma^2} \u0026amp;\\sim \\chi_{p}^{2} \\end{split} $$\n所以，在 $H_0$ 成立的条件下有\n$$ F_0=\\frac{SS_{Reg}/p}{RSS/\\left( n-p-1 \\right)}\\sim F_{p,n-p-1} $$\n这样当给定置信水平 $\\alpha$，在原假设成立的条件下回归平方和 $SS_{Reg}$ 较小，而残差平方和 $RSS$则较大，因此检验统计量 $F_0$ 就应该较小。那么，拒绝原假设的条件就是 $F_0\\ge F_{\\alpha ,p,n-p-1}$。 这里 $F_{\\alpha ,p,n-p-1}$ 表示上侧 $\\alpha$ 分位数（下同）。\n 不带截距项的模型  在这个检验前提下，我们的原假设和备择假设通常是\n$$ \\begin{split} \u0026amp;H_0:\\beta _1=\\beta _2=\\cdots =\\beta _p=0 \\newline \u0026amp;H_1:\\beta _j\\ne 0,\\mathrm{至少对一}个j\\mathrm{成立} \\end{split} $$\n类似的，当原假设成立时有 $\\hat{\\mu}=0$，则\n$$ \\begin{split} \\frac{RSS}{\\sigma^2}\\ \u0026amp;\\sim \\chi_{n-p}^{2} \\newline \\frac{SS_{Reg}}{\\sigma^2} \u0026amp;\\sim \\chi_{p}^{2} \\end{split} $$\n所以，在原假设成立的条件下有\n$$ F_0=\\frac{SS_{Reg}/p}{RSS/\\left( n-p \\right)}~F_{p,n-p} $$\n拒绝原假设的条件同前。\n3.2 单个系数的检验 单个系数的检验通常和从模型中增删变量有关。一般来说，增加一个额外的变量到模型中去，不会使得回归平方和 减小，也不会使得残差平方和 $SS_{Reg}$ 增大。但是，回归平方和 $SS_{Reg}$ 的一点点增大能否充分的保证在模型中引入了一个冗余变量？我们之所以关心，是因为增加一个冗余变量到模型中确实会增大均方误差，因而将模型的可用性降低了。\n检验任何一个单独的回归系数 $\\beta_j$ 的假设通常是\n$$ \\begin{split} \u0026amp;H_0:\\beta _j=0 \\newline \u0026amp;H_1:\\beta _j\\ne 0 \\end{split} $$\n 带截距项的模型  对带截距项的模型有\n$$ \\hat{\\beta}=\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}X_{c}^{\\mathrm{T}}y $$\n因为 $E\\hat{\\beta}=\\beta$ 和 $Var\\left( \\hat{\\beta} \\right) =\\sigma ^2\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}$，显然有\n$$ \\frac{\\hat{\\beta}_j-\\beta _j}{\\sqrt{\\sigma ^2C _{jj}}}\\sim N\\left( 0,1 \\right) $$\n其中 $C_{jj}$ 是矩阵 $\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}$ 第 个对角元。但是由于一般情况下 $\\sigma^2$ 未知，所以我们再考虑 $RSS$ 有\n$$ \\frac{RSS}{\\sigma ^2}\\ \\sim \\chi _{n-p-1}^{2} $$\n所以有\n$$ \\frac{\\left( n-p-1 \\right) s^2}{\\sigma ^2}\\sim \\chi _{n-p-1}^{2} $$\n其中 $s^2$ 是样本的方差，是可以通过样本计算得到的。那么，在原假设成立的条件下有\n$$ t_0 = \\frac{\\hat{\\beta}_j}{\\sqrt{s^2C _{jj}}} \\sim t _{n-p-1} $$\n因此，给定置信水平 $\\alpha$ 在原假设成立的条件下，上式 $t_0$ 的绝对值就不会特别大。那么，拒绝 $H_0$ 的条件就是：$| t_0 |\u0026gt;t_{\\alpha /2,n-p-1}$。\n对于截距项有\n$$ \\frac{\\hat{\\beta}_0-\\beta _0}{\\sqrt{\\sigma ^2\\left( 1/n+\\bar{X}^{\\mathrm{T}}\\left( X _{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\bar{X} \\right)}}\\sim N\\left( 0,1 \\right) $$\n因此\n$$ t_0 = \\frac{\\hat{\\beta}_j}{\\sqrt{s^2(1/n + \\bar{X} ^{\\mathrm{T}}(X _c^{\\mathrm{T}}X _c)^{-1}\\bar{X})}} \\sim t _{n-p-1} $$\n综述，给定置信水平 $\\alpha$ 在原假设 $H_0$ 成立的条件下，拒绝 $H_0$ 的条件就是：$| t_0 |\u0026gt;t_{\\alpha /2,n-p-1}$。\n 不带截距项的模型  对不带截距项的模型有\n$$ \\hat{\\beta}=\\left( X_{}^{\\mathrm{T}}X_{} \\right) ^{-1}X_{}^{\\mathrm{T}}y $$\n类似的，因为 $E\\hat{\\beta}=\\beta $ 和 $E\\hat{\\beta}=\\beta $，所以\n$$ \\frac{\\hat{\\beta}_j-\\beta_j}{\\sqrt{\\sigma ^2C _{jj}}} \\sim N\\left( 0,1 \\right) $$\n其中 $C_{jj}$ 是矩阵 $(X^{\\mathrm{T}}X)^{-1}$ 第 $j$ 个对角元。但是由于一般情况下 $\\sigma^2$ 未知，所以我们再考虑 $RSS$ 有\n$$ \\frac{RSS}{\\sigma ^2}\\ \\sim \\chi _{n-p}^{2} $$\n所以有\n$$ \\frac{\\left( n-p \\right) s^2}{\\sigma ^2} \\sim \\chi _{n-p}^{2} $$\n其中 $s^2$ 是样本的方差。那么在原假设成立的条件下有\n$$ t_0 = \\frac{\\hat{\\beta}_j}{\\sqrt{s^2C _{jj}}} \\sim t _{n-p} $$\n综述，给定置信水平 $\\alpha$ 在原假设 $H_0$ 成立的条件下，拒绝 $H_0$ 的条件就是：$| t_0 |\u0026gt;t_{\\alpha /2,n-p}$。\n4 区间估计 4.1 单个回归系数的置信区间  带截距项的区间估计  根据3.2节的分析和讨论，对于不带截距项的模型有\n$$ \\frac{\\hat{\\beta}_j-\\beta _j}{\\sqrt{s^2C _{jj}}}\\sim t _{n-p-1} $$\n所以，给定置信水平 $\\alpha$ 下 $\\beta_j$ 置信区间为：\n$$ \\hat{\\beta} _{j} - t _{\\alpha /2,n-p-1}\\sqrt{s^2C _{jj}}\\le \\beta _j\\le \\hat{\\beta}_j+t _{\\alpha /2,n-p-1}\\sqrt{s^2C _{jj}} $$\n 不带截距项的区间估计  类似的，给定置信水平 $\\alpha$ 下 $\\beta_j$ 置信区间为\n$$ \\hat{\\beta} _{j} - t _{\\alpha /2,n-p}\\sqrt{s^2C _{jj}}\\le \\beta _j\\le \\hat{\\beta}_j+t _{\\alpha /2,n-p}\\sqrt{s^2C _{jj}} $$\n4.2 回归系数的联合置信区域 前面的置信区间都是针对单个系数进行的，因此置信水平只对一个区间有效。但是，很多问题中需要让置信水平对所有的区间有效，这就是联合置信区域（simultaneous confidence intervals）。\n 带截距项的模型  由3.2节可知\n$$ \\left( \\hat{\\beta}-\\beta \\right) \\sim N\\left( 0,\\sigma ^2\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1} \\right) $$\n所以\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right)}{\\sigma ^2}\\sim \\chi _{p}^{2} $$\n这里 $\\sigma^2$ 未知，为构造检验统计量再考虑 $RSS$ 有\n$$ \\frac{RSS}{\\sigma ^2} \\sim \\chi _{n-p-1}^{2} $$\n所以得到\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right) /p}{RSS/\\left( n-p-1 \\right)} \\sim F_{p,n-p-1} $$\n那么在置信水平 $\\alpha$ 下，所有的回归系数需要满足\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right) /p}{RSS/\\left( n-p-1 \\right)}\\le F_{\\alpha ,p,n-p-1} $$\n 不带截距项的模型  同理可知，对于不带截距项的模型，在给定置信水平 $\\alpha$ 下全部系数需要满足\n$$ \\frac{\\left( \\hat{\\beta}-\\beta \\right) ^{\\mathrm{T}}\\left( X_{c}^{\\mathrm{T}}X_c \\right) ^{-1}\\left( \\hat{\\beta}-\\beta \\right) /p}{RSS/\\left( n-p-1 \\right)}\\le F_{\\alpha ,p,n-p} $$\n对于前述联合置信区域的不等式描述的是一个椭圆约束区域。 当 $p=2$ 时，这个区域相当简单；但是当 $p\\geq 2$ 时，问题就变得复杂的多。\n5 模拟分析 我在简单回归分析中介绍了R求解线性模型的方法，这里我们根据前面的理论分析结果，用R编写系数求解的函数、单个系数的显著性检验函数。这些代码附在最后供参考。\n我们考虑模型\n$$ y=1\\beta _0+X\\beta+\\epsilon $$\n其中 $\\beta _0=3$，$\\beta=\\left( 1,3,2,1,4,5,2,6 \\right) ^{\\mathrm{T}}$，$y\\in \\mathbb{R}^{100}$。根据前述分析，使用最小二乘法进行估计，在R软件中得到结果如下\n可以看出估计得结果很好，模型和单个系数都通过了检验。\n当然，我们可以对相同的数据使用R软件求解带截距的线性模型，得到的结果如下图所示。可以看出，两者是一致的。\n此外，我们也可以对模型考虑不带截距项的估计。这样得到的结果如下图所示\n对比R软件内置函数求解的结果\n首先，两者得到的结果是一样的。其次，我们发现对于一个本身带有截距项的模型使用不带截距项的方法求解，得到的效果就不好。 因为可以看到变量 $x_4$ 的系数估计没有通过检验。\n我们可以通过计算模型得到的残差，如学生化残差，通过分析残差的分布和对应的正态分布之间的异同，从而对回归结果做出分析和判断。如下图所示，是带截距模型的学生化残差分布图（左）以及不带截距的学生化残差分布图（右）\n从图上可以看出来，使用正确的模型得到的残差分布和正态分布较为接近，说明估计的效果好；而使用错误的模型得到的结果，残差分布和正态分布相差较大，估计的效果不好。\n6 自编函数代码 6.1 模型求解 实际使用，主要运行该代码即可。这个代码整合了参数估计、检验等代码，将结果合并输出。\n##求解线性模型\rmylm \u0026lt;- function(x,y,method = \u0026quot;ols\u0026quot;,intercept = T){#\r# 本程序用来求解线性模型参数估计 区间估计和假设检验\r# method可选：ols,max 前者最小二乘 后者极大似然估计\r# intercept为T表示模型带截距 为F表示模型不带截距 # 求解估计量\rb \u0026lt;- mylmestimate(x,y,method = method,intercept = intercept)\r# 求解离差平方和分解\rrsquare \u0026lt;- mylmsquare(x,y,b,intercept = intercept)\r# 单个系数检验\rindivi \u0026lt;- mylmindivi(x,y,b,intercept = intercept)\r# 模型检验\rlmtest \u0026lt;- mylmtest(x,y,b,intercept = intercept)\rif(intercept){\rbname \u0026lt;- c(\u0026quot;(Intercept)\u0026quot;,paste0(rep(\u0026quot;x\u0026quot;,p),1:p))\r}\relse{\rbname \u0026lt;- paste0(rep(\u0026quot;x\u0026quot;,p),1:p)\r}\restimate.test \u0026lt;- data.frame(\r\u0026quot;Beta\u0026quot; = bname,\r\u0026quot;Estimate\u0026quot; = b,\r\u0026quot;t.value\u0026quot; = indivi$Tt,\r\u0026quot;P.value\u0026quot; = indivi$Tp,\r\u0026quot;Sig.\u0026quot; = indivi$Ts\r)\r# 输出结果\rif(intercept){\rcat(\u0026quot;Coefficients:\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rprint(estimate.test)\rcat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;Multiple R-squared:\u0026quot;,(rsquare$sreg/rsquare$syy),\u0026quot;\u0026quot;)\rcat(\u0026quot;Adjusted R-squared:\u0026quot;,(1-(rsquare$rss*(n-1))/(rsquare$syy*(n-p-1))),\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;F-statistic:\u0026quot;,lmtest$f0,\u0026quot;\u0026quot;)\rcat(\u0026quot;on\u0026quot;,p,\u0026quot;\u0026quot;)\rcat(\u0026quot;and\u0026quot;,(n-p-1),\u0026quot;\u0026quot;)\rcat(\u0026quot;DF,\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\u0026quot;)\rcat(\u0026quot;p-value:\u0026quot;,lmtest$p,\u0026quot;\u0026quot;)\r}\relse{\rcat(\u0026quot;Coefficients:\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rprint(estimate.test)\rcat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;---\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;Multiple R-squared:\u0026quot;,(rsquare$sreg/rsquare$syy),\u0026quot;\u0026quot;)\rcat(\u0026quot;Adjusted R-squared:\u0026quot;,(1-(rsquare$rss*n)/(rsquare$syy*(n-p))),\u0026quot;\\n\u0026quot;)\rcat(\u0026quot;F-statistic:\u0026quot;,lmtest$f0,\u0026quot;\u0026quot;)\rcat(\u0026quot;on\u0026quot;,p,\u0026quot;\u0026quot;)\rcat(\u0026quot;and\u0026quot;,(n-p),\u0026quot;\u0026quot;)\rcat(\u0026quot;DF,\u0026quot;,\u0026quot;\u0026quot;,\u0026quot;\u0026quot;)\rcat(\u0026quot;p-value:\u0026quot;,lmtest$p,\u0026quot;\u0026quot;)\r}\r}\r 6.2 参数估计 ##求解线性模型参数估计\rmylmestimate \u0026lt;- function(x,y,method = \u0026quot;ols\u0026quot;,intercept = T){#\rn \u0026lt;- length(y)\rif(method == \u0026quot;ols\u0026quot;){\rif(intercept){\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\r# 得到最小二乘估计\rb1 \u0026lt;- solve(t(xcenter)%*%xcenter)%*%t(xcenter)%*%y\rb0 \u0026lt;- mean(y) - t(xbar0)%*%b1\rb \u0026lt;- c(b0,b1)\r}\relse{\r# 得到最小二乘估计\rb \u0026lt;- solve(t(x)%*%x)%*%t(x)%*%y\r}\r}\relse{\rif(intercept){\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\r# 得到极大似然估计\rb1 \u0026lt;- solve(t(xcenter)%*%xcenter)%*%t(xcenter)%*%y\rb0 \u0026lt;- mean(y) - t(xbar0)%*%b1\rb \u0026lt;- c(b0,b1)\r}\relse{\r# 得到极大似然估计\rb \u0026lt;- solve(t(x)%*%x)%*%t(x)%*%y\r}\r}\rreturn(b)\r}\r 6.3 假设检验 ##求解线性模型假设检验\rmylmtest \u0026lt;- function(x,y,b,intercept = T){\rn \u0026lt;- dim(x)[1]\rp \u0026lt;- dim(x)[2]\rif(intercept){\rybar \u0026lt;- mean(y)\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值\r# 计算各个离差平方和\rrss \u0026lt;- t((y - yhat))%*%(y - yhat)\rsreg \u0026lt;- t((yhat - ybar))%*%(yhat - ybar)\r# 计算检验统计量\rf0 \u0026lt;- (sreg/p)/(rss/(n - p - 1))\r# 输出p值\rp \u0026lt;- 1 - pf(f0,p,n-p-1)\r}\relse{\ryhat \u0026lt;- x%*%b#计算回归值\r# 计算各个离差平方和\rrss \u0026lt;- t((y - yhat))%*%(y - yhat)\rsreg \u0026lt;- t(yhat)%*%(yhat)\r# 计算检验统计量\rf0 \u0026lt;- (sreg/p)/(rss/(n - p))\r# 输出p值\rp \u0026lt;- 1 - pf(f0,p,n-p)\r}\rmodeltest \u0026lt;- data.frame(\u0026quot;f0\u0026quot; = f0,\u0026quot;p\u0026quot; = p)\rreturn(modeltest)\r}\r 6.4 单个系数检验 ##求解线性模型单个系数检验\rmylmindivi \u0026lt;- function(x,y,b,intercept = T){\rn \u0026lt;- dim(x)[1]\rp \u0026lt;- dim(x)[2]\rif(intercept){\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值\rs \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p-1) #计算MSE\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\rcc \u0026lt;- solve(t(xcenter)%*%xcenter)\rdiagc \u0026lt;- diag(cc)\rTt \u0026lt;- 1:(p+1);Tp \u0026lt;- Tt;Ts \u0026lt;- Tt\rTt[1] \u0026lt;- b[1]/sqrt(s*(1/n + t(xbar0)%*%cc%*%xbar0))\rTp[1] \u0026lt;- 2*(1 - pt(Tt[1],n-p-1))\rif(Tp[1] \u0026lt; 0.001 ){# 判断置信度\rTs[1] \u0026lt;- c(\u0026quot;***\u0026quot;)\r}\relse if(Tp[1] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[1] \u0026gt;= 0.001){\rTs[1] \u0026lt;- c(\u0026quot;**\u0026quot;)\r}\relse if(Tp[1] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[1] \u0026gt;= 0.01){\rTs[1] \u0026lt;- c(\u0026quot;*\u0026quot;)\r}\relse if(Tp[1] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[1] \u0026gt;= 0.05){\rTs[1] \u0026lt;- c(\u0026quot;.\u0026quot;)\r}\relse{\rTs[1] \u0026lt;- c(\u0026quot; \u0026quot;)\r}\rfor(i in 2:(p+1)){\rTt[i] \u0026lt;- b[i]/sqrt(s*diagc[i-1])\rTp[i] \u0026lt;- 2*(1 - pt(Tt[i],n-p-1))\rif(Tp[i] \u0026lt; 0.001 ){# 判断置信度\rTs[i] \u0026lt;- c(\u0026quot;***\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){\rTs[i] \u0026lt;- c(\u0026quot;**\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){\rTs[i] \u0026lt;- c(\u0026quot;*\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){\rTs[i] \u0026lt;- c(\u0026quot;.\u0026quot;)\r}\relse{\rTs[i] \u0026lt;- c(\u0026quot; \u0026quot;)\r}\r}\r}\relse{\ryhat \u0026lt;- x%*%b#计算回归值\rs \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p) #计算MSE\rcc \u0026lt;- solve(t(x)%*%x)\rdiagc \u0026lt;- diag(cc)\rTt \u0026lt;- 1:p;Tp \u0026lt;- Tt;Ts \u0026lt;- Tt\rfor(i in 1:p){\rTt[i] \u0026lt;- b[i]/sqrt(s*diagc[i])\rTp[i] \u0026lt;- 2*(1 - pt(Tt[i],n-p))\rif(Tp[i] \u0026lt; 0.001 ){# 判断置信度\rTs[i] \u0026lt;- c(\u0026quot;***\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.01 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.001){\rTs[i] \u0026lt;- c(\u0026quot;**\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.05 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.01){\rTs[i] \u0026lt;- c(\u0026quot;*\u0026quot;)\r}\relse if(Tp[i] \u0026lt; 0.1 \u0026amp;\u0026amp; Tp[i] \u0026gt;= 0.05){\rTs[i] \u0026lt;- c(\u0026quot;.\u0026quot;)\r}\relse{\rTs[i] \u0026lt;- c(\u0026quot; \u0026quot;)\r}\r}\r}\rindivitest \u0026lt;- data.frame(\u0026quot;Tt\u0026quot; = Tt,\u0026quot;Tp\u0026quot; = Tp,\u0026quot;Ts\u0026quot; = Ts)\rreturn(indivitest)\r}\r 6.5 区间估计 ##求解线性模型区间估计\rmylminterval \u0026lt;- function(x,y,b,alpha = 0.05,intercept = T){\rn \u0026lt;- dim(x)[1]\rp \u0026lt;- dim(x)[2]\rif(intercept){\rTi \u0026lt;- matrix(0,nrow = (p+1),ncol = 2)\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值\rs \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p-1) #计算MSE\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\rcc \u0026lt;- solve(t(xcenter)%*%xcenter)\rdiagc \u0026lt;- diag(cc)\rTi[1,1] \u0026lt;- b[1] - qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\rTi[1,2] \u0026lt;- b[1] + qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\rfor(i in 2:(p+1)){\rTi[i,1] \u0026lt;- b[i] - qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\rTi[i,2] \u0026lt;- b[i] + qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\r}\r}\relse{\rTi \u0026lt;- matrix(0,nrow = p,ncol = 2)\ryhat \u0026lt;- x%*%b#计算回归值\rs \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p) #计算MSE\rcc \u0026lt;- solve(t(x)%*%x)\rdiagc \u0026lt;- diag(cc)\rfor(i in 1:p){\rTi[i,1] \u0026lt;- b[i] - qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\rTi[i,2] \u0026lt;- b[i] + qt((1-alpha/2),n-p-1)*sqrt(s*diagc[1])\r}\r}\rcolnames(Ti) \u0026lt;- c(\u0026quot;LowerBound\u0026quot;,\u0026quot;UpperBound\u0026quot;)\rreturn(Ti)\r}\r 6.6 离差平方和分解 ##求解线性模型离差平方和分解\rmylmsquare \u0026lt;- function(x,y,b,intercept = T){\rif(intercept){\rybar \u0026lt;- mean(y)\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]#计算回归值\r# 计算各个离差平方和\rrss \u0026lt;- t((y - yhat))%*%(y - yhat)\rsreg \u0026lt;- t((yhat - ybar))%*%(yhat - ybar)\rsyy \u0026lt;- t((y - ybar))%*%(y - ybar)\r}\relse{\ryhat \u0026lt;- x%*%b#计算回归值\r# 计算各个离差平方和\rrss \u0026lt;- t((y - yhat))%*%(y - yhat)\rsreg \u0026lt;- t(yhat)%*%(yhat)\rsyy \u0026lt;- t(y)%*%y\r}\rrsquare \u0026lt;- data.frame(\u0026quot;syy\u0026quot; = syy,\u0026quot;rss\u0026quot; = rss,\u0026quot;sreg\u0026quot; = sreg)\rreturn(rsquare)\r}\r 6.7 各种残差 ##求解线性模型各种残差\rmylmresidual \u0026lt;- function(x,y,b,intercept = T){\rn \u0026lt;- dim(x)[1]\rp \u0026lt;- dim(x)[2]\rif(intercept){\ryhat \u0026lt;- b[1] + x%*%b[2:(p+1)]# 计算回归值\rsig.hat \u0026lt;- t(y - yhat)%*%(y - yhat)/(n-p-1)# 计算方差估计值\r# 计算帽子矩阵对角元\rxbar0 \u0026lt;- apply(x,2,mean) # 按x的列计算均值\rxbar1 \u0026lt;- matrix(rep(xbar0,n),nrow = n,byrow = T)\rxcenter \u0026lt;- x - xbar1 # 得到按列中心化的矩阵x\rp1 \u0026lt;- matrix(1/n,nrow = n,ncol = n)\rh \u0026lt;- p1 + xcenter%*%solve(t(xcenter)%*%xcenter)%*%t(xcenter)\rdiagh \u0026lt;- diag(h)\r# 计算原始残差\roriginres \u0026lt;- y - yhat\r# 计算standardized residuals\rstandres \u0026lt;- (y - yhat)/sqrt(sig.hat*rep(1,n))\r# 计算student residuals\rstudres \u0026lt;- (y - yhat)/sqrt(sig.hat*(1 - diagh))\r# 计算PRESS residual\rpressres \u0026lt;- (y - yhat)/(1 - diagh)\r}\relse{\ryhat \u0026lt;- x%*%b# 计算回归值\rsig.hat \u0026lt;- t((y - yhat))%*%(y - yhat)/(n-p)# 计算方差估计值\r# 计算原始残差\roriginres \u0026lt;- y - yhat\r# 计算帽子矩阵对角元\rh \u0026lt;- x%*%solve(t(x)%*%x)%*%t(x)\rdiagh \u0026lt;- diag(h)\r# 计算standardized residuals\rstandres \u0026lt;- (y - yhat)/sqrt(sig.hat*rep(1,n))\r# 计算student residuals\rstudres \u0026lt;- (y - yhat)/sqrt(sig.hat*(1 - diagh))\r# 计算PRESS residual\rpressres \u0026lt;- (y - yhat)/(1 - diagh)\r}\rlmres \u0026lt;- data.frame(\u0026quot;oringinal.Residual\u0026quot; = originres,\r\u0026quot;Standardized.Residual\u0026quot; = standres,\r\u0026quot;Student.Residual\u0026quot; = studres,\r\u0026quot;PRESS.Residual\u0026quot; = pressres)\rreturn(lmres)\r}\r ","id":14,"section":"posts","summary":"本文总结了线性模型的主要知识点，分别为参数估计，包括最小二乘估计和极大似然估计，区间估计，假设检验。此外，针对每个内容，本文还给出了相应的R","tags":["多元统计","回归分析"],"title":"线性模型的理论与求解","uri":"https://jiandan94.github.io/2018/04/lm/","year":"2018"},{"content":"惊雷起，\n一霎清明夜雨袭。\n帘外风轻，\n潺潺春语，\n入梦浇睡意。\n心思尽去残花里。\n流水无情空悲寂。\n明朝晓看红湿处，\n杨柳依依，\n梨花风起，\n愁染缙纭堤。\n","id":15,"section":"posts","summary":"惊雷起， 一霎清明夜雨袭。 帘外风轻， 潺潺春语， 入梦浇睡意。 心思尽去残花里。 流水无情空悲寂。 明朝晓看红湿处， 杨柳依依， 梨花风起， 愁染缙纭堤。","tags":["虎溪岁月","诗文"],"title":"夜雨清明","uri":"https://jiandan94.github.io/2018/04/yyqm/","year":"2018"},{"content":"渝州故郡，重庆新府。横断巴楚，环扣水枢。三都之地，嘉陵长江汇韵；四方要冲，名家骚客聚属。都督刘公之雅望，重大建始；校长叶公之懿范，往来鸿儒。禹功有继，文翁留㤖。今哉重大，居985。名师云集，学子仰慕。花香大地，翠拥缙湖。廊腰缦回，林荫杏道真佳境；勾心斗角，诗照长亭好学处。\n余辞乡别离，求学万里。客居虎溪，专业统计。驽钝术浅，常慕鹦鹉之机；乏善可陈，空怀击流之意。所赖数统师亲，传道授业答疑；同窗修睦，直谅多闻明礼。四载光阴荏苒，一处心思凭寄。\n肄业在即，毕设期近。杨虎吾师，富学可亲。教授博导，治学严谨。幸蒙教诲，循循善行。洞火灵犀，醍醐灌顶。师兄师姐，帮助殷殷。评讲文献，指点迷津。心念及此，感激不尽。\n余智术有限，学浅才疏。草创成文，难免谬误。请洒潘江，各倾陆海云尔。\n","id":16,"section":"posts","summary":"渝州故郡，重庆新府。横断巴楚，环扣水枢。三都之地，嘉陵长江汇韵；四方要冲，名家骚客聚属。都督刘公之雅望，重大建始；校长叶公之懿范，往来鸿儒。","tags":["诗文","虎溪岁月"],"title":"本科毕设致谢","uri":"https://jiandan94.github.io/2017/06/bkbszx/","year":"2017"},{"content":"东汉末年，桓灵败政。君失道于百姓；民毋念及“火德”。贪狼盛而紫薇衰，豪强起而暴民乱。权佞当道，途有哀鸿饿殍；英雄逐鹿，有志待价而沽。狼烟四起，烽鼓难息，州吞郡合，而终致天下三分。\n期间名人，不乏有数:盖拥兵自重者，冀州袁绍；勇冠三军者，徐州吕布；衣食祖业者，益州刘璋；风流座客者，荆州刘表。然则公台寡断，奉先乏谋，季玉暗弱，景升志穷……盖以英雄称者，屈指可数矣——江东孙吴，跨江连郡，拒荆刘于江渚，败许曹于赤壁； 汉中刘备，盘踞西蜀，外抚羌夷余力，内安万民有术。此二人宏才雅量，志图天下，可以英雄称焉。然余独爱者，绝非孙刘，唯曹孟德一人而已。\n为操之论，一言难尽。余观夫操之人，亦臣亦君，亦盗亦民；所谓臣君，操之气度，所谓盗民，操之性情。臣君盗民，论其一生，古人云：美玉微瑕。此之余何独慕之。\n先时，宦竖乱朝，外戚干政，黄巾起而荼毒，董贼入而祸乱。惜哉，旧臣不臣，幽泣拥喟于室；新盟虽结，各怀鬼胎于心。操，身非苗裔，本承阉宦之嗣；胸藏鸿图，力尽汉臣之实。所以臣之节气，尽燃于胸，奈何持节守将，徒壁上观。呜呼!谋刀不在，檄文已失；潜龙勿用，养兵屯势。迨及气候稍聚，兵容初整，囊贤达逐汉鹿，挟天子令不臣。柔远能迩，悖(dūn)德允元。若弗尧察舜恭，则音律夺伦，文辞难兴（xìng），四凶作宰，百姓不亲。所以谓操有君度，诚乱世之枭雄。是故英雄不必至贤，而能贤贤，其身未必良将，而善将将。\n及至九锡服銮，出警入跸，左右维诺，庸命卑恭。文若在前，季珪难终，时谓阿瞒窃汉之心昭昭。但帝位尤忝，汉庙未隳。姑妄言之，或操诡谲，扶汉至伪，实则矫诏以便其意欤！抑或留待子嗣，登阁僭位，旧臣切念新恩，以期戮力而为。虽然，盗名之性，穷口莫辩。使操后观司马事，未知其意何如?噫!奈何天不假年，痼疾难消，病笃岌危，梦魇复绕。未尝嘱白帝之托，恍若山野小民，而忧妻妾营生。盖戎马峥峥寿尽，富贵闻达浮云，岂不闻鸟之将死，其鸣也哀；人之将死，其情也善。\n嗟乎， 余辞乡别离，求学万里，毕业将近，投笔无期。驽钝术浅，尝慕鹦鹉之机；乏善可陈，空怀击流之意。今复观三国旧事，聊以赋文，暂长精神而已。\n","id":17,"section":"posts","summary":"东汉末年，桓灵败政。君失道于百姓；民毋念及“火德”。贪狼盛而紫薇衰，豪强起而暴民乱。权佞当道，途有哀鸿饿殍；英雄逐鹿，有志待价而沽。狼烟四起","tags":["虎溪岁月","诗文"],"title":"曹操赋","uri":"https://jiandan94.github.io/2017/03/ccf/","year":"2017"},{"content":"渝州冬早立，近日尚秋衣。\n今明披绣闼，始觉冬意袭。\n瑟瑟西风浸，萧萧梧叶稀。\n是处斑驳景，唯独缙纭堤。\n擎伞盖已去，漏春仍昨夕。\n相对相忘言，只道无和羲。\n","id":18,"section":"posts","summary":"渝州冬早立，近日尚秋衣。 今明披绣闼，始觉冬意袭。 瑟瑟西风浸，萧萧梧叶稀。 是处斑驳景，唯独缙纭堤。 擎伞盖已去，漏春仍昨夕。 相对相忘言，只道无和","tags":["虎溪岁月","诗文"],"title":"冬日过缙纭堤","uri":"https://jiandan94.github.io/2016/11/drgjyd/","year":"2016"},{"content":"蓝桥古驿无踪觅，\n不见崔郎遇云英。\n可怜浆向虽易乞，\n难容相访饮牛津。\n","id":19,"section":"posts","summary":"蓝桥古驿无踪觅， 不见崔郎遇云英。 可怜浆向虽易乞， 难容相访饮牛津。","tags":["虎溪岁月","诗文"],"title":"读纳兰词","uri":"https://jiandan94.github.io/2016/11/dnlc/","year":"2016"},{"content":"银杏叶黄暑气收，\n天凉月高怕登楼。\n怕登楼。\n一见鸿书销眉愁。\n夜浣碧纱掩薄秋，\n累君多奔走，\n愿撷晚霞作兰舟。\n君言舟摇珠帘招，\n风也飘飘雨潇潇。\n雨潇潇。\n赌书廊前有陈雕。\n我抱长琴在虹桥，\n银字流水调，\n不教红樱负绿蕉。\n","id":20,"section":"posts","summary":"银杏叶黄暑气收， 天凉月高怕登楼。 怕登楼。 一见鸿书销眉愁。 夜浣碧纱掩薄秋， 累君多奔走， 愿撷晚霞作兰舟。 君言舟摇珠帘招， 风也飘飘雨潇潇。 雨潇潇。","tags":["虎溪岁月","诗文"],"title":"过银杏道","uri":"https://jiandan94.github.io/2016/10/gyxd/","year":"2016"},{"content":"骤雨如烟喧小楼，\n群芳摇落意无休。\n望断巴山犹未尽，\n点点雨丝点点愁。\n","id":21,"section":"posts","summary":"骤雨如烟喧小楼， 群芳摇落意无休。 望断巴山犹未尽， 点点雨丝点点愁。","tags":["虎溪岁月","诗文"],"title":"松园记雨","uri":"https://jiandan94.github.io/2016/06/syjy/","year":"2016"},{"content":"盛夏偏作晚秋天，\n小晴忽复雨涟涟。\n一任灯花自消落，\n辗转心思顾影怜。\n","id":22,"section":"posts","summary":"盛夏偏作晚秋天， 小晴忽复雨涟涟。 一任灯花自消落， 辗转心思顾影怜。","tags":["虎溪岁月","诗文"],"title":"夏日有感","uri":"https://jiandan94.github.io/2016/05/xryg/","year":"2016"},{"content":" 最喜欢这首诗！也许我天生就是一个懒散的人吧~\n 独坐轩窗向小楼，\n斜阳灼灼木幽幽。\n懒理红尘烦扰事，\n遥映菱花慢梳头。\n","id":23,"section":"posts","summary":"最喜欢这首诗！也许我天生就是一个懒散的人吧~ 独坐轩窗向小楼， 斜阳灼灼木幽幽。 懒理红尘烦扰事， 遥映菱花慢梳头。","tags":["虎溪岁月","诗文"],"title":"无题","uri":"https://jiandan94.github.io/2016/05/wt/","year":"2016"},{"content":"北风呼号雪飘飘，一人在家甚无聊。\n前日刚说有雨雪，明朝又要防寒潮。\n漫将闲书消冷寂，且煮陈茶度寒宵。\n无意庭院凄风紧，腊梅残落竟逍遥。\n","id":24,"section":"posts","summary":"北风呼号雪飘飘，一人在家甚无聊。 前日刚说有雨雪，明朝又要防寒潮。 漫将闲书消冷寂，且煮陈茶度寒宵。 无意庭院凄风紧，腊梅残落竟逍遥。","tags":["诗文"],"title":"初雪","uri":"https://jiandan94.github.io/2016/01/cx/","year":"2016"},{"content":"说起虎溪的冬天，第一反应也许就是那些浸淫着寒风冷雨的灰色画面撺掇起来的时节；因为就是在整个渝州，晴朗的日子也是很少遇上的。\n可是今早起床，掀开窗幔，扑面袭来的竟是暖心的惊喜——阳光！连雄姿都快消磨殆尽的松树们也开始抖擞起精神，于我来说，这不可不算作不能和别处的孩子为着第一场雪的到来而振奋的补偿吧。虽然，期末考已经提上日程，七零八碎的琐事充斥在举手投足间，但是眼睁睁地无视这来之不易的阳光，应该堪称“暴殄天物”了！一把椅子，一本卢梭的《社会契约论》，半杯纯咖啡足矣，短暂遗忘了整个世界的我坐在阳台上只对着眼前这暖暖的阳光，不禁想到Shelly 说的那句\u0026quot;A man,to be greatly good,must imagine intensively and comprehensively;he must put himself in the place of another and of many others;the pains and pleasures of his species become his own.\u0026rdquo;\n晚饭特地比往常早吃许久，为的是趁着冬阳的余温重读一番连日阴雨掩藏下的虎溪。我最爱在缙湖湖畔悠然漫步，却不是恋那葱茏依旧的杨柳草木，也不是想那整日怡然自得的天鹅锦鲤，为的是面前的一湖荷花（叶）。\n来虎溪也已经一年有余，习惯了麻辣刺激的重庆火锅，习惯了潮湿阴晦的天气，将将不能习惯这界限不甚分明的春夏秋冬。你看不到时间的更迭，更读不懂生命的意义。只有在这儿——缙湖，从这一池的荷叶中才能看到四季的流转，从“小荷才露尖尖角”到“接天莲叶无穷碧”，再到“荷尽已无擎雨盖”；你能深深地感受着生命的呼吸。也许是耗尽了最后一丝春夏蓄积的精华，当我现在看到他们时，尽管腰杆挺立，但头颅低垂，望着波澜不惊的水面，望着自己曾经成长的印迹。当我正想用“凄凉”或是“伤感”这样的词来形容它们时，不经意的一瞥这整个一湖的静立者们，一瞥与之格格不入的周围的绿柳青草，忽然间觉得我只能望着，满怀敬意，不敢言语。\n我时常找机会来这儿独自漫步，看看这里唯一的生命呼吸者们，也感受着自己生命的呼吸。生命只有在理解死亡时才觉出其中意义，也许，生活只有当乱中取静时才会一品个中滋味。\n","id":25,"section":"posts","summary":"说起虎溪的冬天，第一反应也许就是那些浸淫着寒风冷雨的灰色画面撺掇起来的时节；因为就是在整个渝州，晴朗的日子也是很少遇上的。 可是今早起床，掀开","tags":["虎溪岁月"],"title":"冬日虎溪","uri":"https://jiandan94.github.io/2015/12/winter-huxi/","year":"2015"},{"content":"凉风沁骨别暑天，\n芙蓉燎栗雨涟涟。\n十五中秋明月夜，\n卧听吴郎斧声坚。\n","id":26,"section":"posts","summary":"凉风沁骨别暑天， 芙蓉燎栗雨涟涟。 十五中秋明月夜， 卧听吴郎斧声坚。","tags":["虎溪岁月","诗文"],"title":"中秋有感","uri":"https://jiandan94.github.io/2015/09/zqyg/","year":"2015"},{"content":"最近考试频繁，都没有一个人好好在学校走走了。也不知是渝州受了厄尔尼诺的影响，还是老天眷顾虎溪的莘莘学子，天多半阴沉着，不见重庆孩子口中那往日毒辣的阳光。然而这几天，云淡风轻，艳阳高照，一靠近宿舍楼，远远地就听见呼呼地空调风扇声——入夏了，这次应该是真的！尽管天显得有点热，但好天气还是要出去走走的嘛~\n缙湖今年迎来了几只新宠，一直没时间去问候问候，所以刚刚考完试，疲惫不堪的我得了个多云天气的日子，就迫不及待地向缙湖走去。但当路过情人坡一段时，我不由自主地停下了脚步：好像有什么不对劲？细细观察才发现，原来是之前校工们给这里新换了草皮，那几天情人坡放眼望去，“伤痕累累”，“满目疮痍”，而且刚刚植上的草皮看上去矮小，枯黄······想想浪漫美丽的情人坡原来是这样修炼成长起来的呀！也许是最近雨过天晴，阳光正好，草们似乎卯足了劲儿往上蹿，现在伤疤不见，又是一片“夏”光啊！一下子，我忍不住停下脚步想来端详一下这群小家伙们。\n忽然间，我发现，猛地看上去长势一片大好的草皮实际却不是这样的：假如你稍微拨开一丛生长旺盛的草，你会发现他们的身下掩盖了另一丛草皮——只不过那丛生机不再，稀稀落落、参差不齐，看上去已是气数将尽。原来，生命的美好，是可以建立在其他同伴的尸体上的！当阴绵的雨水褪尽，迎来暖和的阳光时，每一株草都拼命地吸收能量，疯狂生长。要是有一株慢了节拍，或者遇到什么其他“挫折”，其他的草却不会热情地停下脚步等待那些陪伴自己度过苦日子的伙伴，反而踏着他们的躯体继续自己的生长。冰心说，“成功的花，人们只惊羡她现时的明艳！然而当初她的芽儿，浸透了奋斗的泪泉，洒遍了牺牲的血雨。”现在看来那些血雨多半是来自他们同伴的罢。\n想到这些，我不由得叹息：草们究竟不是人类，看我们虽然血液中也有生物原始的自私和贪婪，但是我们创立了文明，我们有，我们有······我们有什么？我楞住了，不是我想不来颂扬人类文明的例子，而是一瞬间闪在脑子里的竟然是人类中的那些“草”！\n然而，人类究竟比草还是高明点。我们发明面具掩饰，依靠“伪善”迷惑，凭借一张巧嘴用来颠倒黑白······看看这些草，像人类的种种野心和阴谋对它们来说竟然在光天化日就进行着；而我们却需要戴上面具，依靠黑夜，假借伪善······我现在站在他们面前嘲笑它们，它们却没有奚落我。我不知道，为什么明明宣扬着“文明”的我们中的我，面对着这群晴空下的阴谋者们，现在脸却红的发烫。\n有时候，我们以为的不是我们可以随便像我们以为的那样去以为；你觉得自己得到了什么，却输给别人半个灵魂！\n","id":27,"section":"posts","summary":"最近考试频繁，都没有一个人好好在学校走走了。也不知是渝州受了厄尔尼诺的影响，还是老天眷顾虎溪的莘莘学子，天多半阴沉着，不见重庆孩子口中那往日","tags":["虎溪岁月"],"title":"晴空下的阴谋","uri":"https://jiandan94.github.io/2015/06/qkym/","year":"2015"},{"content":" 2012年，读高二的我看到高三学长和学姐在举行“百日誓师大会”。他们的喊声不由得触动了我，让我对未来很是憧憬。几日后，便写下了这首诗。\n 江流浮沉风云涌，江畔何人起敛容。\n跣足常趋握发殿，躬身频至玉蟾宫。\n雄心勃勃扫漠北，虎视眈眈向江东。\n待到青梅煮酒日，拔剑示君论英雄。\n","id":28,"section":"posts","summary":"2012年，读高二的我看到高三学长和学姐在举行“百日誓师大会”。他们的喊声不由得触动了我，让我对未来很是憧憬。几日后，便写下了这首诗。 江流浮","tags":["虎溪岁月","诗文"],"title":"咏曹操","uri":"https://jiandan94.github.io/2012/03/ycc/","year":"2012"}],"tags":[{"title":"r语言","uri":"https://jiandan94.github.io/tags/r%E8%AF%AD%E8%A8%80/"},{"title":"分类算法","uri":"https://jiandan94.github.io/tags/%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95/"},{"title":"回归分析","uri":"https://jiandan94.github.io/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"},{"title":"多元统计","uri":"https://jiandan94.github.io/tags/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1/"},{"title":"影评","uri":"https://jiandan94.github.io/tags/%E5%BD%B1%E8%AF%84/"},{"title":"数学史","uri":"https://jiandan94.github.io/tags/%E6%95%B0%E5%AD%A6%E5%8F%B2/"},{"title":"时政","uri":"https://jiandan94.github.io/tags/%E6%97%B6%E6%94%BF/"},{"title":"相关性分析","uri":"https://jiandan94.github.io/tags/%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90/"},{"title":"算法","uri":"https://jiandan94.github.io/tags/%E7%AE%97%E6%B3%95/"},{"title":"纵向数据分析","uri":"https://jiandan94.github.io/tags/%E7%BA%B5%E5%90%91%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"title":"虎溪岁月","uri":"https://jiandan94.github.io/tags/%E8%99%8E%E6%BA%AA%E5%B2%81%E6%9C%88/"},{"title":"诗文","uri":"https://jiandan94.github.io/tags/%E8%AF%97%E6%96%87/"}]}